{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path\n",
    "import errno\n",
    "import sys\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import sampler\n",
    "import torchvision.transforms as T\n",
    "\n",
    "\n",
    "#import help_func\n",
    "#import new_ALL_Conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "if sys.version_info[0] == 2:\n",
    "    import cPickle as pickle\n",
    "else:\n",
    "    import pickle\n",
    "\n",
    "transform = T.Compose([\n",
    "                T.ToTensor(),\n",
    "                T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "            ])\n",
    "NUM_TRAIN=49000\n",
    "train_batch_size=4\n",
    "test_batch_size=4\n",
    "\n",
    "\n",
    "cifar10_train = torchvision.datasets.CIFAR10('./cs231n/datasets', train=True, download=True,\n",
    "                             transform=transform)\n",
    "loader_train = torch.utils.data.DataLoader(cifar10_train, batch_size=train_batch_size, \n",
    "                          sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN)))\n",
    "\n",
    "cifar10_val = torchvision.datasets.CIFAR10('./cs231n/datasets', train=True, download=True,\n",
    "                           transform=transform)\n",
    "loader_val = torch.utils.data.DataLoader(cifar10_val, batch_size=train_batch_size, \n",
    "                        sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN, 50000)))\n",
    "\n",
    "cifar10_test = torchvision.datasets.CIFAR10('./cs231n/datasets', train=False, download=True, \n",
    "                            transform=transform)\n",
    "loader_test = torch.utils.data.DataLoader(cifar10_test, batch_size=test_batch_size)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 32, 32, 3)\n",
      "(1000, 32, 32, 3)\n",
      "(5000, 32, 32, 3)\n",
      "(1000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "import torchvision.datasets as dset\n",
    "from PIL import Image    \n",
    "import os\n",
    "import os.path\n",
    "import errno\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "class CIFAR100(dset.CIFAR10):\n",
    "\n",
    "    base_folder = '.'\n",
    "    train_list = [\n",
    "        ['train', '16019d7e3df5f24257cddd939b257f8d'],\n",
    "    ]\n",
    "\n",
    "    test_list = [\n",
    "        ['test', 'f0ef6b0ae62326f3e7ffdfab6717acfc'],\n",
    "    ]\n",
    "    \n",
    "    def __init__(self, root, train=True,\n",
    "                 transform=None, target_transform=None,\n",
    "                 download=False):\n",
    "        self.root = os.path.expanduser(root)\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.train = train  # training set or test set\n",
    "\n",
    "        if download:\n",
    "            self.download()\n",
    "\n",
    "        if not self._check_integrity():\n",
    "            raise RuntimeError('Dataset not found or corrupted.' +\n",
    "                               ' You can use download=True to download it')\n",
    "\n",
    "        # now load the picked numpy arrays\n",
    "        self.test_data = []\n",
    "        if self.train:\n",
    "            self.train_data = []\n",
    "            self.train_labels = []\n",
    "            for fentry in self.train_list:\n",
    "                f = fentry[0]\n",
    "                file = os.path.join(self.root, self.base_folder, f)\n",
    "                fo = open(file, 'rb')\n",
    "                if sys.version_info[0] == 2:\n",
    "                    entry = pickle.load(fo)\n",
    "                else:\n",
    "                    entry = pickle.load(fo, encoding='latin1')\n",
    "                self.train_data.append(entry['data'])\n",
    "                if 'labels' in entry:\n",
    "                    self.train_labels += entry['labels']\n",
    "                else:\n",
    "                    self.train_labels += entry['fine_labels']\n",
    "                fo.close()\n",
    "\n",
    "            self.train_data = np.concatenate(self.train_data)\n",
    "            self.train_data = self.train_data.reshape((5000, 3, 32, 32))\n",
    "            self.train_data = self.train_data.transpose((0, 2, 3, 1))  # convert to HWC\n",
    "            print(self.train_data.shape)\n",
    "        else:\n",
    "            f = 'test'\n",
    "            file = os.path.join(self.root, self.base_folder, f)\n",
    "            fo = open(file, 'rb')\n",
    "            if sys.version_info[0] == 2:\n",
    "                entry = pickle.load(fo)\n",
    "            else:\n",
    "                entry = pickle.load(fo, encoding='latin1')\n",
    "            self.test_data = np.array(entry['data'])\n",
    "            if 'labels' in entry:\n",
    "                self.test_labels = entry['data']\n",
    "            else:\n",
    "                self.test_labels = entry['fine_labels']\n",
    "            fo.close()\n",
    "            self.test_data = self.test_data.reshape((1000, 3, 32, 32))\n",
    "            self.test_data = self.test_data.transpose((0, 2, 3, 1))  # convert to HWC\n",
    "            print(self.test_data.shape)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is index of the target class.\n",
    "        \"\"\"\n",
    "        if self.train:\n",
    "            img, target = self.train_data[index], self.train_labels[index]\n",
    "        else:\n",
    "            img, target = self.test_data[index], self.test_labels[index]\n",
    "\n",
    "        # doing this so that it is consistent with all other datasets\n",
    "        # to return a PIL Image\n",
    "        img = Image.fromarray(img.astype('uint8')).convert('RGB')\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.train:\n",
    "            return len(self.train_data)\n",
    "        else:\n",
    "            return len(self.test_data)                \n",
    "            \n",
    "    def _check_integrity(self):\n",
    "        return True\n",
    "    \n",
    "\n",
    "\n",
    "if sys.version_info[0] == 2:\n",
    "    import cPickle as pickle\n",
    "else:\n",
    "    import pickle\n",
    "\n",
    "transform = T.Compose([\n",
    "                T.ToTensor(),\n",
    "                T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "            ])\n",
    "\n",
    "A_train = CIFAR100('./class1', train=True, download=False,\n",
    "                             transform=transform)\n",
    "loaderA_train = torch.utils.data.DataLoader(A_train, batch_size=4)\n",
    "\n",
    "\n",
    "\n",
    "A_test = CIFAR100('./class1', train=False, download=False,\n",
    "                             transform=transform)\n",
    "loaderA_test = torch.utils.data.DataLoader(A_test, batch_size=4)\n",
    "\n",
    "\n",
    "B_train = CIFAR100('./class2', train=True, download=False,\n",
    "                             transform=transform)\n",
    "loaderB_train = torch.utils.data.DataLoader(B_train, batch_size=4)\n",
    "\n",
    "\n",
    "B_test = CIFAR100('./class2', train=False, download=False,\n",
    "                             transform=transform)\n",
    "loaderB_test = torch.utils.data.DataLoader(B_test, batch_size=4)\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right\n"
     ]
    }
   ],
   "source": [
    "class Stride_CNN_C(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super (Stride_CNN_C, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 96, kernel_size=3, stride=1, padding=1)\n",
    "        nn.init.kaiming_normal_(self.conv1.weight)\n",
    "        nn.init.constant_(self.conv1.bias, 0)\n",
    "        \n",
    "        self.dropout1 = nn.Dropout2d(0.2)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(96, 96, kernel_size=3, stride=2, padding=0)\n",
    "        nn.init.kaiming_normal_(self.conv2.weight)\n",
    "        nn.init.constant_(self.conv2.bias, 0)\n",
    "        \n",
    "        \n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(96, 192, kernel_size=3, stride=1, padding=1)\n",
    "        nn.init.kaiming_normal_(self.conv3.weight)\n",
    "        nn.init.constant_(self.conv3.bias, 0)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(192, 192, kernel_size=3, stride=2, padding=0)\n",
    "        nn.init.kaiming_normal_(self.conv4.weight)\n",
    "        nn.init.constant_(self.conv4.bias, 0)\n",
    "        \n",
    "       \n",
    "        self.dropout3 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.conv5 = nn.Conv2d(192, 192, kernel_size=3, padding=3)\n",
    "        nn.init.kaiming_normal_(self.conv5.weight)\n",
    "        nn.init.constant_(self.conv5.bias, 0)\n",
    "        \n",
    "        self.conv6 = nn.Conv2d(192, 192, kernel_size=1)\n",
    "        nn.init.kaiming_normal_(self.conv6.weight)\n",
    "        nn.init.constant_(self.conv6.bias, 0)\n",
    "        \n",
    "        self.conv7 = nn.Conv2d(192, self.num_classes, kernel_size=1)\n",
    "        nn.init.kaiming_normal_(self.conv7.weight)\n",
    "        nn.init.constant_(self.conv7.bias, 0)\n",
    "        \n",
    "        self.glb_avg = nn.AvgPool2d(6)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.conv1(x))\n",
    "        out = self.dropout1(out)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = F.relu(out)\n",
    "        \n",
    "        \n",
    "        out = self.dropout2(out)\n",
    "        \n",
    "        out = self.conv3(out)\n",
    "        out = F.relu(out)\n",
    "        \n",
    "        out = self.conv4(out)\n",
    "        out = F.relu(out)\n",
    "        \n",
    "        out = self.dropout3(out)\n",
    "        \n",
    "        out = self.conv5(out)\n",
    "        out = F.relu(out)\n",
    "        \n",
    "        out = self.conv6(out)\n",
    "        out = F.relu(out)\n",
    "        \n",
    "        out = self.conv7(out)\n",
    "        out = F.relu(out)\n",
    "        \n",
    "        out = self.glb_avg(out)\n",
    "        out = out.view(-1, self.num_classes)\n",
    "        return out\n",
    "import new_ALL_Conv\n",
    "stride_cnn_c = new_ALL_Conv.Stride_CNN_C()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from six.moves import cPickle\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import sampler\n",
    "\n",
    "USE_GPU = True\n",
    "dtype = torch.float32 # we will be using float throughout this tutorial\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "def check_accuracy(loader, model):\n",
    "    if loader.dataset.train:\n",
    "        print('Checking accuracy on validation set')\n",
    "    else:\n",
    "        print('Checking accuracy on test set')   \n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "            scores = model(x)\n",
    "            _, preds = scores.max(1)\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_samples += preds.size(0)\n",
    "        acc = float(num_correct) / num_samples *100\n",
    "        print('Got %d / %d correct (%.2f)' % (num_correct, num_samples,  acc))\n",
    "    return acc*100\n",
    "\n",
    "def running_model_B(run_num, net, net_name, lr_list, epoch_list, loader_train, \n",
    "                   loader_test):\n",
    "    train_batch_size = 4\n",
    "    test_batch_size = 4\n",
    "    \n",
    "\n",
    "    # Constant to control how frequently we print train loss\n",
    "    print_every = 100\n",
    "\n",
    "    print('using device:', device)\n",
    "    \n",
    "    #net = BaseNet_A()\n",
    "    net = net.to(device=device)\n",
    "    criterion = nn.CrossEntropyLoss()        \n",
    "        \n",
    "    lr_1, lr_2, lr_3, lr_4 = lr_list[0], lr_list[1], lr_list[2], lr_list[3]\n",
    "    weight_decay = 0.001\n",
    "\n",
    "    max_epoch = 350\n",
    "    display_interval = 500\n",
    "\n",
    "    train_size = 5000\n",
    "    test_size = 1000\n",
    "\n",
    "    num_train_batch = train_size/train_batch_size\n",
    "    num_test_batch = test_size/test_batch_size\n",
    "\n",
    "    train_loss = np.zeros((max_epoch,1))\n",
    "    val_acc = np.zeros((max_epoch,1))\n",
    "    #train_acc = np.zeros((max_epoch,1))\n",
    "    #test_loss = np.zeros((max_epoch,1))\n",
    "    #test_acc = np.zeros((max_epoch,1))\n",
    "\n",
    "    epoch_acc = [] # max_epoch x num\n",
    "    print(\"begin training\")\n",
    "    for epoch in range(max_epoch):\n",
    "        if(epoch<epoch_list[0]):\n",
    "            lr = lr_1\n",
    "        elif(epoch<epoch_list[1]):\n",
    "            lr = lr_2\n",
    "        elif(epoch<epoch_list[2]):\n",
    "            lr = lr_3\n",
    "        else:\n",
    "            lr = lr_4\n",
    "    \n",
    "        optimizer = optim.SGD( net.parameters(), lr=0.001,\n",
    "                              momentum=0.9, weight_decay=weight_decay)\n",
    "    \n",
    "        running_epoch_loss = 0.\n",
    "        running_loss_print = 0.\n",
    "        epoch_total_num = 0\n",
    "        correct_num = 0\n",
    "    \n",
    "        i_acc = []\n",
    "        #for i, data in enumerate(trainloader):\n",
    "        for i, data in enumerate(loader_train):\n",
    "            net.train()\n",
    "        \n",
    "            inputs_data, labels_data = data\n",
    "            inputs, labels = Variable(inputs_data), Variable(labels_data)\n",
    "            inputs = inputs.to(device=device, dtype=dtype)\n",
    "            labels = labels.to(device=device, dtype=torch.long)\n",
    "        \n",
    "    \n",
    "        \n",
    "        \n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "        \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "            running_epoch_loss += loss.item()\n",
    "            running_loss_print += loss.item()\n",
    "            if i%500 == 499: #net a, b, c 500 print once\n",
    "                \n",
    "                acc = check_accuracy(loader_test, net)\n",
    "                i_acc.append(acc)\n",
    "                print('%d epoch, %5d iteration, loss:%.3f' \n",
    "                      %(epoch+1, i+1, running_loss_print/1000) )\n",
    "                running_loss_print = 0.\n",
    "            \n",
    "            #_, pred = torch.max(outputs, 1)\n",
    "            #epoch_total_num += labels.size(0)\n",
    "            #correct_num += (pred==labels).sum()\n",
    "        \n",
    "        \n",
    "        train_loss[epoch] = running_epoch_loss/num_train_batch\n",
    "        epoch_acc.append(i_acc)\n",
    "        \n",
    "        val_acc[epoch] = np.sum(epoch_acc[epoch])/49\n",
    "        #val_acc[epoch] = np.sum(epoch_acc[epoch])\n",
    "        #train_acc[epoch] = correct_num/epoch_total_num*100\n",
    "    \n",
    "    \n",
    "        # test accuracy and loss\n",
    "        '''\n",
    "        ts_runningloss_epoch = 0.\n",
    "        ts_correct = 0\n",
    "        ts_epoch_num = 0\n",
    "        for data in testloader:\n",
    "            inputs_data, labels_data = data\n",
    "            inputs, labels = Variable(inputs_data), Variable(labels_data)\n",
    "            inputs = inputs.to(device=device, dtype=dtype)\n",
    "            labels = labels.to(device=device, dtype=torch.long)\n",
    "        \n",
    "            net.eval()\n",
    "            ts_outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "        \n",
    "            ts_runningloss_epoch += loss.item()\n",
    "            _, ts_pred = torch.max(ts_outputs, 1)\n",
    "        \n",
    "            ts_epoch_num += labels.size(0)\n",
    "            ts_correct += (ts_pred==labels).sum()\n",
    "        test_loss[epoch] = ts_runningloss_epoch/num_test_batch\n",
    "        test_acc[epoch] = ts_correct/ts_epoch_num*100\n",
    "        '''\n",
    "        print(\" num %d epoch \" %epoch)\n",
    "        print(\"####### Training Loss #######\")\n",
    "        print(train_loss[epoch])\n",
    "        #print(\"####### Validation Accuracy #######\")\n",
    "        #print(val_acc[epoch])\n",
    "        #print(\"####### Training Accuracy #######\")\n",
    "        #print(train_acc[epoch])\n",
    "        #print(\"####### Testing Loss #######\")\n",
    "        #print(test_loss[epoch])\n",
    "        #print(\"####### Testing Accuracy #######\")\n",
    "        #print(test_acc[epoch])\n",
    "    \n",
    "    print('finish training \\n')\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "##################################################################################################\n",
    "    print('now begin saving datum for next step plotting')\n",
    "    \n",
    "    save_path = '../datum_for_plotting/run_num_' + str(run_num)+'/'+ net_name\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    f = open(save_path + '/train_loss.save', 'wb')\n",
    "    cPickle.dump(train_loss, f, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "    f.close()\n",
    "\n",
    "    f = open(save_path + '/val_acc.save', 'wb')\n",
    "    cPickle.dump(val_acc, f, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "    f.close()\n",
    "\n",
    "    f = open(save_path + '/epoch_acc.save', 'wb')\n",
    "    cPickle.dump(epoch_acc, f, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "    f.close()\n",
    "\n",
    "    array_epoch_acc = np.array(epoch_acc)\n",
    "    f = open(save_path + '/array_epoch_acc.save', 'wb')\n",
    "    cPickle.dump(array_epoch_acc, f, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "    f.close()\n",
    "\n",
    "    #f = open(save_path + '/test_acc.save', 'wb')\n",
    "    #cPickle.dump(test_acc, f, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "    #f.close()\n",
    "\n",
    "\n",
    "\n",
    "    torch.save(net, save_path+'/'+ net_name +'.pkl') # save whole net structure and params\n",
    "    torch.save(net.state_dict, save_path+'/'+ net_name +'_params.pkl') # only save model params\n",
    " \n",
    "    \n",
    "    \n",
    "    \n",
    "##################################################################################################    \n",
    "    print(\"now plotting accuracies and losses\")  \n",
    "    itern_axis_train = np.array(np.linspace(1,max_epoch,num=max_epoch))\n",
    "    itern_axis_test = np.array(np.linspace(1,max_epoch, num=max_epoch))\n",
    "\n",
    "    plt.plot(itern_axis_train, train_loss,'-b.', label='Train')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.savefig(save_path + '/train_loss' + str(max_epoch) + '.png')\n",
    "    \n",
    "    a = np.concatenate(array_epoch_acc)\n",
    "    length = a.shape[0]\n",
    "    plt.plot(length, a, '--r', label='Test')\n",
    "    plt.xlabel('iteration')\n",
    "    plt.ylabel('loss')\n",
    "    plt.savefig(save_path + '/testing_accuracy' + str(max_epoch) + '.png')\n",
    "    \n",
    "    return net\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n",
      "begin training\n",
      "Checking accuracy on test set\n",
      "Got 147 / 1000 correct (14.70)\n",
      "1 epoch,   500 iteration, loss:1.152\n",
      "Checking accuracy on test set\n",
      "Got 165 / 1000 correct (16.50)\n",
      "1 epoch,  1000 iteration, loss:1.149\n",
      " num 0 epoch \n",
      "####### Training Loss #######\n",
      "[2.2961514]\n",
      "Checking accuracy on test set\n",
      "Got 259 / 1000 correct (25.90)\n",
      "2 epoch,   500 iteration, loss:1.110\n",
      "Checking accuracy on test set\n",
      "Got 259 / 1000 correct (25.90)\n",
      "2 epoch,  1000 iteration, loss:1.083\n",
      " num 1 epoch \n",
      "####### Training Loss #######\n",
      "[2.17632085]\n",
      "Checking accuracy on test set\n",
      "Got 293 / 1000 correct (29.30)\n",
      "3 epoch,   500 iteration, loss:1.033\n",
      "Checking accuracy on test set\n",
      "Got 314 / 1000 correct (31.40)\n",
      "3 epoch,  1000 iteration, loss:1.026\n",
      " num 2 epoch \n",
      "####### Training Loss #######\n",
      "[2.04469434]\n",
      "Checking accuracy on test set\n",
      "Got 335 / 1000 correct (33.50)\n",
      "4 epoch,   500 iteration, loss:0.996\n",
      "Checking accuracy on test set\n",
      "Got 337 / 1000 correct (33.70)\n",
      "4 epoch,  1000 iteration, loss:0.997\n",
      " num 3 epoch \n",
      "####### Training Loss #######\n",
      "[1.98092401]\n",
      "Checking accuracy on test set\n",
      "Got 343 / 1000 correct (34.30)\n",
      "5 epoch,   500 iteration, loss:0.966\n",
      "Checking accuracy on test set\n",
      "Got 339 / 1000 correct (33.90)\n",
      "5 epoch,  1000 iteration, loss:0.960\n",
      " num 4 epoch \n",
      "####### Training Loss #######\n",
      "[1.90809192]\n",
      "Checking accuracy on test set\n",
      "Got 352 / 1000 correct (35.20)\n",
      "6 epoch,   500 iteration, loss:0.935\n",
      "Checking accuracy on test set\n",
      "Got 390 / 1000 correct (39.00)\n",
      "6 epoch,  1000 iteration, loss:0.932\n",
      " num 5 epoch \n",
      "####### Training Loss #######\n",
      "[1.8457622]\n",
      "Checking accuracy on test set\n",
      "Got 369 / 1000 correct (36.90)\n",
      "7 epoch,   500 iteration, loss:0.896\n",
      "Checking accuracy on test set\n",
      "Got 394 / 1000 correct (39.40)\n",
      "7 epoch,  1000 iteration, loss:0.894\n",
      " num 6 epoch \n",
      "####### Training Loss #######\n",
      "[1.77080297]\n",
      "Checking accuracy on test set\n",
      "Got 395 / 1000 correct (39.50)\n",
      "8 epoch,   500 iteration, loss:0.876\n",
      "Checking accuracy on test set\n",
      "Got 431 / 1000 correct (43.10)\n",
      "8 epoch,  1000 iteration, loss:0.860\n",
      " num 7 epoch \n",
      "####### Training Loss #######\n",
      "[1.7083402]\n",
      "Checking accuracy on test set\n",
      "Got 449 / 1000 correct (44.90)\n",
      "9 epoch,   500 iteration, loss:0.840\n",
      "Checking accuracy on test set\n",
      "Got 483 / 1000 correct (48.30)\n",
      "9 epoch,  1000 iteration, loss:0.817\n",
      " num 8 epoch \n",
      "####### Training Loss #######\n",
      "[1.62933167]\n",
      "Checking accuracy on test set\n",
      "Got 483 / 1000 correct (48.30)\n",
      "10 epoch,   500 iteration, loss:0.793\n",
      "Checking accuracy on test set\n",
      "Got 497 / 1000 correct (49.70)\n",
      "10 epoch,  1000 iteration, loss:0.778\n",
      " num 9 epoch \n",
      "####### Training Loss #######\n",
      "[1.54958734]\n",
      "Checking accuracy on test set\n",
      "Got 496 / 1000 correct (49.60)\n",
      "11 epoch,   500 iteration, loss:0.765\n",
      "Checking accuracy on test set\n",
      "Got 508 / 1000 correct (50.80)\n",
      "11 epoch,  1000 iteration, loss:0.756\n",
      " num 10 epoch \n",
      "####### Training Loss #######\n",
      "[1.49456743]\n",
      "Checking accuracy on test set\n",
      "Got 511 / 1000 correct (51.10)\n",
      "12 epoch,   500 iteration, loss:0.738\n",
      "Checking accuracy on test set\n",
      "Got 524 / 1000 correct (52.40)\n",
      "12 epoch,  1000 iteration, loss:0.730\n",
      " num 11 epoch \n",
      "####### Training Loss #######\n",
      "[1.44151493]\n",
      "Checking accuracy on test set\n",
      "Got 511 / 1000 correct (51.10)\n",
      "13 epoch,   500 iteration, loss:0.709\n",
      "Checking accuracy on test set\n",
      "Got 540 / 1000 correct (54.00)\n",
      "13 epoch,  1000 iteration, loss:0.698\n",
      " num 12 epoch \n",
      "####### Training Loss #######\n",
      "[1.38286061]\n",
      "Checking accuracy on test set\n",
      "Got 523 / 1000 correct (52.30)\n",
      "14 epoch,   500 iteration, loss:0.694\n",
      "Checking accuracy on test set\n",
      "Got 566 / 1000 correct (56.60)\n",
      "14 epoch,  1000 iteration, loss:0.685\n",
      " num 13 epoch \n",
      "####### Training Loss #######\n",
      "[1.35751588]\n",
      "Checking accuracy on test set\n",
      "Got 561 / 1000 correct (56.10)\n",
      "15 epoch,   500 iteration, loss:0.671\n",
      "Checking accuracy on test set\n",
      "Got 578 / 1000 correct (57.80)\n",
      "15 epoch,  1000 iteration, loss:0.659\n",
      " num 14 epoch \n",
      "####### Training Loss #######\n",
      "[1.30885598]\n",
      "Checking accuracy on test set\n",
      "Got 566 / 1000 correct (56.60)\n",
      "16 epoch,   500 iteration, loss:0.643\n",
      "Checking accuracy on test set\n",
      "Got 579 / 1000 correct (57.90)\n",
      "16 epoch,  1000 iteration, loss:0.630\n",
      " num 15 epoch \n",
      "####### Training Loss #######\n",
      "[1.25420019]\n",
      "Checking accuracy on test set\n",
      "Got 561 / 1000 correct (56.10)\n",
      "17 epoch,   500 iteration, loss:0.631\n",
      "Checking accuracy on test set\n",
      "Got 576 / 1000 correct (57.60)\n",
      "17 epoch,  1000 iteration, loss:0.627\n",
      " num 16 epoch \n",
      "####### Training Loss #######\n",
      "[1.23825265]\n",
      "Checking accuracy on test set\n",
      "Got 576 / 1000 correct (57.60)\n",
      "18 epoch,   500 iteration, loss:0.617\n",
      "Checking accuracy on test set\n",
      "Got 613 / 1000 correct (61.30)\n",
      "18 epoch,  1000 iteration, loss:0.601\n",
      " num 17 epoch \n",
      "####### Training Loss #######\n",
      "[1.19903303]\n",
      "Checking accuracy on test set\n",
      "Got 590 / 1000 correct (59.00)\n",
      "19 epoch,   500 iteration, loss:0.599\n",
      "Checking accuracy on test set\n",
      "Got 608 / 1000 correct (60.80)\n",
      "19 epoch,  1000 iteration, loss:0.591\n",
      " num 18 epoch \n",
      "####### Training Loss #######\n",
      "[1.16723771]\n",
      "Checking accuracy on test set\n",
      "Got 609 / 1000 correct (60.90)\n",
      "20 epoch,   500 iteration, loss:0.581\n",
      "Checking accuracy on test set\n",
      "Got 601 / 1000 correct (60.10)\n",
      "20 epoch,  1000 iteration, loss:0.567\n",
      " num 19 epoch \n",
      "####### Training Loss #######\n",
      "[1.12985561]\n",
      "Checking accuracy on test set\n",
      "Got 593 / 1000 correct (59.30)\n",
      "21 epoch,   500 iteration, loss:0.569\n",
      "Checking accuracy on test set\n",
      "Got 610 / 1000 correct (61.00)\n",
      "21 epoch,  1000 iteration, loss:0.563\n",
      " num 20 epoch \n",
      "####### Training Loss #######\n",
      "[1.10996598]\n",
      "Checking accuracy on test set\n",
      "Got 609 / 1000 correct (60.90)\n",
      "22 epoch,   500 iteration, loss:0.558\n",
      "Checking accuracy on test set\n",
      "Got 629 / 1000 correct (62.90)\n",
      "22 epoch,  1000 iteration, loss:0.539\n",
      " num 21 epoch \n",
      "####### Training Loss #######\n",
      "[1.07600211]\n",
      "Checking accuracy on test set\n",
      "Got 626 / 1000 correct (62.60)\n",
      "23 epoch,   500 iteration, loss:0.542\n",
      "Checking accuracy on test set\n",
      "Got 619 / 1000 correct (61.90)\n",
      "23 epoch,  1000 iteration, loss:0.527\n",
      " num 22 epoch \n",
      "####### Training Loss #######\n",
      "[1.04542074]\n",
      "Checking accuracy on test set\n",
      "Got 625 / 1000 correct (62.50)\n",
      "24 epoch,   500 iteration, loss:0.532\n",
      "Checking accuracy on test set\n",
      "Got 631 / 1000 correct (63.10)\n",
      "24 epoch,  1000 iteration, loss:0.520\n",
      " num 23 epoch \n",
      "####### Training Loss #######\n",
      "[1.02912647]\n",
      "Checking accuracy on test set\n",
      "Got 632 / 1000 correct (63.20)\n",
      "25 epoch,   500 iteration, loss:0.518\n",
      "Checking accuracy on test set\n",
      "Got 626 / 1000 correct (62.60)\n",
      "25 epoch,  1000 iteration, loss:0.507\n",
      " num 24 epoch \n",
      "####### Training Loss #######\n",
      "[1.00160806]\n",
      "Checking accuracy on test set\n",
      "Got 653 / 1000 correct (65.30)\n",
      "26 epoch,   500 iteration, loss:0.505\n",
      "Checking accuracy on test set\n",
      "Got 642 / 1000 correct (64.20)\n",
      "26 epoch,  1000 iteration, loss:0.495\n",
      " num 25 epoch \n",
      "####### Training Loss #######\n",
      "[0.97677731]\n",
      "Checking accuracy on test set\n",
      "Got 646 / 1000 correct (64.60)\n",
      "27 epoch,   500 iteration, loss:0.496\n",
      "Checking accuracy on test set\n",
      "Got 649 / 1000 correct (64.90)\n",
      "27 epoch,  1000 iteration, loss:0.485\n",
      " num 26 epoch \n",
      "####### Training Loss #######\n",
      "[0.95955801]\n",
      "Checking accuracy on test set\n",
      "Got 655 / 1000 correct (65.50)\n",
      "28 epoch,   500 iteration, loss:0.481\n",
      "Checking accuracy on test set\n",
      "Got 649 / 1000 correct (64.90)\n",
      "28 epoch,  1000 iteration, loss:0.473\n",
      " num 27 epoch \n",
      "####### Training Loss #######\n",
      "[0.93519037]\n",
      "Checking accuracy on test set\n",
      "Got 648 / 1000 correct (64.80)\n",
      "29 epoch,   500 iteration, loss:0.457\n",
      "Checking accuracy on test set\n",
      "Got 664 / 1000 correct (66.40)\n",
      "29 epoch,  1000 iteration, loss:0.452\n",
      " num 28 epoch \n",
      "####### Training Loss #######\n",
      "[0.89321771]\n",
      "Checking accuracy on test set\n",
      "Got 680 / 1000 correct (68.00)\n",
      "30 epoch,   500 iteration, loss:0.458\n",
      "Checking accuracy on test set\n",
      "Got 643 / 1000 correct (64.30)\n",
      "30 epoch,  1000 iteration, loss:0.444\n",
      " num 29 epoch \n",
      "####### Training Loss #######\n",
      "[0.88151194]\n",
      "Checking accuracy on test set\n",
      "Got 678 / 1000 correct (67.80)\n",
      "31 epoch,   500 iteration, loss:0.437\n",
      "Checking accuracy on test set\n",
      "Got 624 / 1000 correct (62.40)\n",
      "31 epoch,  1000 iteration, loss:0.422\n",
      " num 30 epoch \n",
      "####### Training Loss #######\n",
      "[0.84782784]\n",
      "Checking accuracy on test set\n",
      "Got 670 / 1000 correct (67.00)\n",
      "32 epoch,   500 iteration, loss:0.436\n",
      "Checking accuracy on test set\n",
      "Got 636 / 1000 correct (63.60)\n",
      "32 epoch,  1000 iteration, loss:0.428\n",
      " num 31 epoch \n",
      "####### Training Loss #######\n",
      "[0.84050599]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking accuracy on test set\n",
      "Got 634 / 1000 correct (63.40)\n",
      "33 epoch,   500 iteration, loss:0.428\n",
      "Checking accuracy on test set\n",
      "Got 640 / 1000 correct (64.00)\n",
      "33 epoch,  1000 iteration, loss:0.411\n",
      " num 32 epoch \n",
      "####### Training Loss #######\n",
      "[0.82390486]\n",
      "Checking accuracy on test set\n",
      "Got 658 / 1000 correct (65.80)\n",
      "34 epoch,   500 iteration, loss:0.411\n",
      "Checking accuracy on test set\n",
      "Got 637 / 1000 correct (63.70)\n",
      "34 epoch,  1000 iteration, loss:0.404\n",
      " num 33 epoch \n",
      "####### Training Loss #######\n",
      "[0.79793177]\n",
      "Checking accuracy on test set\n",
      "Got 669 / 1000 correct (66.90)\n",
      "35 epoch,   500 iteration, loss:0.394\n",
      "Checking accuracy on test set\n",
      "Got 647 / 1000 correct (64.70)\n",
      "35 epoch,  1000 iteration, loss:0.395\n",
      " num 34 epoch \n",
      "####### Training Loss #######\n",
      "[0.76979165]\n",
      "Checking accuracy on test set\n",
      "Got 644 / 1000 correct (64.40)\n",
      "36 epoch,   500 iteration, loss:0.384\n",
      "Checking accuracy on test set\n",
      "Got 667 / 1000 correct (66.70)\n",
      "36 epoch,  1000 iteration, loss:0.382\n",
      " num 35 epoch \n",
      "####### Training Loss #######\n",
      "[0.75104704]\n",
      "Checking accuracy on test set\n",
      "Got 662 / 1000 correct (66.20)\n",
      "37 epoch,   500 iteration, loss:0.381\n",
      "Checking accuracy on test set\n",
      "Got 647 / 1000 correct (64.70)\n",
      "37 epoch,  1000 iteration, loss:0.362\n",
      " num 36 epoch \n",
      "####### Training Loss #######\n",
      "[0.72711349]\n",
      "Checking accuracy on test set\n",
      "Got 659 / 1000 correct (65.90)\n",
      "38 epoch,   500 iteration, loss:0.370\n",
      "Checking accuracy on test set\n",
      "Got 656 / 1000 correct (65.60)\n",
      "38 epoch,  1000 iteration, loss:0.371\n",
      " num 37 epoch \n",
      "####### Training Loss #######\n",
      "[0.72219082]\n",
      "Checking accuracy on test set\n",
      "Got 629 / 1000 correct (62.90)\n",
      "39 epoch,   500 iteration, loss:0.344\n",
      "Checking accuracy on test set\n",
      "Got 683 / 1000 correct (68.30)\n",
      "39 epoch,  1000 iteration, loss:0.366\n",
      " num 38 epoch \n",
      "####### Training Loss #######\n",
      "[0.69221865]\n",
      "Checking accuracy on test set\n",
      "Got 668 / 1000 correct (66.80)\n",
      "40 epoch,   500 iteration, loss:0.351\n",
      "Checking accuracy on test set\n",
      "Got 652 / 1000 correct (65.20)\n",
      "40 epoch,  1000 iteration, loss:0.342\n",
      " num 39 epoch \n",
      "####### Training Loss #######\n",
      "[0.67791957]\n",
      "Checking accuracy on test set\n",
      "Got 692 / 1000 correct (69.20)\n",
      "41 epoch,   500 iteration, loss:0.353\n",
      "Checking accuracy on test set\n",
      "Got 652 / 1000 correct (65.20)\n",
      "41 epoch,  1000 iteration, loss:0.345\n",
      " num 40 epoch \n",
      "####### Training Loss #######\n",
      "[0.67375211]\n",
      "Checking accuracy on test set\n",
      "Got 678 / 1000 correct (67.80)\n",
      "42 epoch,   500 iteration, loss:0.337\n",
      "Checking accuracy on test set\n",
      "Got 663 / 1000 correct (66.30)\n",
      "42 epoch,  1000 iteration, loss:0.339\n",
      " num 41 epoch \n",
      "####### Training Loss #######\n",
      "[0.65922382]\n",
      "Checking accuracy on test set\n",
      "Got 694 / 1000 correct (69.40)\n",
      "43 epoch,   500 iteration, loss:0.319\n",
      "Checking accuracy on test set\n",
      "Got 684 / 1000 correct (68.40)\n",
      "43 epoch,  1000 iteration, loss:0.304\n",
      " num 42 epoch \n",
      "####### Training Loss #######\n",
      "[0.60889776]\n",
      "Checking accuracy on test set\n",
      "Got 690 / 1000 correct (69.00)\n",
      "44 epoch,   500 iteration, loss:0.304\n",
      "Checking accuracy on test set\n",
      "Got 657 / 1000 correct (65.70)\n",
      "44 epoch,  1000 iteration, loss:0.308\n",
      " num 43 epoch \n",
      "####### Training Loss #######\n",
      "[0.60309823]\n",
      "Checking accuracy on test set\n",
      "Got 682 / 1000 correct (68.20)\n",
      "45 epoch,   500 iteration, loss:0.306\n",
      "Checking accuracy on test set\n",
      "Got 666 / 1000 correct (66.60)\n",
      "45 epoch,  1000 iteration, loss:0.301\n",
      " num 44 epoch \n",
      "####### Training Loss #######\n",
      "[0.59953246]\n",
      "Checking accuracy on test set\n",
      "Got 674 / 1000 correct (67.40)\n",
      "46 epoch,   500 iteration, loss:0.285\n",
      "Checking accuracy on test set\n",
      "Got 659 / 1000 correct (65.90)\n",
      "46 epoch,  1000 iteration, loss:0.294\n",
      " num 45 epoch \n",
      "####### Training Loss #######\n",
      "[0.57071959]\n",
      "Checking accuracy on test set\n",
      "Got 676 / 1000 correct (67.60)\n",
      "47 epoch,   500 iteration, loss:0.294\n",
      "Checking accuracy on test set\n",
      "Got 674 / 1000 correct (67.40)\n",
      "47 epoch,  1000 iteration, loss:0.284\n",
      " num 46 epoch \n",
      "####### Training Loss #######\n",
      "[0.56764381]\n",
      "Checking accuracy on test set\n",
      "Got 688 / 1000 correct (68.80)\n",
      "48 epoch,   500 iteration, loss:0.274\n",
      "Checking accuracy on test set\n",
      "Got 682 / 1000 correct (68.20)\n",
      "48 epoch,  1000 iteration, loss:0.282\n",
      " num 47 epoch \n",
      "####### Training Loss #######\n",
      "[0.53630915]\n",
      "Checking accuracy on test set\n",
      "Got 666 / 1000 correct (66.60)\n",
      "49 epoch,   500 iteration, loss:0.275\n",
      "Checking accuracy on test set\n",
      "Got 667 / 1000 correct (66.70)\n",
      "49 epoch,  1000 iteration, loss:0.267\n",
      " num 48 epoch \n",
      "####### Training Loss #######\n",
      "[0.53039788]\n",
      "Checking accuracy on test set\n",
      "Got 673 / 1000 correct (67.30)\n",
      "50 epoch,   500 iteration, loss:0.263\n",
      "Checking accuracy on test set\n",
      "Got 649 / 1000 correct (64.90)\n",
      "50 epoch,  1000 iteration, loss:0.264\n",
      " num 49 epoch \n",
      "####### Training Loss #######\n",
      "[0.52576998]\n",
      "Checking accuracy on test set\n",
      "Got 692 / 1000 correct (69.20)\n",
      "51 epoch,   500 iteration, loss:0.265\n",
      "Checking accuracy on test set\n",
      "Got 666 / 1000 correct (66.60)\n",
      "51 epoch,  1000 iteration, loss:0.251\n",
      " num 50 epoch \n",
      "####### Training Loss #######\n",
      "[0.5087577]\n",
      "Checking accuracy on test set\n",
      "Got 674 / 1000 correct (67.40)\n",
      "52 epoch,   500 iteration, loss:0.252\n",
      "Checking accuracy on test set\n",
      "Got 659 / 1000 correct (65.90)\n",
      "52 epoch,  1000 iteration, loss:0.259\n",
      " num 51 epoch \n",
      "####### Training Loss #######\n",
      "[0.4948999]\n",
      "Checking accuracy on test set\n",
      "Got 656 / 1000 correct (65.60)\n",
      "53 epoch,   500 iteration, loss:0.246\n",
      "Checking accuracy on test set\n",
      "Got 660 / 1000 correct (66.00)\n",
      "53 epoch,  1000 iteration, loss:0.241\n",
      " num 52 epoch \n",
      "####### Training Loss #######\n",
      "[0.47913791]\n",
      "Checking accuracy on test set\n",
      "Got 673 / 1000 correct (67.30)\n",
      "54 epoch,   500 iteration, loss:0.240\n",
      "Checking accuracy on test set\n",
      "Got 674 / 1000 correct (67.40)\n",
      "54 epoch,  1000 iteration, loss:0.231\n",
      " num 53 epoch \n",
      "####### Training Loss #######\n",
      "[0.46650033]\n",
      "Checking accuracy on test set\n",
      "Got 654 / 1000 correct (65.40)\n",
      "55 epoch,   500 iteration, loss:0.233\n",
      "Checking accuracy on test set\n",
      "Got 681 / 1000 correct (68.10)\n",
      "55 epoch,  1000 iteration, loss:0.231\n",
      " num 54 epoch \n",
      "####### Training Loss #######\n",
      "[0.45068435]\n",
      "Checking accuracy on test set\n",
      "Got 660 / 1000 correct (66.00)\n",
      "56 epoch,   500 iteration, loss:0.235\n",
      "Checking accuracy on test set\n",
      "Got 681 / 1000 correct (68.10)\n",
      "56 epoch,  1000 iteration, loss:0.214\n",
      " num 55 epoch \n",
      "####### Training Loss #######\n",
      "[0.44350027]\n",
      "Checking accuracy on test set\n",
      "Got 649 / 1000 correct (64.90)\n",
      "57 epoch,   500 iteration, loss:0.211\n",
      "Checking accuracy on test set\n",
      "Got 641 / 1000 correct (64.10)\n",
      "57 epoch,  1000 iteration, loss:0.216\n",
      " num 56 epoch \n",
      "####### Training Loss #######\n",
      "[0.42064999]\n",
      "Checking accuracy on test set\n",
      "Got 689 / 1000 correct (68.90)\n",
      "58 epoch,   500 iteration, loss:0.224\n",
      "Checking accuracy on test set\n",
      "Got 665 / 1000 correct (66.50)\n",
      "58 epoch,  1000 iteration, loss:0.212\n",
      " num 57 epoch \n",
      "####### Training Loss #######\n",
      "[0.42692001]\n",
      "Checking accuracy on test set\n",
      "Got 688 / 1000 correct (68.80)\n",
      "59 epoch,   500 iteration, loss:0.198\n",
      "Checking accuracy on test set\n",
      "Got 686 / 1000 correct (68.60)\n",
      "59 epoch,  1000 iteration, loss:0.196\n",
      " num 58 epoch \n",
      "####### Training Loss #######\n",
      "[0.39024797]\n",
      "Checking accuracy on test set\n",
      "Got 691 / 1000 correct (69.10)\n",
      "60 epoch,   500 iteration, loss:0.189\n",
      "Checking accuracy on test set\n",
      "Got 657 / 1000 correct (65.70)\n",
      "60 epoch,  1000 iteration, loss:0.205\n",
      " num 59 epoch \n",
      "####### Training Loss #######\n",
      "[0.38881568]\n",
      "Checking accuracy on test set\n",
      "Got 665 / 1000 correct (66.50)\n",
      "61 epoch,   500 iteration, loss:0.191\n",
      "Checking accuracy on test set\n",
      "Got 685 / 1000 correct (68.50)\n",
      "61 epoch,  1000 iteration, loss:0.189\n",
      " num 60 epoch \n",
      "####### Training Loss #######\n",
      "[0.37152217]\n",
      "Checking accuracy on test set\n",
      "Got 698 / 1000 correct (69.80)\n",
      "62 epoch,   500 iteration, loss:0.188\n",
      "Checking accuracy on test set\n",
      "Got 642 / 1000 correct (64.20)\n",
      "62 epoch,  1000 iteration, loss:0.201\n",
      " num 61 epoch \n",
      "####### Training Loss #######\n",
      "[0.38533414]\n",
      "Checking accuracy on test set\n",
      "Got 691 / 1000 correct (69.10)\n",
      "63 epoch,   500 iteration, loss:0.188\n",
      "Checking accuracy on test set\n",
      "Got 660 / 1000 correct (66.00)\n",
      "63 epoch,  1000 iteration, loss:0.192\n",
      " num 62 epoch \n",
      "####### Training Loss #######\n",
      "[0.37316727]\n",
      "Checking accuracy on test set\n",
      "Got 673 / 1000 correct (67.30)\n",
      "64 epoch,   500 iteration, loss:0.173\n",
      "Checking accuracy on test set\n",
      "Got 677 / 1000 correct (67.70)\n",
      "64 epoch,  1000 iteration, loss:0.193\n",
      " num 63 epoch \n",
      "####### Training Loss #######\n",
      "[0.36399906]\n",
      "Checking accuracy on test set\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 690 / 1000 correct (69.00)\n",
      "65 epoch,   500 iteration, loss:0.167\n",
      "Checking accuracy on test set\n",
      "Got 682 / 1000 correct (68.20)\n",
      "65 epoch,  1000 iteration, loss:0.189\n",
      " num 64 epoch \n",
      "####### Training Loss #######\n",
      "[0.34476063]\n",
      "Checking accuracy on test set\n",
      "Got 668 / 1000 correct (66.80)\n",
      "66 epoch,   500 iteration, loss:0.168\n",
      "Checking accuracy on test set\n",
      "Got 670 / 1000 correct (67.00)\n",
      "66 epoch,  1000 iteration, loss:0.172\n",
      " num 65 epoch \n",
      "####### Training Loss #######\n",
      "[0.33270323]\n",
      "Checking accuracy on test set\n",
      "Got 680 / 1000 correct (68.00)\n",
      "67 epoch,   500 iteration, loss:0.180\n",
      "Checking accuracy on test set\n",
      "Got 671 / 1000 correct (67.10)\n",
      "67 epoch,  1000 iteration, loss:0.164\n",
      " num 66 epoch \n",
      "####### Training Loss #######\n",
      "[0.33383038]\n",
      "Checking accuracy on test set\n",
      "Got 657 / 1000 correct (65.70)\n",
      "68 epoch,   500 iteration, loss:0.173\n",
      "Checking accuracy on test set\n",
      "Got 647 / 1000 correct (64.70)\n",
      "68 epoch,  1000 iteration, loss:0.175\n",
      " num 67 epoch \n",
      "####### Training Loss #######\n",
      "[0.33228206]\n",
      "Checking accuracy on test set\n",
      "Got 674 / 1000 correct (67.40)\n",
      "69 epoch,   500 iteration, loss:0.169\n",
      "Checking accuracy on test set\n",
      "Got 690 / 1000 correct (69.00)\n",
      "69 epoch,  1000 iteration, loss:0.165\n",
      " num 68 epoch \n",
      "####### Training Loss #######\n",
      "[0.32681619]\n",
      "Checking accuracy on test set\n",
      "Got 671 / 1000 correct (67.10)\n",
      "70 epoch,   500 iteration, loss:0.153\n",
      "Checking accuracy on test set\n",
      "Got 660 / 1000 correct (66.00)\n",
      "70 epoch,  1000 iteration, loss:0.158\n",
      " num 69 epoch \n",
      "####### Training Loss #######\n",
      "[0.3098098]\n",
      "Checking accuracy on test set\n",
      "Got 681 / 1000 correct (68.10)\n",
      "71 epoch,   500 iteration, loss:0.141\n",
      "Checking accuracy on test set\n",
      "Got 680 / 1000 correct (68.00)\n",
      "71 epoch,  1000 iteration, loss:0.162\n",
      " num 70 epoch \n",
      "####### Training Loss #######\n",
      "[0.29540409]\n",
      "Checking accuracy on test set\n",
      "Got 645 / 1000 correct (64.50)\n",
      "72 epoch,   500 iteration, loss:0.156\n",
      "Checking accuracy on test set\n",
      "Got 666 / 1000 correct (66.60)\n",
      "72 epoch,  1000 iteration, loss:0.147\n",
      " num 71 epoch \n",
      "####### Training Loss #######\n",
      "[0.31379855]\n",
      "Checking accuracy on test set\n",
      "Got 645 / 1000 correct (64.50)\n",
      "73 epoch,   500 iteration, loss:0.145\n",
      "Checking accuracy on test set\n",
      "Got 669 / 1000 correct (66.90)\n",
      "73 epoch,  1000 iteration, loss:0.145\n",
      " num 72 epoch \n",
      "####### Training Loss #######\n",
      "[0.28494959]\n",
      "Checking accuracy on test set\n",
      "Got 659 / 1000 correct (65.90)\n",
      "74 epoch,   500 iteration, loss:0.155\n",
      "Checking accuracy on test set\n",
      "Got 652 / 1000 correct (65.20)\n",
      "74 epoch,  1000 iteration, loss:0.141\n",
      " num 73 epoch \n",
      "####### Training Loss #######\n",
      "[0.29107377]\n",
      "Checking accuracy on test set\n",
      "Got 691 / 1000 correct (69.10)\n",
      "75 epoch,   500 iteration, loss:0.147\n",
      "Checking accuracy on test set\n",
      "Got 656 / 1000 correct (65.60)\n",
      "75 epoch,  1000 iteration, loss:0.147\n",
      " num 74 epoch \n",
      "####### Training Loss #######\n",
      "[0.2859837]\n",
      "Checking accuracy on test set\n",
      "Got 667 / 1000 correct (66.70)\n",
      "76 epoch,   500 iteration, loss:0.142\n",
      "Checking accuracy on test set\n",
      "Got 676 / 1000 correct (67.60)\n",
      "76 epoch,  1000 iteration, loss:0.136\n",
      " num 75 epoch \n",
      "####### Training Loss #######\n",
      "[0.28035146]\n",
      "Checking accuracy on test set\n",
      "Got 681 / 1000 correct (68.10)\n",
      "77 epoch,   500 iteration, loss:0.126\n",
      "Checking accuracy on test set\n",
      "Got 671 / 1000 correct (67.10)\n",
      "77 epoch,  1000 iteration, loss:0.139\n",
      " num 76 epoch \n",
      "####### Training Loss #######\n",
      "[0.26619255]\n",
      "Checking accuracy on test set\n",
      "Got 688 / 1000 correct (68.80)\n",
      "78 epoch,   500 iteration, loss:0.120\n",
      "Checking accuracy on test set\n",
      "Got 657 / 1000 correct (65.70)\n",
      "78 epoch,  1000 iteration, loss:0.119\n",
      " num 77 epoch \n",
      "####### Training Loss #######\n",
      "[0.23964522]\n",
      "Checking accuracy on test set\n",
      "Got 680 / 1000 correct (68.00)\n",
      "79 epoch,   500 iteration, loss:0.144\n",
      "Checking accuracy on test set\n",
      "Got 673 / 1000 correct (67.30)\n",
      "79 epoch,  1000 iteration, loss:0.136\n",
      " num 78 epoch \n",
      "####### Training Loss #######\n",
      "[0.27670894]\n",
      "Checking accuracy on test set\n",
      "Got 646 / 1000 correct (64.60)\n",
      "80 epoch,   500 iteration, loss:0.129\n",
      "Checking accuracy on test set\n",
      "Got 681 / 1000 correct (68.10)\n",
      "80 epoch,  1000 iteration, loss:0.124\n",
      " num 79 epoch \n",
      "####### Training Loss #######\n",
      "[0.25299187]\n",
      "Checking accuracy on test set\n",
      "Got 657 / 1000 correct (65.70)\n",
      "81 epoch,   500 iteration, loss:0.116\n",
      "Checking accuracy on test set\n",
      "Got 634 / 1000 correct (63.40)\n",
      "81 epoch,  1000 iteration, loss:0.135\n",
      " num 80 epoch \n",
      "####### Training Loss #######\n",
      "[0.24198572]\n",
      "Checking accuracy on test set\n",
      "Got 674 / 1000 correct (67.40)\n",
      "82 epoch,   500 iteration, loss:0.131\n",
      "Checking accuracy on test set\n",
      "Got 671 / 1000 correct (67.10)\n",
      "82 epoch,  1000 iteration, loss:0.132\n",
      " num 81 epoch \n",
      "####### Training Loss #######\n",
      "[0.25205591]\n",
      "Checking accuracy on test set\n",
      "Got 681 / 1000 correct (68.10)\n",
      "83 epoch,   500 iteration, loss:0.130\n",
      "Checking accuracy on test set\n",
      "Got 653 / 1000 correct (65.30)\n",
      "83 epoch,  1000 iteration, loss:0.115\n",
      " num 82 epoch \n",
      "####### Training Loss #######\n",
      "[0.24758349]\n",
      "Checking accuracy on test set\n",
      "Got 689 / 1000 correct (68.90)\n",
      "84 epoch,   500 iteration, loss:0.119\n",
      "Checking accuracy on test set\n",
      "Got 663 / 1000 correct (66.30)\n",
      "84 epoch,  1000 iteration, loss:0.133\n",
      " num 83 epoch \n",
      "####### Training Loss #######\n",
      "[0.25898943]\n",
      "Checking accuracy on test set\n",
      "Got 669 / 1000 correct (66.90)\n",
      "85 epoch,   500 iteration, loss:0.123\n",
      "Checking accuracy on test set\n",
      "Got 675 / 1000 correct (67.50)\n",
      "85 epoch,  1000 iteration, loss:0.135\n",
      " num 84 epoch \n",
      "####### Training Loss #######\n",
      "[0.25103824]\n",
      "Checking accuracy on test set\n",
      "Got 675 / 1000 correct (67.50)\n",
      "86 epoch,   500 iteration, loss:0.126\n",
      "Checking accuracy on test set\n",
      "Got 679 / 1000 correct (67.90)\n",
      "86 epoch,  1000 iteration, loss:0.114\n",
      " num 85 epoch \n",
      "####### Training Loss #######\n",
      "[0.23477796]\n",
      "Checking accuracy on test set\n",
      "Got 689 / 1000 correct (68.90)\n",
      "87 epoch,   500 iteration, loss:0.113\n",
      "Checking accuracy on test set\n",
      "Got 658 / 1000 correct (65.80)\n",
      "87 epoch,  1000 iteration, loss:0.119\n",
      " num 86 epoch \n",
      "####### Training Loss #######\n",
      "[0.23324881]\n",
      "Checking accuracy on test set\n",
      "Got 656 / 1000 correct (65.60)\n",
      "88 epoch,   500 iteration, loss:0.119\n",
      "Checking accuracy on test set\n",
      "Got 684 / 1000 correct (68.40)\n",
      "88 epoch,  1000 iteration, loss:0.112\n",
      " num 87 epoch \n",
      "####### Training Loss #######\n",
      "[0.22308375]\n",
      "Checking accuracy on test set\n",
      "Got 677 / 1000 correct (67.70)\n",
      "89 epoch,   500 iteration, loss:0.112\n",
      "Checking accuracy on test set\n",
      "Got 650 / 1000 correct (65.00)\n",
      "89 epoch,  1000 iteration, loss:0.114\n",
      " num 88 epoch \n",
      "####### Training Loss #######\n",
      "[0.23150896]\n",
      "Checking accuracy on test set\n",
      "Got 666 / 1000 correct (66.60)\n",
      "90 epoch,   500 iteration, loss:0.116\n",
      "Checking accuracy on test set\n",
      "Got 670 / 1000 correct (67.00)\n",
      "90 epoch,  1000 iteration, loss:0.105\n",
      " num 89 epoch \n",
      "####### Training Loss #######\n",
      "[0.21470729]\n",
      "Checking accuracy on test set\n",
      "Got 651 / 1000 correct (65.10)\n",
      "91 epoch,   500 iteration, loss:0.104\n",
      "Checking accuracy on test set\n",
      "Got 678 / 1000 correct (67.80)\n",
      "91 epoch,  1000 iteration, loss:0.128\n",
      " num 90 epoch \n",
      "####### Training Loss #######\n",
      "[0.22344953]\n",
      "Checking accuracy on test set\n",
      "Got 658 / 1000 correct (65.80)\n",
      "92 epoch,   500 iteration, loss:0.110\n",
      "Checking accuracy on test set\n",
      "Got 695 / 1000 correct (69.50)\n",
      "92 epoch,  1000 iteration, loss:0.114\n",
      " num 91 epoch \n",
      "####### Training Loss #######\n",
      "[0.21610769]\n",
      "Checking accuracy on test set\n",
      "Got 682 / 1000 correct (68.20)\n",
      "93 epoch,   500 iteration, loss:0.091\n",
      "Checking accuracy on test set\n",
      "Got 667 / 1000 correct (66.70)\n",
      "93 epoch,  1000 iteration, loss:0.096\n",
      " num 92 epoch \n",
      "####### Training Loss #######\n",
      "[0.19280175]\n",
      "Checking accuracy on test set\n",
      "Got 671 / 1000 correct (67.10)\n",
      "94 epoch,   500 iteration, loss:0.122\n",
      "Checking accuracy on test set\n",
      "Got 669 / 1000 correct (66.90)\n",
      "94 epoch,  1000 iteration, loss:0.108\n",
      " num 93 epoch \n",
      "####### Training Loss #######\n",
      "[0.21413213]\n",
      "Checking accuracy on test set\n",
      "Got 668 / 1000 correct (66.80)\n",
      "95 epoch,   500 iteration, loss:0.100\n",
      "Checking accuracy on test set\n",
      "Got 659 / 1000 correct (65.90)\n",
      "95 epoch,  1000 iteration, loss:0.098\n",
      " num 94 epoch \n",
      "####### Training Loss #######\n",
      "[0.19878176]\n",
      "Checking accuracy on test set\n",
      "Got 667 / 1000 correct (66.70)\n",
      "96 epoch,   500 iteration, loss:0.101\n",
      "Checking accuracy on test set\n",
      "Got 675 / 1000 correct (67.50)\n",
      "96 epoch,  1000 iteration, loss:0.097\n",
      " num 95 epoch \n",
      "####### Training Loss #######\n",
      "[0.1958765]\n",
      "Checking accuracy on test set\n",
      "Got 672 / 1000 correct (67.20)\n",
      "97 epoch,   500 iteration, loss:0.095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking accuracy on test set\n",
      "Got 662 / 1000 correct (66.20)\n",
      "97 epoch,  1000 iteration, loss:0.102\n",
      " num 96 epoch \n",
      "####### Training Loss #######\n",
      "[0.19448034]\n",
      "Checking accuracy on test set\n",
      "Got 686 / 1000 correct (68.60)\n",
      "98 epoch,   500 iteration, loss:0.093\n",
      "Checking accuracy on test set\n",
      "Got 678 / 1000 correct (67.80)\n",
      "98 epoch,  1000 iteration, loss:0.100\n",
      " num 97 epoch \n",
      "####### Training Loss #######\n",
      "[0.1913845]\n",
      "Checking accuracy on test set\n",
      "Got 676 / 1000 correct (67.60)\n",
      "99 epoch,   500 iteration, loss:0.100\n",
      "Checking accuracy on test set\n",
      "Got 705 / 1000 correct (70.50)\n",
      "99 epoch,  1000 iteration, loss:0.094\n",
      " num 98 epoch \n",
      "####### Training Loss #######\n",
      "[0.19422606]\n",
      "Checking accuracy on test set\n",
      "Got 698 / 1000 correct (69.80)\n",
      "100 epoch,   500 iteration, loss:0.095\n",
      "Checking accuracy on test set\n",
      "Got 686 / 1000 correct (68.60)\n",
      "100 epoch,  1000 iteration, loss:0.078\n",
      " num 99 epoch \n",
      "####### Training Loss #######\n",
      "[0.17310048]\n",
      "Checking accuracy on test set\n",
      "Got 664 / 1000 correct (66.40)\n",
      "101 epoch,   500 iteration, loss:0.091\n",
      "Checking accuracy on test set\n",
      "Got 683 / 1000 correct (68.30)\n",
      "101 epoch,  1000 iteration, loss:0.080\n",
      " num 100 epoch \n",
      "####### Training Loss #######\n",
      "[0.17545863]\n",
      "Checking accuracy on test set\n",
      "Got 658 / 1000 correct (65.80)\n",
      "102 epoch,   500 iteration, loss:0.093\n",
      "Checking accuracy on test set\n",
      "Got 648 / 1000 correct (64.80)\n",
      "102 epoch,  1000 iteration, loss:0.094\n",
      " num 101 epoch \n",
      "####### Training Loss #######\n",
      "[0.18540655]\n",
      "Checking accuracy on test set\n",
      "Got 680 / 1000 correct (68.00)\n",
      "103 epoch,   500 iteration, loss:0.080\n",
      "Checking accuracy on test set\n",
      "Got 678 / 1000 correct (67.80)\n",
      "103 epoch,  1000 iteration, loss:0.092\n",
      " num 102 epoch \n",
      "####### Training Loss #######\n",
      "[0.17509823]\n",
      "Checking accuracy on test set\n",
      "Got 657 / 1000 correct (65.70)\n",
      "104 epoch,   500 iteration, loss:0.091\n",
      "Checking accuracy on test set\n",
      "Got 672 / 1000 correct (67.20)\n",
      "104 epoch,  1000 iteration, loss:0.110\n",
      " num 103 epoch \n",
      "####### Training Loss #######\n",
      "[0.20192271]\n",
      "Checking accuracy on test set\n",
      "Got 665 / 1000 correct (66.50)\n",
      "105 epoch,   500 iteration, loss:0.095\n",
      "Checking accuracy on test set\n",
      "Got 676 / 1000 correct (67.60)\n",
      "105 epoch,  1000 iteration, loss:0.088\n",
      " num 104 epoch \n",
      "####### Training Loss #######\n",
      "[0.18117772]\n",
      "Checking accuracy on test set\n",
      "Got 670 / 1000 correct (67.00)\n",
      "106 epoch,   500 iteration, loss:0.083\n",
      "Checking accuracy on test set\n",
      "Got 663 / 1000 correct (66.30)\n",
      "106 epoch,  1000 iteration, loss:0.089\n",
      " num 105 epoch \n",
      "####### Training Loss #######\n",
      "[0.17854635]\n",
      "Checking accuracy on test set\n",
      "Got 681 / 1000 correct (68.10)\n",
      "107 epoch,   500 iteration, loss:0.088\n",
      "Checking accuracy on test set\n",
      "Got 685 / 1000 correct (68.50)\n",
      "107 epoch,  1000 iteration, loss:0.077\n",
      " num 106 epoch \n",
      "####### Training Loss #######\n",
      "[0.15444643]\n",
      "Checking accuracy on test set\n",
      "Got 676 / 1000 correct (67.60)\n",
      "108 epoch,   500 iteration, loss:0.077\n",
      "Checking accuracy on test set\n",
      "Got 673 / 1000 correct (67.30)\n",
      "108 epoch,  1000 iteration, loss:0.089\n",
      " num 107 epoch \n",
      "####### Training Loss #######\n",
      "[0.16402044]\n",
      "Checking accuracy on test set\n",
      "Got 686 / 1000 correct (68.60)\n",
      "109 epoch,   500 iteration, loss:0.087\n",
      "Checking accuracy on test set\n",
      "Got 682 / 1000 correct (68.20)\n",
      "109 epoch,  1000 iteration, loss:0.085\n",
      " num 108 epoch \n",
      "####### Training Loss #######\n",
      "[0.17463507]\n",
      "Checking accuracy on test set\n",
      "Got 687 / 1000 correct (68.70)\n",
      "110 epoch,   500 iteration, loss:0.101\n",
      "Checking accuracy on test set\n",
      "Got 673 / 1000 correct (67.30)\n",
      "110 epoch,  1000 iteration, loss:0.094\n",
      " num 109 epoch \n",
      "####### Training Loss #######\n",
      "[0.19660617]\n",
      "Checking accuracy on test set\n",
      "Got 697 / 1000 correct (69.70)\n",
      "111 epoch,   500 iteration, loss:0.088\n",
      "Checking accuracy on test set\n",
      "Got 663 / 1000 correct (66.30)\n",
      "111 epoch,  1000 iteration, loss:0.080\n",
      " num 110 epoch \n",
      "####### Training Loss #######\n",
      "[0.16628907]\n",
      "Checking accuracy on test set\n",
      "Got 708 / 1000 correct (70.80)\n",
      "112 epoch,   500 iteration, loss:0.076\n",
      "Checking accuracy on test set\n",
      "Got 692 / 1000 correct (69.20)\n",
      "112 epoch,  1000 iteration, loss:0.088\n",
      " num 111 epoch \n",
      "####### Training Loss #######\n",
      "[0.16614064]\n",
      "Checking accuracy on test set\n",
      "Got 666 / 1000 correct (66.60)\n",
      "113 epoch,   500 iteration, loss:0.085\n",
      "Checking accuracy on test set\n",
      "Got 672 / 1000 correct (67.20)\n",
      "113 epoch,  1000 iteration, loss:0.078\n",
      " num 112 epoch \n",
      "####### Training Loss #######\n",
      "[0.1593793]\n",
      "Checking accuracy on test set\n",
      "Got 687 / 1000 correct (68.70)\n",
      "114 epoch,   500 iteration, loss:0.088\n",
      "Checking accuracy on test set\n",
      "Got 647 / 1000 correct (64.70)\n",
      "114 epoch,  1000 iteration, loss:0.067\n",
      " num 113 epoch \n",
      "####### Training Loss #######\n",
      "[0.15184362]\n",
      "Checking accuracy on test set\n",
      "Got 696 / 1000 correct (69.60)\n",
      "115 epoch,   500 iteration, loss:0.073\n",
      "Checking accuracy on test set\n",
      "Got 686 / 1000 correct (68.60)\n",
      "115 epoch,  1000 iteration, loss:0.075\n",
      " num 114 epoch \n",
      "####### Training Loss #######\n",
      "[0.15322704]\n",
      "Checking accuracy on test set\n",
      "Got 684 / 1000 correct (68.40)\n",
      "116 epoch,   500 iteration, loss:0.091\n",
      "Checking accuracy on test set\n",
      "Got 668 / 1000 correct (66.80)\n",
      "116 epoch,  1000 iteration, loss:0.072\n",
      " num 115 epoch \n",
      "####### Training Loss #######\n",
      "[0.16654667]\n",
      "Checking accuracy on test set\n",
      "Got 695 / 1000 correct (69.50)\n",
      "117 epoch,   500 iteration, loss:0.082\n",
      "Checking accuracy on test set\n",
      "Got 690 / 1000 correct (69.00)\n",
      "117 epoch,  1000 iteration, loss:0.078\n",
      " num 116 epoch \n",
      "####### Training Loss #######\n",
      "[0.15377072]\n",
      "Checking accuracy on test set\n",
      "Got 683 / 1000 correct (68.30)\n",
      "118 epoch,   500 iteration, loss:0.088\n",
      "Checking accuracy on test set\n",
      "Got 669 / 1000 correct (66.90)\n",
      "118 epoch,  1000 iteration, loss:0.075\n",
      " num 117 epoch \n",
      "####### Training Loss #######\n",
      "[0.15810923]\n",
      "Checking accuracy on test set\n",
      "Got 685 / 1000 correct (68.50)\n",
      "119 epoch,   500 iteration, loss:0.086\n",
      "Checking accuracy on test set\n",
      "Got 664 / 1000 correct (66.40)\n",
      "119 epoch,  1000 iteration, loss:0.080\n",
      " num 118 epoch \n",
      "####### Training Loss #######\n",
      "[0.16604004]\n",
      "Checking accuracy on test set\n",
      "Got 679 / 1000 correct (67.90)\n",
      "120 epoch,   500 iteration, loss:0.072\n",
      "Checking accuracy on test set\n",
      "Got 698 / 1000 correct (69.80)\n",
      "120 epoch,  1000 iteration, loss:0.087\n",
      " num 119 epoch \n",
      "####### Training Loss #######\n",
      "[0.14679844]\n",
      "Checking accuracy on test set\n",
      "Got 700 / 1000 correct (70.00)\n",
      "121 epoch,   500 iteration, loss:0.072\n",
      "Checking accuracy on test set\n",
      "Got 672 / 1000 correct (67.20)\n",
      "121 epoch,  1000 iteration, loss:0.062\n",
      " num 120 epoch \n",
      "####### Training Loss #######\n",
      "[0.13862854]\n",
      "Checking accuracy on test set\n",
      "Got 691 / 1000 correct (69.10)\n",
      "122 epoch,   500 iteration, loss:0.066\n",
      "Checking accuracy on test set\n",
      "Got 659 / 1000 correct (65.90)\n",
      "122 epoch,  1000 iteration, loss:0.076\n",
      " num 121 epoch \n",
      "####### Training Loss #######\n",
      "[0.15010488]\n",
      "Checking accuracy on test set\n",
      "Got 671 / 1000 correct (67.10)\n",
      "123 epoch,   500 iteration, loss:0.079\n",
      "Checking accuracy on test set\n",
      "Got 673 / 1000 correct (67.30)\n",
      "123 epoch,  1000 iteration, loss:0.079\n",
      " num 122 epoch \n",
      "####### Training Loss #######\n",
      "[0.1570117]\n",
      "Checking accuracy on test set\n",
      "Got 698 / 1000 correct (69.80)\n",
      "124 epoch,   500 iteration, loss:0.082\n",
      "Checking accuracy on test set\n",
      "Got 690 / 1000 correct (69.00)\n",
      "124 epoch,  1000 iteration, loss:0.080\n",
      " num 123 epoch \n",
      "####### Training Loss #######\n",
      "[0.16022426]\n",
      "Checking accuracy on test set\n",
      "Got 675 / 1000 correct (67.50)\n",
      "125 epoch,   500 iteration, loss:0.066\n",
      "Checking accuracy on test set\n",
      "Got 675 / 1000 correct (67.50)\n",
      "125 epoch,  1000 iteration, loss:0.081\n",
      " num 124 epoch \n",
      "####### Training Loss #######\n",
      "[0.14317838]\n",
      "Checking accuracy on test set\n",
      "Got 687 / 1000 correct (68.70)\n",
      "126 epoch,   500 iteration, loss:0.076\n",
      "Checking accuracy on test set\n",
      "Got 696 / 1000 correct (69.60)\n",
      "126 epoch,  1000 iteration, loss:0.074\n",
      " num 125 epoch \n",
      "####### Training Loss #######\n",
      "[0.1516152]\n",
      "Checking accuracy on test set\n",
      "Got 689 / 1000 correct (68.90)\n",
      "127 epoch,   500 iteration, loss:0.079\n",
      "Checking accuracy on test set\n",
      "Got 674 / 1000 correct (67.40)\n",
      "127 epoch,  1000 iteration, loss:0.074\n",
      " num 126 epoch \n",
      "####### Training Loss #######\n",
      "[0.15134202]\n",
      "Checking accuracy on test set\n",
      "Got 670 / 1000 correct (67.00)\n",
      "128 epoch,   500 iteration, loss:0.073\n",
      "Checking accuracy on test set\n",
      "Got 677 / 1000 correct (67.70)\n",
      "128 epoch,  1000 iteration, loss:0.078\n",
      " num 127 epoch \n",
      "####### Training Loss #######\n",
      "[0.14708566]\n",
      "Checking accuracy on test set\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 655 / 1000 correct (65.50)\n",
      "129 epoch,   500 iteration, loss:0.073\n",
      "Checking accuracy on test set\n",
      "Got 661 / 1000 correct (66.10)\n",
      "129 epoch,  1000 iteration, loss:0.070\n",
      " num 128 epoch \n",
      "####### Training Loss #######\n",
      "[0.14299123]\n",
      "Checking accuracy on test set\n",
      "Got 679 / 1000 correct (67.90)\n",
      "130 epoch,   500 iteration, loss:0.074\n",
      "Checking accuracy on test set\n",
      "Got 662 / 1000 correct (66.20)\n",
      "130 epoch,  1000 iteration, loss:0.070\n",
      " num 129 epoch \n",
      "####### Training Loss #######\n",
      "[0.14168456]\n",
      "Checking accuracy on test set\n",
      "Got 685 / 1000 correct (68.50)\n",
      "131 epoch,   500 iteration, loss:0.073\n",
      "Checking accuracy on test set\n",
      "Got 678 / 1000 correct (67.80)\n",
      "131 epoch,  1000 iteration, loss:0.070\n",
      " num 130 epoch \n",
      "####### Training Loss #######\n",
      "[0.14455018]\n",
      "Checking accuracy on test set\n",
      "Got 679 / 1000 correct (67.90)\n",
      "132 epoch,   500 iteration, loss:0.071\n",
      "Checking accuracy on test set\n",
      "Got 685 / 1000 correct (68.50)\n",
      "132 epoch,  1000 iteration, loss:0.071\n",
      " num 131 epoch \n",
      "####### Training Loss #######\n",
      "[0.14213467]\n",
      "Checking accuracy on test set\n",
      "Got 691 / 1000 correct (69.10)\n",
      "133 epoch,   500 iteration, loss:0.068\n",
      "Checking accuracy on test set\n",
      "Got 649 / 1000 correct (64.90)\n",
      "133 epoch,  1000 iteration, loss:0.071\n",
      " num 132 epoch \n",
      "####### Training Loss #######\n",
      "[0.14335059]\n",
      "Checking accuracy on test set\n",
      "Got 675 / 1000 correct (67.50)\n",
      "134 epoch,   500 iteration, loss:0.062\n",
      "Checking accuracy on test set\n",
      "Got 685 / 1000 correct (68.50)\n",
      "134 epoch,  1000 iteration, loss:0.062\n",
      " num 133 epoch \n",
      "####### Training Loss #######\n",
      "[0.1281972]\n",
      "Checking accuracy on test set\n",
      "Got 685 / 1000 correct (68.50)\n",
      "135 epoch,   500 iteration, loss:0.053\n",
      "Checking accuracy on test set\n",
      "Got 667 / 1000 correct (66.70)\n",
      "135 epoch,  1000 iteration, loss:0.057\n",
      " num 134 epoch \n",
      "####### Training Loss #######\n",
      "[0.11187198]\n",
      "Checking accuracy on test set\n",
      "Got 673 / 1000 correct (67.30)\n",
      "136 epoch,   500 iteration, loss:0.070\n",
      "Checking accuracy on test set\n",
      "Got 668 / 1000 correct (66.80)\n",
      "136 epoch,  1000 iteration, loss:0.067\n",
      " num 135 epoch \n",
      "####### Training Loss #######\n",
      "[0.14604872]\n",
      "Checking accuracy on test set\n",
      "Got 671 / 1000 correct (67.10)\n",
      "137 epoch,   500 iteration, loss:0.069\n",
      "Checking accuracy on test set\n",
      "Got 670 / 1000 correct (67.00)\n",
      "137 epoch,  1000 iteration, loss:0.069\n",
      " num 136 epoch \n",
      "####### Training Loss #######\n",
      "[0.14271914]\n",
      "Checking accuracy on test set\n",
      "Got 695 / 1000 correct (69.50)\n",
      "138 epoch,   500 iteration, loss:0.065\n",
      "Checking accuracy on test set\n",
      "Got 648 / 1000 correct (64.80)\n",
      "138 epoch,  1000 iteration, loss:0.070\n",
      " num 137 epoch \n",
      "####### Training Loss #######\n",
      "[0.13454569]\n",
      "Checking accuracy on test set\n",
      "Got 659 / 1000 correct (65.90)\n",
      "139 epoch,   500 iteration, loss:0.059\n",
      "Checking accuracy on test set\n",
      "Got 674 / 1000 correct (67.40)\n",
      "139 epoch,  1000 iteration, loss:0.072\n",
      " num 138 epoch \n",
      "####### Training Loss #######\n",
      "[0.13420252]\n",
      "Checking accuracy on test set\n",
      "Got 667 / 1000 correct (66.70)\n",
      "140 epoch,   500 iteration, loss:0.077\n",
      "Checking accuracy on test set\n",
      "Got 673 / 1000 correct (67.30)\n",
      "140 epoch,  1000 iteration, loss:0.076\n",
      " num 139 epoch \n",
      "####### Training Loss #######\n",
      "[0.14516811]\n",
      "Checking accuracy on test set\n",
      "Got 677 / 1000 correct (67.70)\n",
      "141 epoch,   500 iteration, loss:0.062\n",
      "Checking accuracy on test set\n",
      "Got 676 / 1000 correct (67.60)\n",
      "141 epoch,  1000 iteration, loss:0.064\n",
      " num 140 epoch \n",
      "####### Training Loss #######\n",
      "[0.12125328]\n",
      "Checking accuracy on test set\n",
      "Got 670 / 1000 correct (67.00)\n",
      "142 epoch,   500 iteration, loss:0.071\n",
      "Checking accuracy on test set\n",
      "Got 674 / 1000 correct (67.40)\n",
      "142 epoch,  1000 iteration, loss:0.065\n",
      " num 141 epoch \n",
      "####### Training Loss #######\n",
      "[0.12671741]\n",
      "Checking accuracy on test set\n",
      "Got 666 / 1000 correct (66.60)\n",
      "143 epoch,   500 iteration, loss:0.056\n",
      "Checking accuracy on test set\n",
      "Got 699 / 1000 correct (69.90)\n",
      "143 epoch,  1000 iteration, loss:0.072\n",
      " num 142 epoch \n",
      "####### Training Loss #######\n",
      "[0.13194197]\n",
      "Checking accuracy on test set\n",
      "Got 693 / 1000 correct (69.30)\n",
      "144 epoch,   500 iteration, loss:0.066\n",
      "Checking accuracy on test set\n",
      "Got 683 / 1000 correct (68.30)\n",
      "144 epoch,  1000 iteration, loss:0.074\n",
      " num 143 epoch \n",
      "####### Training Loss #######\n",
      "[0.13584372]\n",
      "Checking accuracy on test set\n",
      "Got 667 / 1000 correct (66.70)\n",
      "145 epoch,   500 iteration, loss:0.068\n",
      "Checking accuracy on test set\n",
      "Got 668 / 1000 correct (66.80)\n",
      "145 epoch,  1000 iteration, loss:0.075\n",
      " num 144 epoch \n",
      "####### Training Loss #######\n",
      "[0.14969406]\n",
      "Checking accuracy on test set\n",
      "Got 667 / 1000 correct (66.70)\n",
      "146 epoch,   500 iteration, loss:0.070\n",
      "Checking accuracy on test set\n",
      "Got 678 / 1000 correct (67.80)\n",
      "146 epoch,  1000 iteration, loss:0.076\n",
      " num 145 epoch \n",
      "####### Training Loss #######\n",
      "[0.13778146]\n",
      "Checking accuracy on test set\n",
      "Got 672 / 1000 correct (67.20)\n",
      "147 epoch,   500 iteration, loss:0.069\n",
      "Checking accuracy on test set\n",
      "Got 674 / 1000 correct (67.40)\n",
      "147 epoch,  1000 iteration, loss:0.068\n",
      " num 146 epoch \n",
      "####### Training Loss #######\n",
      "[0.13863433]\n",
      "Checking accuracy on test set\n",
      "Got 672 / 1000 correct (67.20)\n",
      "148 epoch,   500 iteration, loss:0.066\n",
      "Checking accuracy on test set\n",
      "Got 684 / 1000 correct (68.40)\n",
      "148 epoch,  1000 iteration, loss:0.055\n",
      " num 147 epoch \n",
      "####### Training Loss #######\n",
      "[0.12603276]\n",
      "Checking accuracy on test set\n",
      "Got 684 / 1000 correct (68.40)\n",
      "149 epoch,   500 iteration, loss:0.062\n",
      "Checking accuracy on test set\n",
      "Got 685 / 1000 correct (68.50)\n",
      "149 epoch,  1000 iteration, loss:0.066\n",
      " num 148 epoch \n",
      "####### Training Loss #######\n",
      "[0.12583012]\n",
      "Checking accuracy on test set\n",
      "Got 679 / 1000 correct (67.90)\n",
      "150 epoch,   500 iteration, loss:0.065\n",
      "Checking accuracy on test set\n",
      "Got 667 / 1000 correct (66.70)\n",
      "150 epoch,  1000 iteration, loss:0.065\n",
      " num 149 epoch \n",
      "####### Training Loss #######\n",
      "[0.12377139]\n",
      "Checking accuracy on test set\n",
      "Got 657 / 1000 correct (65.70)\n",
      "151 epoch,   500 iteration, loss:0.067\n",
      "Checking accuracy on test set\n",
      "Got 674 / 1000 correct (67.40)\n",
      "151 epoch,  1000 iteration, loss:0.066\n",
      " num 150 epoch \n",
      "####### Training Loss #######\n",
      "[0.13163242]\n",
      "Checking accuracy on test set\n",
      "Got 676 / 1000 correct (67.60)\n",
      "152 epoch,   500 iteration, loss:0.074\n",
      "Checking accuracy on test set\n",
      "Got 667 / 1000 correct (66.70)\n",
      "152 epoch,  1000 iteration, loss:0.068\n",
      " num 151 epoch \n",
      "####### Training Loss #######\n",
      "[0.14250197]\n",
      "Checking accuracy on test set\n",
      "Got 683 / 1000 correct (68.30)\n",
      "153 epoch,   500 iteration, loss:0.060\n",
      "Checking accuracy on test set\n",
      "Got 687 / 1000 correct (68.70)\n",
      "153 epoch,  1000 iteration, loss:0.054\n",
      " num 152 epoch \n",
      "####### Training Loss #######\n",
      "[0.11898043]\n",
      "Checking accuracy on test set\n",
      "Got 674 / 1000 correct (67.40)\n",
      "154 epoch,   500 iteration, loss:0.055\n",
      "Checking accuracy on test set\n",
      "Got 699 / 1000 correct (69.90)\n",
      "154 epoch,  1000 iteration, loss:0.052\n",
      " num 153 epoch \n",
      "####### Training Loss #######\n",
      "[0.11057533]\n",
      "Checking accuracy on test set\n",
      "Got 682 / 1000 correct (68.20)\n",
      "155 epoch,   500 iteration, loss:0.074\n",
      "Checking accuracy on test set\n",
      "Got 681 / 1000 correct (68.10)\n",
      "155 epoch,  1000 iteration, loss:0.058\n",
      " num 154 epoch \n",
      "####### Training Loss #######\n",
      "[0.12186045]\n",
      "Checking accuracy on test set\n",
      "Got 666 / 1000 correct (66.60)\n",
      "156 epoch,   500 iteration, loss:0.061\n",
      "Checking accuracy on test set\n",
      "Got 647 / 1000 correct (64.70)\n",
      "156 epoch,  1000 iteration, loss:0.063\n",
      " num 155 epoch \n",
      "####### Training Loss #######\n",
      "[0.12626768]\n",
      "Checking accuracy on test set\n",
      "Got 709 / 1000 correct (70.90)\n",
      "157 epoch,   500 iteration, loss:0.064\n",
      "Checking accuracy on test set\n",
      "Got 680 / 1000 correct (68.00)\n",
      "157 epoch,  1000 iteration, loss:0.054\n",
      " num 156 epoch \n",
      "####### Training Loss #######\n",
      "[0.11505103]\n",
      "Checking accuracy on test set\n",
      "Got 699 / 1000 correct (69.90)\n",
      "158 epoch,   500 iteration, loss:0.060\n",
      "Checking accuracy on test set\n",
      "Got 679 / 1000 correct (67.90)\n",
      "158 epoch,  1000 iteration, loss:0.085\n",
      " num 157 epoch \n",
      "####### Training Loss #######\n",
      "[0.14270446]\n",
      "Checking accuracy on test set\n",
      "Got 671 / 1000 correct (67.10)\n",
      "159 epoch,   500 iteration, loss:0.071\n",
      "Checking accuracy on test set\n",
      "Got 643 / 1000 correct (64.30)\n",
      "159 epoch,  1000 iteration, loss:0.066\n",
      " num 158 epoch \n",
      "####### Training Loss #######\n",
      "[0.1325307]\n",
      "Checking accuracy on test set\n",
      "Got 691 / 1000 correct (69.10)\n",
      "160 epoch,   500 iteration, loss:0.066\n",
      "Checking accuracy on test set\n",
      "Got 667 / 1000 correct (66.70)\n",
      "160 epoch,  1000 iteration, loss:0.062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " num 159 epoch \n",
      "####### Training Loss #######\n",
      "[0.12270385]\n",
      "Checking accuracy on test set\n",
      "Got 693 / 1000 correct (69.30)\n",
      "161 epoch,   500 iteration, loss:0.067\n",
      "Checking accuracy on test set\n",
      "Got 692 / 1000 correct (69.20)\n",
      "161 epoch,  1000 iteration, loss:0.065\n",
      " num 160 epoch \n",
      "####### Training Loss #######\n",
      "[0.12679277]\n",
      "Checking accuracy on test set\n",
      "Got 679 / 1000 correct (67.90)\n",
      "162 epoch,   500 iteration, loss:0.056\n",
      "Checking accuracy on test set\n",
      "Got 663 / 1000 correct (66.30)\n",
      "162 epoch,  1000 iteration, loss:0.057\n",
      " num 161 epoch \n",
      "####### Training Loss #######\n",
      "[0.11487569]\n",
      "Checking accuracy on test set\n",
      "Got 687 / 1000 correct (68.70)\n",
      "163 epoch,   500 iteration, loss:0.057\n",
      "Checking accuracy on test set\n",
      "Got 665 / 1000 correct (66.50)\n",
      "163 epoch,  1000 iteration, loss:0.060\n",
      " num 162 epoch \n",
      "####### Training Loss #######\n",
      "[0.11278151]\n",
      "Checking accuracy on test set\n",
      "Got 659 / 1000 correct (65.90)\n",
      "164 epoch,   500 iteration, loss:0.051\n",
      "Checking accuracy on test set\n",
      "Got 684 / 1000 correct (68.40)\n",
      "164 epoch,  1000 iteration, loss:0.061\n",
      " num 163 epoch \n",
      "####### Training Loss #######\n",
      "[0.11428982]\n",
      "Checking accuracy on test set\n",
      "Got 686 / 1000 correct (68.60)\n",
      "165 epoch,   500 iteration, loss:0.060\n",
      "Checking accuracy on test set\n",
      "Got 685 / 1000 correct (68.50)\n",
      "165 epoch,  1000 iteration, loss:0.078\n",
      " num 164 epoch \n",
      "####### Training Loss #######\n",
      "[0.13428492]\n",
      "Checking accuracy on test set\n",
      "Got 657 / 1000 correct (65.70)\n",
      "166 epoch,   500 iteration, loss:0.056\n",
      "Checking accuracy on test set\n",
      "Got 691 / 1000 correct (69.10)\n",
      "166 epoch,  1000 iteration, loss:0.053\n",
      " num 165 epoch \n",
      "####### Training Loss #######\n",
      "[0.10439198]\n",
      "Checking accuracy on test set\n",
      "Got 685 / 1000 correct (68.50)\n",
      "167 epoch,   500 iteration, loss:0.060\n",
      "Checking accuracy on test set\n",
      "Got 688 / 1000 correct (68.80)\n",
      "167 epoch,  1000 iteration, loss:0.061\n",
      " num 166 epoch \n",
      "####### Training Loss #######\n",
      "[0.11951152]\n",
      "Checking accuracy on test set\n",
      "Got 690 / 1000 correct (69.00)\n",
      "168 epoch,   500 iteration, loss:0.058\n",
      "Checking accuracy on test set\n",
      "Got 692 / 1000 correct (69.20)\n",
      "168 epoch,  1000 iteration, loss:0.059\n",
      " num 167 epoch \n",
      "####### Training Loss #######\n",
      "[0.10780486]\n",
      "Checking accuracy on test set\n",
      "Got 642 / 1000 correct (64.20)\n",
      "169 epoch,   500 iteration, loss:0.067\n",
      "Checking accuracy on test set\n",
      "Got 704 / 1000 correct (70.40)\n",
      "169 epoch,  1000 iteration, loss:0.060\n",
      " num 168 epoch \n",
      "####### Training Loss #######\n",
      "[0.12211714]\n",
      "Checking accuracy on test set\n",
      "Got 680 / 1000 correct (68.00)\n",
      "170 epoch,   500 iteration, loss:0.052\n",
      "Checking accuracy on test set\n",
      "Got 690 / 1000 correct (69.00)\n",
      "170 epoch,  1000 iteration, loss:0.069\n",
      " num 169 epoch \n",
      "####### Training Loss #######\n",
      "[0.12061861]\n",
      "Checking accuracy on test set\n",
      "Got 685 / 1000 correct (68.50)\n",
      "171 epoch,   500 iteration, loss:0.059\n",
      "Checking accuracy on test set\n",
      "Got 657 / 1000 correct (65.70)\n",
      "171 epoch,  1000 iteration, loss:0.060\n",
      " num 170 epoch \n",
      "####### Training Loss #######\n",
      "[0.11620747]\n",
      "Checking accuracy on test set\n",
      "Got 690 / 1000 correct (69.00)\n",
      "172 epoch,   500 iteration, loss:0.059\n",
      "Checking accuracy on test set\n",
      "Got 683 / 1000 correct (68.30)\n",
      "172 epoch,  1000 iteration, loss:0.054\n",
      " num 171 epoch \n",
      "####### Training Loss #######\n",
      "[0.11776541]\n",
      "Checking accuracy on test set\n",
      "Got 688 / 1000 correct (68.80)\n",
      "173 epoch,   500 iteration, loss:0.059\n",
      "Checking accuracy on test set\n",
      "Got 695 / 1000 correct (69.50)\n",
      "173 epoch,  1000 iteration, loss:0.064\n",
      " num 172 epoch \n",
      "####### Training Loss #######\n",
      "[0.1220767]\n",
      "Checking accuracy on test set\n",
      "Got 656 / 1000 correct (65.60)\n",
      "174 epoch,   500 iteration, loss:0.057\n",
      "Checking accuracy on test set\n",
      "Got 658 / 1000 correct (65.80)\n",
      "174 epoch,  1000 iteration, loss:0.061\n",
      " num 173 epoch \n",
      "####### Training Loss #######\n",
      "[0.12370779]\n",
      "Checking accuracy on test set\n",
      "Got 687 / 1000 correct (68.70)\n",
      "175 epoch,   500 iteration, loss:0.054\n",
      "Checking accuracy on test set\n",
      "Got 673 / 1000 correct (67.30)\n",
      "175 epoch,  1000 iteration, loss:0.061\n",
      " num 174 epoch \n",
      "####### Training Loss #######\n",
      "[0.11594194]\n",
      "Checking accuracy on test set\n",
      "Got 670 / 1000 correct (67.00)\n",
      "176 epoch,   500 iteration, loss:0.068\n",
      "Checking accuracy on test set\n",
      "Got 664 / 1000 correct (66.40)\n",
      "176 epoch,  1000 iteration, loss:0.055\n",
      " num 175 epoch \n",
      "####### Training Loss #######\n",
      "[0.12176501]\n",
      "Checking accuracy on test set\n",
      "Got 662 / 1000 correct (66.20)\n",
      "177 epoch,   500 iteration, loss:0.060\n",
      "Checking accuracy on test set\n",
      "Got 688 / 1000 correct (68.80)\n",
      "177 epoch,  1000 iteration, loss:0.053\n",
      " num 176 epoch \n",
      "####### Training Loss #######\n",
      "[0.11039988]\n",
      "Checking accuracy on test set\n",
      "Got 684 / 1000 correct (68.40)\n",
      "178 epoch,   500 iteration, loss:0.063\n",
      "Checking accuracy on test set\n",
      "Got 667 / 1000 correct (66.70)\n",
      "178 epoch,  1000 iteration, loss:0.059\n",
      " num 177 epoch \n",
      "####### Training Loss #######\n",
      "[0.12138788]\n",
      "Checking accuracy on test set\n",
      "Got 679 / 1000 correct (67.90)\n",
      "179 epoch,   500 iteration, loss:0.052\n",
      "Checking accuracy on test set\n",
      "Got 695 / 1000 correct (69.50)\n",
      "179 epoch,  1000 iteration, loss:0.073\n",
      " num 178 epoch \n",
      "####### Training Loss #######\n",
      "[0.12773696]\n",
      "Checking accuracy on test set\n",
      "Got 685 / 1000 correct (68.50)\n",
      "180 epoch,   500 iteration, loss:0.049\n",
      "Checking accuracy on test set\n",
      "Got 676 / 1000 correct (67.60)\n",
      "180 epoch,  1000 iteration, loss:0.047\n",
      " num 179 epoch \n",
      "####### Training Loss #######\n",
      "[0.09801576]\n",
      "Checking accuracy on test set\n",
      "Got 690 / 1000 correct (69.00)\n",
      "181 epoch,   500 iteration, loss:0.062\n",
      "Checking accuracy on test set\n",
      "Got 688 / 1000 correct (68.80)\n",
      "181 epoch,  1000 iteration, loss:0.049\n",
      " num 180 epoch \n",
      "####### Training Loss #######\n",
      "[0.10816358]\n",
      "Checking accuracy on test set\n",
      "Got 677 / 1000 correct (67.70)\n",
      "182 epoch,   500 iteration, loss:0.062\n",
      "Checking accuracy on test set\n",
      "Got 660 / 1000 correct (66.00)\n",
      "182 epoch,  1000 iteration, loss:0.063\n",
      " num 181 epoch \n",
      "####### Training Loss #######\n",
      "[0.12555691]\n",
      "Checking accuracy on test set\n",
      "Got 685 / 1000 correct (68.50)\n",
      "183 epoch,   500 iteration, loss:0.059\n",
      "Checking accuracy on test set\n",
      "Got 687 / 1000 correct (68.70)\n",
      "183 epoch,  1000 iteration, loss:0.059\n",
      " num 182 epoch \n",
      "####### Training Loss #######\n",
      "[0.11691009]\n",
      "Checking accuracy on test set\n",
      "Got 679 / 1000 correct (67.90)\n",
      "184 epoch,   500 iteration, loss:0.059\n",
      "Checking accuracy on test set\n",
      "Got 679 / 1000 correct (67.90)\n",
      "184 epoch,  1000 iteration, loss:0.056\n",
      " num 183 epoch \n",
      "####### Training Loss #######\n",
      "[0.11504956]\n",
      "Checking accuracy on test set\n",
      "Got 690 / 1000 correct (69.00)\n",
      "185 epoch,   500 iteration, loss:0.066\n",
      "Checking accuracy on test set\n",
      "Got 667 / 1000 correct (66.70)\n",
      "185 epoch,  1000 iteration, loss:0.057\n",
      " num 184 epoch \n",
      "####### Training Loss #######\n",
      "[0.11870176]\n",
      "Checking accuracy on test set\n",
      "Got 681 / 1000 correct (68.10)\n",
      "186 epoch,   500 iteration, loss:0.051\n",
      "Checking accuracy on test set\n",
      "Got 667 / 1000 correct (66.70)\n",
      "186 epoch,  1000 iteration, loss:0.057\n",
      " num 185 epoch \n",
      "####### Training Loss #######\n",
      "[0.10769834]\n",
      "Checking accuracy on test set\n",
      "Got 673 / 1000 correct (67.30)\n",
      "187 epoch,   500 iteration, loss:0.061\n",
      "Checking accuracy on test set\n",
      "Got 676 / 1000 correct (67.60)\n",
      "187 epoch,  1000 iteration, loss:0.054\n",
      " num 186 epoch \n",
      "####### Training Loss #######\n",
      "[0.11458951]\n",
      "Checking accuracy on test set\n",
      "Got 678 / 1000 correct (67.80)\n",
      "188 epoch,   500 iteration, loss:0.071\n",
      "Checking accuracy on test set\n",
      "Got 673 / 1000 correct (67.30)\n",
      "188 epoch,  1000 iteration, loss:0.045\n",
      " num 187 epoch \n",
      "####### Training Loss #######\n",
      "[0.11490041]\n",
      "Checking accuracy on test set\n",
      "Got 696 / 1000 correct (69.60)\n",
      "189 epoch,   500 iteration, loss:0.053\n",
      "Checking accuracy on test set\n",
      "Got 665 / 1000 correct (66.50)\n",
      "189 epoch,  1000 iteration, loss:0.051\n",
      " num 188 epoch \n",
      "####### Training Loss #######\n",
      "[0.11393984]\n",
      "Checking accuracy on test set\n",
      "Got 655 / 1000 correct (65.50)\n",
      "190 epoch,   500 iteration, loss:0.054\n",
      "Checking accuracy on test set\n",
      "Got 647 / 1000 correct (64.70)\n",
      "190 epoch,  1000 iteration, loss:0.060\n",
      " num 189 epoch \n",
      "####### Training Loss #######\n",
      "[0.12151632]\n",
      "Checking accuracy on test set\n",
      "Got 667 / 1000 correct (66.70)\n",
      "191 epoch,   500 iteration, loss:0.057\n",
      "Checking accuracy on test set\n",
      "Got 673 / 1000 correct (67.30)\n",
      "191 epoch,  1000 iteration, loss:0.044\n",
      " num 190 epoch \n",
      "####### Training Loss #######\n",
      "[0.09683463]\n",
      "Checking accuracy on test set\n",
      "Got 653 / 1000 correct (65.30)\n",
      "192 epoch,   500 iteration, loss:0.053\n",
      "Checking accuracy on test set\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 688 / 1000 correct (68.80)\n",
      "192 epoch,  1000 iteration, loss:0.052\n",
      " num 191 epoch \n",
      "####### Training Loss #######\n",
      "[0.10581699]\n",
      "Checking accuracy on test set\n",
      "Got 694 / 1000 correct (69.40)\n",
      "193 epoch,   500 iteration, loss:0.043\n",
      "Checking accuracy on test set\n",
      "Got 671 / 1000 correct (67.10)\n",
      "193 epoch,  1000 iteration, loss:0.051\n",
      " num 192 epoch \n",
      "####### Training Loss #######\n",
      "[0.09598161]\n",
      "Checking accuracy on test set\n",
      "Got 685 / 1000 correct (68.50)\n",
      "194 epoch,   500 iteration, loss:0.060\n",
      "Checking accuracy on test set\n",
      "Got 684 / 1000 correct (68.40)\n",
      "194 epoch,  1000 iteration, loss:0.055\n",
      " num 193 epoch \n",
      "####### Training Loss #######\n",
      "[0.11793477]\n",
      "Checking accuracy on test set\n",
      "Got 682 / 1000 correct (68.20)\n",
      "195 epoch,   500 iteration, loss:0.050\n",
      "Checking accuracy on test set\n",
      "Got 680 / 1000 correct (68.00)\n",
      "195 epoch,  1000 iteration, loss:0.048\n",
      " num 194 epoch \n",
      "####### Training Loss #######\n",
      "[0.10155848]\n",
      "Checking accuracy on test set\n",
      "Got 686 / 1000 correct (68.60)\n",
      "196 epoch,   500 iteration, loss:0.048\n",
      "Checking accuracy on test set\n",
      "Got 662 / 1000 correct (66.20)\n",
      "196 epoch,  1000 iteration, loss:0.051\n",
      " num 195 epoch \n",
      "####### Training Loss #######\n",
      "[0.10236271]\n",
      "Checking accuracy on test set\n",
      "Got 680 / 1000 correct (68.00)\n",
      "197 epoch,   500 iteration, loss:0.061\n",
      "Checking accuracy on test set\n",
      "Got 669 / 1000 correct (66.90)\n",
      "197 epoch,  1000 iteration, loss:0.049\n",
      " num 196 epoch \n",
      "####### Training Loss #######\n",
      "[0.11155869]\n",
      "Checking accuracy on test set\n",
      "Got 683 / 1000 correct (68.30)\n",
      "198 epoch,   500 iteration, loss:0.050\n",
      "Checking accuracy on test set\n",
      "Got 671 / 1000 correct (67.10)\n",
      "198 epoch,  1000 iteration, loss:0.057\n",
      " num 197 epoch \n",
      "####### Training Loss #######\n",
      "[0.10518723]\n",
      "Checking accuracy on test set\n",
      "Got 684 / 1000 correct (68.40)\n",
      "199 epoch,   500 iteration, loss:0.046\n",
      "Checking accuracy on test set\n",
      "Got 663 / 1000 correct (66.30)\n",
      "199 epoch,  1000 iteration, loss:0.054\n",
      " num 198 epoch \n",
      "####### Training Loss #######\n",
      "[0.10147709]\n",
      "Checking accuracy on test set\n",
      "Got 710 / 1000 correct (71.00)\n",
      "200 epoch,   500 iteration, loss:0.044\n",
      "Checking accuracy on test set\n",
      "Got 674 / 1000 correct (67.40)\n",
      "200 epoch,  1000 iteration, loss:0.048\n",
      " num 199 epoch \n",
      "####### Training Loss #######\n",
      "[0.09384861]\n",
      "Checking accuracy on test set\n",
      "Got 691 / 1000 correct (69.10)\n",
      "201 epoch,   500 iteration, loss:0.051\n",
      "Checking accuracy on test set\n",
      "Got 695 / 1000 correct (69.50)\n",
      "201 epoch,  1000 iteration, loss:0.054\n",
      " num 200 epoch \n",
      "####### Training Loss #######\n",
      "[0.1037101]\n",
      "Checking accuracy on test set\n",
      "Got 696 / 1000 correct (69.60)\n",
      "202 epoch,   500 iteration, loss:0.062\n",
      "Checking accuracy on test set\n",
      "Got 683 / 1000 correct (68.30)\n",
      "202 epoch,  1000 iteration, loss:0.057\n",
      " num 201 epoch \n",
      "####### Training Loss #######\n",
      "[0.1156442]\n",
      "Checking accuracy on test set\n",
      "Got 698 / 1000 correct (69.80)\n",
      "203 epoch,   500 iteration, loss:0.068\n",
      "Checking accuracy on test set\n",
      "Got 673 / 1000 correct (67.30)\n",
      "203 epoch,  1000 iteration, loss:0.053\n",
      " num 202 epoch \n",
      "####### Training Loss #######\n",
      "[0.11196269]\n",
      "Checking accuracy on test set\n",
      "Got 689 / 1000 correct (68.90)\n",
      "204 epoch,   500 iteration, loss:0.056\n",
      "Checking accuracy on test set\n",
      "Got 692 / 1000 correct (69.20)\n",
      "204 epoch,  1000 iteration, loss:0.049\n",
      " num 203 epoch \n",
      "####### Training Loss #######\n",
      "[0.1036314]\n",
      "Checking accuracy on test set\n",
      "Got 691 / 1000 correct (69.10)\n",
      "205 epoch,   500 iteration, loss:0.052\n",
      "Checking accuracy on test set\n",
      "Got 663 / 1000 correct (66.30)\n",
      "205 epoch,  1000 iteration, loss:0.049\n",
      " num 204 epoch \n",
      "####### Training Loss #######\n",
      "[0.09822097]\n",
      "Checking accuracy on test set\n",
      "Got 694 / 1000 correct (69.40)\n",
      "206 epoch,   500 iteration, loss:0.056\n",
      "Checking accuracy on test set\n",
      "Got 664 / 1000 correct (66.40)\n",
      "206 epoch,  1000 iteration, loss:0.057\n",
      " num 205 epoch \n",
      "####### Training Loss #######\n",
      "[0.11039431]\n",
      "Checking accuracy on test set\n",
      "Got 682 / 1000 correct (68.20)\n",
      "207 epoch,   500 iteration, loss:0.056\n",
      "Checking accuracy on test set\n",
      "Got 684 / 1000 correct (68.40)\n",
      "207 epoch,  1000 iteration, loss:0.051\n",
      " num 206 epoch \n",
      "####### Training Loss #######\n",
      "[0.11011583]\n",
      "Checking accuracy on test set\n",
      "Got 653 / 1000 correct (65.30)\n",
      "208 epoch,   500 iteration, loss:0.065\n",
      "Checking accuracy on test set\n",
      "Got 670 / 1000 correct (67.00)\n",
      "208 epoch,  1000 iteration, loss:0.045\n",
      " num 207 epoch \n",
      "####### Training Loss #######\n",
      "[0.11545364]\n",
      "Checking accuracy on test set\n",
      "Got 687 / 1000 correct (68.70)\n",
      "209 epoch,   500 iteration, loss:0.061\n",
      "Checking accuracy on test set\n",
      "Got 678 / 1000 correct (67.80)\n",
      "209 epoch,  1000 iteration, loss:0.049\n",
      " num 208 epoch \n",
      "####### Training Loss #######\n",
      "[0.10427182]\n",
      "Checking accuracy on test set\n",
      "Got 686 / 1000 correct (68.60)\n",
      "210 epoch,   500 iteration, loss:0.046\n",
      "Checking accuracy on test set\n",
      "Got 704 / 1000 correct (70.40)\n",
      "210 epoch,  1000 iteration, loss:0.058\n",
      " num 209 epoch \n",
      "####### Training Loss #######\n",
      "[0.10482682]\n",
      "Checking accuracy on test set\n",
      "Got 688 / 1000 correct (68.80)\n",
      "211 epoch,   500 iteration, loss:0.054\n",
      "Checking accuracy on test set\n",
      "Got 687 / 1000 correct (68.70)\n",
      "211 epoch,  1000 iteration, loss:0.051\n",
      " num 210 epoch \n",
      "####### Training Loss #######\n",
      "[0.10102753]\n",
      "Checking accuracy on test set\n",
      "Got 699 / 1000 correct (69.90)\n",
      "212 epoch,   500 iteration, loss:0.054\n",
      "Checking accuracy on test set\n",
      "Got 681 / 1000 correct (68.10)\n",
      "212 epoch,  1000 iteration, loss:0.052\n",
      " num 211 epoch \n",
      "####### Training Loss #######\n",
      "[0.10810308]\n",
      "Checking accuracy on test set\n",
      "Got 671 / 1000 correct (67.10)\n",
      "213 epoch,   500 iteration, loss:0.061\n",
      "Checking accuracy on test set\n",
      "Got 680 / 1000 correct (68.00)\n",
      "213 epoch,  1000 iteration, loss:0.051\n",
      " num 212 epoch \n",
      "####### Training Loss #######\n",
      "[0.11240129]\n",
      "Checking accuracy on test set\n",
      "Got 656 / 1000 correct (65.60)\n",
      "214 epoch,   500 iteration, loss:0.049\n",
      "Checking accuracy on test set\n",
      "Got 683 / 1000 correct (68.30)\n",
      "214 epoch,  1000 iteration, loss:0.075\n",
      " num 213 epoch \n",
      "####### Training Loss #######\n",
      "[0.12224456]\n",
      "Checking accuracy on test set\n",
      "Got 670 / 1000 correct (67.00)\n",
      "215 epoch,   500 iteration, loss:0.052\n",
      "Checking accuracy on test set\n",
      "Got 678 / 1000 correct (67.80)\n",
      "215 epoch,  1000 iteration, loss:0.062\n",
      " num 214 epoch \n",
      "####### Training Loss #######\n",
      "[0.11139742]\n",
      "Checking accuracy on test set\n",
      "Got 683 / 1000 correct (68.30)\n",
      "216 epoch,   500 iteration, loss:0.051\n",
      "Checking accuracy on test set\n",
      "Got 699 / 1000 correct (69.90)\n",
      "216 epoch,  1000 iteration, loss:0.041\n",
      " num 215 epoch \n",
      "####### Training Loss #######\n",
      "[0.09812377]\n",
      "Checking accuracy on test set\n",
      "Got 691 / 1000 correct (69.10)\n",
      "217 epoch,   500 iteration, loss:0.060\n",
      "Checking accuracy on test set\n",
      "Got 677 / 1000 correct (67.70)\n",
      "217 epoch,  1000 iteration, loss:0.064\n",
      " num 216 epoch \n",
      "####### Training Loss #######\n",
      "[0.11994633]\n",
      "Checking accuracy on test set\n",
      "Got 683 / 1000 correct (68.30)\n",
      "218 epoch,   500 iteration, loss:0.046\n",
      "Checking accuracy on test set\n",
      "Got 671 / 1000 correct (67.10)\n",
      "218 epoch,  1000 iteration, loss:0.050\n",
      " num 217 epoch \n",
      "####### Training Loss #######\n",
      "[0.095441]\n",
      "Checking accuracy on test set\n",
      "Got 708 / 1000 correct (70.80)\n",
      "219 epoch,   500 iteration, loss:0.053\n",
      "Checking accuracy on test set\n",
      "Got 696 / 1000 correct (69.60)\n",
      "219 epoch,  1000 iteration, loss:0.040\n",
      " num 218 epoch \n",
      "####### Training Loss #######\n",
      "[0.10668862]\n",
      "Checking accuracy on test set\n",
      "Got 711 / 1000 correct (71.10)\n",
      "220 epoch,   500 iteration, loss:0.045\n",
      "Checking accuracy on test set\n",
      "Got 665 / 1000 correct (66.50)\n",
      "220 epoch,  1000 iteration, loss:0.048\n",
      " num 219 epoch \n",
      "####### Training Loss #######\n",
      "[0.09696445]\n",
      "Checking accuracy on test set\n",
      "Got 657 / 1000 correct (65.70)\n",
      "221 epoch,   500 iteration, loss:0.055\n",
      "Checking accuracy on test set\n",
      "Got 670 / 1000 correct (67.00)\n",
      "221 epoch,  1000 iteration, loss:0.047\n",
      " num 220 epoch \n",
      "####### Training Loss #######\n",
      "[0.09954638]\n",
      "Checking accuracy on test set\n",
      "Got 692 / 1000 correct (69.20)\n",
      "222 epoch,   500 iteration, loss:0.061\n",
      "Checking accuracy on test set\n",
      "Got 686 / 1000 correct (68.60)\n",
      "222 epoch,  1000 iteration, loss:0.063\n",
      " num 221 epoch \n",
      "####### Training Loss #######\n",
      "[0.1275582]\n",
      "Checking accuracy on test set\n",
      "Got 701 / 1000 correct (70.10)\n",
      "223 epoch,   500 iteration, loss:0.042\n",
      "Checking accuracy on test set\n",
      "Got 682 / 1000 correct (68.20)\n",
      "223 epoch,  1000 iteration, loss:0.055\n",
      " num 222 epoch \n",
      "####### Training Loss #######\n",
      "[0.09229392]\n",
      "Checking accuracy on test set\n",
      "Got 690 / 1000 correct (69.00)\n",
      "224 epoch,   500 iteration, loss:0.045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking accuracy on test set\n",
      "Got 679 / 1000 correct (67.90)\n",
      "224 epoch,  1000 iteration, loss:0.069\n",
      " num 223 epoch \n",
      "####### Training Loss #######\n",
      "[0.1123435]\n",
      "Checking accuracy on test set\n",
      "Got 687 / 1000 correct (68.70)\n",
      "225 epoch,   500 iteration, loss:0.063\n",
      "Checking accuracy on test set\n",
      "Got 658 / 1000 correct (65.80)\n",
      "225 epoch,  1000 iteration, loss:0.055\n",
      " num 224 epoch \n",
      "####### Training Loss #######\n",
      "[0.12080355]\n",
      "Checking accuracy on test set\n",
      "Got 665 / 1000 correct (66.50)\n",
      "226 epoch,   500 iteration, loss:0.057\n",
      "Checking accuracy on test set\n",
      "Got 696 / 1000 correct (69.60)\n",
      "226 epoch,  1000 iteration, loss:0.063\n",
      " num 225 epoch \n",
      "####### Training Loss #######\n",
      "[0.11219526]\n",
      "Checking accuracy on test set\n",
      "Got 676 / 1000 correct (67.60)\n",
      "227 epoch,   500 iteration, loss:0.051\n",
      "Checking accuracy on test set\n",
      "Got 681 / 1000 correct (68.10)\n",
      "227 epoch,  1000 iteration, loss:0.058\n",
      " num 226 epoch \n",
      "####### Training Loss #######\n",
      "[0.10373717]\n",
      "Checking accuracy on test set\n",
      "Got 662 / 1000 correct (66.20)\n",
      "228 epoch,   500 iteration, loss:0.053\n",
      "Checking accuracy on test set\n",
      "Got 664 / 1000 correct (66.40)\n",
      "228 epoch,  1000 iteration, loss:0.058\n",
      " num 227 epoch \n",
      "####### Training Loss #######\n",
      "[0.11399382]\n",
      "Checking accuracy on test set\n",
      "Got 683 / 1000 correct (68.30)\n",
      "229 epoch,   500 iteration, loss:0.052\n",
      "Checking accuracy on test set\n",
      "Got 687 / 1000 correct (68.70)\n",
      "229 epoch,  1000 iteration, loss:0.043\n",
      " num 228 epoch \n",
      "####### Training Loss #######\n",
      "[0.10425927]\n",
      "Checking accuracy on test set\n",
      "Got 693 / 1000 correct (69.30)\n",
      "230 epoch,   500 iteration, loss:0.054\n",
      "Checking accuracy on test set\n",
      "Got 675 / 1000 correct (67.50)\n",
      "230 epoch,  1000 iteration, loss:0.043\n",
      " num 229 epoch \n",
      "####### Training Loss #######\n",
      "[0.10217419]\n",
      "Checking accuracy on test set\n",
      "Got 677 / 1000 correct (67.70)\n",
      "231 epoch,   500 iteration, loss:0.058\n",
      "Checking accuracy on test set\n",
      "Got 687 / 1000 correct (68.70)\n",
      "231 epoch,  1000 iteration, loss:0.045\n",
      " num 230 epoch \n",
      "####### Training Loss #######\n",
      "[0.09926374]\n",
      "Checking accuracy on test set\n",
      "Got 678 / 1000 correct (67.80)\n",
      "232 epoch,   500 iteration, loss:0.037\n",
      "Checking accuracy on test set\n",
      "Got 691 / 1000 correct (69.10)\n",
      "232 epoch,  1000 iteration, loss:0.044\n",
      " num 231 epoch \n",
      "####### Training Loss #######\n",
      "[0.0817256]\n",
      "Checking accuracy on test set\n",
      "Got 695 / 1000 correct (69.50)\n",
      "233 epoch,   500 iteration, loss:0.046\n",
      "Checking accuracy on test set\n",
      "Got 704 / 1000 correct (70.40)\n",
      "233 epoch,  1000 iteration, loss:0.052\n",
      " num 232 epoch \n",
      "####### Training Loss #######\n",
      "[0.09544142]\n",
      "Checking accuracy on test set\n",
      "Got 691 / 1000 correct (69.10)\n",
      "234 epoch,   500 iteration, loss:0.047\n",
      "Checking accuracy on test set\n",
      "Got 676 / 1000 correct (67.60)\n",
      "234 epoch,  1000 iteration, loss:0.044\n",
      " num 233 epoch \n",
      "####### Training Loss #######\n",
      "[0.08889705]\n",
      "Checking accuracy on test set\n",
      "Got 694 / 1000 correct (69.40)\n",
      "235 epoch,   500 iteration, loss:0.039\n",
      "Checking accuracy on test set\n",
      "Got 687 / 1000 correct (68.70)\n",
      "235 epoch,  1000 iteration, loss:0.047\n",
      " num 234 epoch \n",
      "####### Training Loss #######\n",
      "[0.09215065]\n",
      "Checking accuracy on test set\n",
      "Got 667 / 1000 correct (66.70)\n",
      "236 epoch,   500 iteration, loss:0.052\n",
      "Checking accuracy on test set\n",
      "Got 684 / 1000 correct (68.40)\n",
      "236 epoch,  1000 iteration, loss:0.046\n",
      " num 235 epoch \n",
      "####### Training Loss #######\n",
      "[0.09603558]\n",
      "Checking accuracy on test set\n",
      "Got 706 / 1000 correct (70.60)\n",
      "237 epoch,   500 iteration, loss:0.060\n",
      "Checking accuracy on test set\n",
      "Got 691 / 1000 correct (69.10)\n",
      "237 epoch,  1000 iteration, loss:0.055\n",
      " num 236 epoch \n",
      "####### Training Loss #######\n",
      "[0.1165031]\n",
      "Checking accuracy on test set\n",
      "Got 685 / 1000 correct (68.50)\n",
      "238 epoch,   500 iteration, loss:0.063\n",
      "Checking accuracy on test set\n",
      "Got 696 / 1000 correct (69.60)\n",
      "238 epoch,  1000 iteration, loss:0.046\n",
      " num 237 epoch \n",
      "####### Training Loss #######\n",
      "[0.10115592]\n",
      "Checking accuracy on test set\n",
      "Got 682 / 1000 correct (68.20)\n",
      "239 epoch,   500 iteration, loss:0.045\n",
      "Checking accuracy on test set\n",
      "Got 676 / 1000 correct (67.60)\n",
      "239 epoch,  1000 iteration, loss:0.050\n",
      " num 238 epoch \n",
      "####### Training Loss #######\n",
      "[0.09836981]\n",
      "Checking accuracy on test set\n",
      "Got 707 / 1000 correct (70.70)\n",
      "240 epoch,   500 iteration, loss:0.042\n",
      "Checking accuracy on test set\n",
      "Got 697 / 1000 correct (69.70)\n",
      "240 epoch,  1000 iteration, loss:0.053\n",
      " num 239 epoch \n",
      "####### Training Loss #######\n",
      "[0.10347858]\n",
      "Checking accuracy on test set\n",
      "Got 704 / 1000 correct (70.40)\n",
      "241 epoch,   500 iteration, loss:0.047\n",
      "Checking accuracy on test set\n",
      "Got 696 / 1000 correct (69.60)\n",
      "241 epoch,  1000 iteration, loss:0.062\n",
      " num 240 epoch \n",
      "####### Training Loss #######\n",
      "[0.10431116]\n",
      "Checking accuracy on test set\n",
      "Got 658 / 1000 correct (65.80)\n",
      "242 epoch,   500 iteration, loss:0.046\n",
      "Checking accuracy on test set\n",
      "Got 669 / 1000 correct (66.90)\n",
      "242 epoch,  1000 iteration, loss:0.050\n",
      " num 241 epoch \n",
      "####### Training Loss #######\n",
      "[0.09846027]\n",
      "Checking accuracy on test set\n",
      "Got 698 / 1000 correct (69.80)\n",
      "243 epoch,   500 iteration, loss:0.048\n",
      "Checking accuracy on test set\n",
      "Got 678 / 1000 correct (67.80)\n",
      "243 epoch,  1000 iteration, loss:0.055\n",
      " num 242 epoch \n",
      "####### Training Loss #######\n",
      "[0.10197731]\n",
      "Checking accuracy on test set\n",
      "Got 697 / 1000 correct (69.70)\n",
      "244 epoch,   500 iteration, loss:0.053\n",
      "Checking accuracy on test set\n",
      "Got 669 / 1000 correct (66.90)\n",
      "244 epoch,  1000 iteration, loss:0.046\n",
      " num 243 epoch \n",
      "####### Training Loss #######\n",
      "[0.09422759]\n",
      "Checking accuracy on test set\n",
      "Got 678 / 1000 correct (67.80)\n",
      "245 epoch,   500 iteration, loss:0.045\n",
      "Checking accuracy on test set\n",
      "Got 687 / 1000 correct (68.70)\n",
      "245 epoch,  1000 iteration, loss:0.059\n",
      " num 244 epoch \n",
      "####### Training Loss #######\n",
      "[0.10131006]\n",
      "Checking accuracy on test set\n",
      "Got 690 / 1000 correct (69.00)\n",
      "246 epoch,   500 iteration, loss:0.064\n",
      "Checking accuracy on test set\n",
      "Got 646 / 1000 correct (64.60)\n",
      "246 epoch,  1000 iteration, loss:0.044\n",
      " num 245 epoch \n",
      "####### Training Loss #######\n",
      "[0.10934377]\n",
      "Checking accuracy on test set\n",
      "Got 692 / 1000 correct (69.20)\n",
      "247 epoch,   500 iteration, loss:0.050\n",
      "Checking accuracy on test set\n",
      "Got 697 / 1000 correct (69.70)\n",
      "247 epoch,  1000 iteration, loss:0.034\n",
      " num 246 epoch \n",
      "####### Training Loss #######\n",
      "[0.08161028]\n",
      "Checking accuracy on test set\n",
      "Got 688 / 1000 correct (68.80)\n",
      "248 epoch,   500 iteration, loss:0.047\n",
      "Checking accuracy on test set\n",
      "Got 644 / 1000 correct (64.40)\n",
      "248 epoch,  1000 iteration, loss:0.049\n",
      " num 247 epoch \n",
      "####### Training Loss #######\n",
      "[0.09951095]\n",
      "Checking accuracy on test set\n",
      "Got 687 / 1000 correct (68.70)\n",
      "249 epoch,   500 iteration, loss:0.035\n",
      "Checking accuracy on test set\n",
      "Got 674 / 1000 correct (67.40)\n",
      "249 epoch,  1000 iteration, loss:0.053\n",
      " num 248 epoch \n",
      "####### Training Loss #######\n",
      "[0.08866398]\n",
      "Checking accuracy on test set\n",
      "Got 686 / 1000 correct (68.60)\n",
      "250 epoch,   500 iteration, loss:0.041\n",
      "Checking accuracy on test set\n",
      "Got 706 / 1000 correct (70.60)\n",
      "250 epoch,  1000 iteration, loss:0.049\n",
      " num 249 epoch \n",
      "####### Training Loss #######\n",
      "[0.08557999]\n",
      "Checking accuracy on test set\n",
      "Got 673 / 1000 correct (67.30)\n",
      "251 epoch,   500 iteration, loss:0.060\n",
      "Checking accuracy on test set\n",
      "Got 685 / 1000 correct (68.50)\n",
      "251 epoch,  1000 iteration, loss:0.044\n",
      " num 250 epoch \n",
      "####### Training Loss #######\n",
      "[0.1077426]\n",
      "Checking accuracy on test set\n",
      "Got 679 / 1000 correct (67.90)\n",
      "252 epoch,   500 iteration, loss:0.048\n",
      "Checking accuracy on test set\n",
      "Got 671 / 1000 correct (67.10)\n",
      "252 epoch,  1000 iteration, loss:0.051\n",
      " num 251 epoch \n",
      "####### Training Loss #######\n",
      "[0.09575662]\n",
      "Checking accuracy on test set\n",
      "Got 673 / 1000 correct (67.30)\n",
      "253 epoch,   500 iteration, loss:0.048\n",
      "Checking accuracy on test set\n",
      "Got 684 / 1000 correct (68.40)\n",
      "253 epoch,  1000 iteration, loss:0.035\n",
      " num 252 epoch \n",
      "####### Training Loss #######\n",
      "[0.08704962]\n",
      "Checking accuracy on test set\n",
      "Got 705 / 1000 correct (70.50)\n",
      "254 epoch,   500 iteration, loss:0.039\n",
      "Checking accuracy on test set\n",
      "Got 692 / 1000 correct (69.20)\n",
      "254 epoch,  1000 iteration, loss:0.062\n",
      " num 253 epoch \n",
      "####### Training Loss #######\n",
      "[0.09917212]\n",
      "Checking accuracy on test set\n",
      "Got 680 / 1000 correct (68.00)\n",
      "255 epoch,   500 iteration, loss:0.048\n",
      "Checking accuracy on test set\n",
      "Got 688 / 1000 correct (68.80)\n",
      "255 epoch,  1000 iteration, loss:0.054\n",
      " num 254 epoch \n",
      "####### Training Loss #######\n",
      "[0.09911356]\n",
      "Checking accuracy on test set\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 694 / 1000 correct (69.40)\n",
      "256 epoch,   500 iteration, loss:0.045\n",
      "Checking accuracy on test set\n",
      "Got 665 / 1000 correct (66.50)\n",
      "256 epoch,  1000 iteration, loss:0.051\n",
      " num 255 epoch \n",
      "####### Training Loss #######\n",
      "[0.09874156]\n",
      "Checking accuracy on test set\n",
      "Got 690 / 1000 correct (69.00)\n",
      "257 epoch,   500 iteration, loss:0.044\n",
      "Checking accuracy on test set\n",
      "Got 618 / 1000 correct (61.80)\n",
      "257 epoch,  1000 iteration, loss:0.048\n",
      " num 256 epoch \n",
      "####### Training Loss #######\n",
      "[0.10440614]\n",
      "Checking accuracy on test set\n",
      "Got 707 / 1000 correct (70.70)\n",
      "258 epoch,   500 iteration, loss:0.048\n",
      "Checking accuracy on test set\n",
      "Got 672 / 1000 correct (67.20)\n",
      "258 epoch,  1000 iteration, loss:0.051\n",
      " num 257 epoch \n",
      "####### Training Loss #######\n",
      "[0.09511678]\n",
      "Checking accuracy on test set\n",
      "Got 698 / 1000 correct (69.80)\n",
      "259 epoch,   500 iteration, loss:0.053\n",
      "Checking accuracy on test set\n",
      "Got 682 / 1000 correct (68.20)\n",
      "259 epoch,  1000 iteration, loss:0.036\n",
      " num 258 epoch \n",
      "####### Training Loss #######\n",
      "[0.09155028]\n",
      "Checking accuracy on test set\n",
      "Got 690 / 1000 correct (69.00)\n",
      "260 epoch,   500 iteration, loss:0.047\n",
      "Checking accuracy on test set\n",
      "Got 677 / 1000 correct (67.70)\n",
      "260 epoch,  1000 iteration, loss:0.046\n",
      " num 259 epoch \n",
      "####### Training Loss #######\n",
      "[0.09240824]\n",
      "Checking accuracy on test set\n",
      "Got 681 / 1000 correct (68.10)\n",
      "261 epoch,   500 iteration, loss:0.045\n",
      "Checking accuracy on test set\n",
      "Got 670 / 1000 correct (67.00)\n",
      "261 epoch,  1000 iteration, loss:0.048\n",
      " num 260 epoch \n",
      "####### Training Loss #######\n",
      "[0.0924882]\n",
      "Checking accuracy on test set\n",
      "Got 690 / 1000 correct (69.00)\n",
      "262 epoch,   500 iteration, loss:0.059\n",
      "Checking accuracy on test set\n",
      "Got 693 / 1000 correct (69.30)\n",
      "262 epoch,  1000 iteration, loss:0.041\n",
      " num 261 epoch \n",
      "####### Training Loss #######\n",
      "[0.09814518]\n",
      "Checking accuracy on test set\n",
      "Got 687 / 1000 correct (68.70)\n",
      "263 epoch,   500 iteration, loss:0.053\n",
      "Checking accuracy on test set\n",
      "Got 704 / 1000 correct (70.40)\n",
      "263 epoch,  1000 iteration, loss:0.050\n",
      " num 262 epoch \n",
      "####### Training Loss #######\n",
      "[0.09993958]\n",
      "Checking accuracy on test set\n",
      "Got 683 / 1000 correct (68.30)\n",
      "264 epoch,   500 iteration, loss:0.049\n",
      "Checking accuracy on test set\n",
      "Got 695 / 1000 correct (69.50)\n",
      "264 epoch,  1000 iteration, loss:0.047\n",
      " num 263 epoch \n",
      "####### Training Loss #######\n",
      "[0.08832677]\n",
      "Checking accuracy on test set\n",
      "Got 690 / 1000 correct (69.00)\n",
      "265 epoch,   500 iteration, loss:0.057\n",
      "Checking accuracy on test set\n",
      "Got 672 / 1000 correct (67.20)\n",
      "265 epoch,  1000 iteration, loss:0.063\n",
      " num 264 epoch \n",
      "####### Training Loss #######\n",
      "[0.11548]\n",
      "Checking accuracy on test set\n",
      "Got 698 / 1000 correct (69.80)\n",
      "266 epoch,   500 iteration, loss:0.048\n",
      "Checking accuracy on test set\n",
      "Got 686 / 1000 correct (68.60)\n",
      "266 epoch,  1000 iteration, loss:0.049\n",
      " num 265 epoch \n",
      "####### Training Loss #######\n",
      "[0.09673841]\n",
      "Checking accuracy on test set\n",
      "Got 679 / 1000 correct (67.90)\n",
      "267 epoch,   500 iteration, loss:0.043\n",
      "Checking accuracy on test set\n",
      "Got 667 / 1000 correct (66.70)\n",
      "267 epoch,  1000 iteration, loss:0.046\n",
      " num 266 epoch \n",
      "####### Training Loss #######\n",
      "[0.08934627]\n",
      "Checking accuracy on test set\n",
      "Got 683 / 1000 correct (68.30)\n",
      "268 epoch,   500 iteration, loss:0.054\n",
      "Checking accuracy on test set\n",
      "Got 697 / 1000 correct (69.70)\n",
      "268 epoch,  1000 iteration, loss:0.049\n",
      " num 267 epoch \n",
      "####### Training Loss #######\n",
      "[0.10419782]\n",
      "Checking accuracy on test set\n",
      "Got 700 / 1000 correct (70.00)\n",
      "269 epoch,   500 iteration, loss:0.042\n",
      "Checking accuracy on test set\n",
      "Got 662 / 1000 correct (66.20)\n",
      "269 epoch,  1000 iteration, loss:0.065\n",
      " num 268 epoch \n",
      "####### Training Loss #######\n",
      "[0.10911161]\n",
      "Checking accuracy on test set\n",
      "Got 708 / 1000 correct (70.80)\n",
      "270 epoch,   500 iteration, loss:0.046\n",
      "Checking accuracy on test set\n",
      "Got 706 / 1000 correct (70.60)\n",
      "270 epoch,  1000 iteration, loss:0.046\n",
      " num 269 epoch \n",
      "####### Training Loss #######\n",
      "[0.09066319]\n",
      "Checking accuracy on test set\n",
      "Got 696 / 1000 correct (69.60)\n",
      "271 epoch,   500 iteration, loss:0.050\n",
      "Checking accuracy on test set\n",
      "Got 691 / 1000 correct (69.10)\n",
      "271 epoch,  1000 iteration, loss:0.045\n",
      " num 270 epoch \n",
      "####### Training Loss #######\n",
      "[0.09185127]\n",
      "Checking accuracy on test set\n",
      "Got 695 / 1000 correct (69.50)\n",
      "272 epoch,   500 iteration, loss:0.045\n",
      "Checking accuracy on test set\n",
      "Got 694 / 1000 correct (69.40)\n",
      "272 epoch,  1000 iteration, loss:0.050\n",
      " num 271 epoch \n",
      "####### Training Loss #######\n",
      "[0.09721717]\n",
      "Checking accuracy on test set\n",
      "Got 687 / 1000 correct (68.70)\n",
      "273 epoch,   500 iteration, loss:0.059\n",
      "Checking accuracy on test set\n",
      "Got 666 / 1000 correct (66.60)\n",
      "273 epoch,  1000 iteration, loss:0.053\n",
      " num 272 epoch \n",
      "####### Training Loss #######\n",
      "[0.11539919]\n",
      "Checking accuracy on test set\n",
      "Got 684 / 1000 correct (68.40)\n",
      "274 epoch,   500 iteration, loss:0.038\n",
      "Checking accuracy on test set\n",
      "Got 666 / 1000 correct (66.60)\n",
      "274 epoch,  1000 iteration, loss:0.054\n",
      " num 273 epoch \n",
      "####### Training Loss #######\n",
      "[0.09589698]\n",
      "Checking accuracy on test set\n",
      "Got 673 / 1000 correct (67.30)\n",
      "275 epoch,   500 iteration, loss:0.047\n",
      "Checking accuracy on test set\n",
      "Got 681 / 1000 correct (68.10)\n",
      "275 epoch,  1000 iteration, loss:0.049\n",
      " num 274 epoch \n",
      "####### Training Loss #######\n",
      "[0.09104839]\n",
      "Checking accuracy on test set\n",
      "Got 684 / 1000 correct (68.40)\n",
      "276 epoch,   500 iteration, loss:0.039\n",
      "Checking accuracy on test set\n",
      "Got 681 / 1000 correct (68.10)\n",
      "276 epoch,  1000 iteration, loss:0.042\n",
      " num 275 epoch \n",
      "####### Training Loss #######\n",
      "[0.0851878]\n",
      "Checking accuracy on test set\n",
      "Got 667 / 1000 correct (66.70)\n",
      "277 epoch,   500 iteration, loss:0.047\n",
      "Checking accuracy on test set\n",
      "Got 672 / 1000 correct (67.20)\n",
      "277 epoch,  1000 iteration, loss:0.051\n",
      " num 276 epoch \n",
      "####### Training Loss #######\n",
      "[0.09957716]\n",
      "Checking accuracy on test set\n",
      "Got 686 / 1000 correct (68.60)\n",
      "278 epoch,   500 iteration, loss:0.049\n",
      "Checking accuracy on test set\n",
      "Got 672 / 1000 correct (67.20)\n",
      "278 epoch,  1000 iteration, loss:0.050\n",
      " num 277 epoch \n",
      "####### Training Loss #######\n",
      "[0.09273646]\n",
      "Checking accuracy on test set\n",
      "Got 691 / 1000 correct (69.10)\n",
      "279 epoch,   500 iteration, loss:0.049\n",
      "Checking accuracy on test set\n",
      "Got 682 / 1000 correct (68.20)\n",
      "279 epoch,  1000 iteration, loss:0.055\n",
      " num 278 epoch \n",
      "####### Training Loss #######\n",
      "[0.10071377]\n",
      "Checking accuracy on test set\n",
      "Got 689 / 1000 correct (68.90)\n",
      "280 epoch,   500 iteration, loss:0.036\n",
      "Checking accuracy on test set\n",
      "Got 680 / 1000 correct (68.00)\n",
      "280 epoch,  1000 iteration, loss:0.041\n",
      " num 279 epoch \n",
      "####### Training Loss #######\n",
      "[0.0846371]\n",
      "Checking accuracy on test set\n",
      "Got 689 / 1000 correct (68.90)\n",
      "281 epoch,   500 iteration, loss:0.048\n",
      "Checking accuracy on test set\n",
      "Got 675 / 1000 correct (67.50)\n",
      "281 epoch,  1000 iteration, loss:0.043\n",
      " num 280 epoch \n",
      "####### Training Loss #######\n",
      "[0.08951041]\n",
      "Checking accuracy on test set\n",
      "Got 680 / 1000 correct (68.00)\n",
      "282 epoch,   500 iteration, loss:0.038\n",
      "Checking accuracy on test set\n",
      "Got 671 / 1000 correct (67.10)\n",
      "282 epoch,  1000 iteration, loss:0.066\n",
      " num 281 epoch \n",
      "####### Training Loss #######\n",
      "[0.10252872]\n",
      "Checking accuracy on test set\n",
      "Got 679 / 1000 correct (67.90)\n",
      "283 epoch,   500 iteration, loss:0.038\n",
      "Checking accuracy on test set\n",
      "Got 690 / 1000 correct (69.00)\n",
      "283 epoch,  1000 iteration, loss:0.057\n",
      " num 282 epoch \n",
      "####### Training Loss #######\n",
      "[0.08553412]\n",
      "Checking accuracy on test set\n",
      "Got 679 / 1000 correct (67.90)\n",
      "284 epoch,   500 iteration, loss:0.054\n",
      "Checking accuracy on test set\n",
      "Got 664 / 1000 correct (66.40)\n",
      "284 epoch,  1000 iteration, loss:0.060\n",
      " num 283 epoch \n",
      "####### Training Loss #######\n",
      "[0.11195584]\n",
      "Checking accuracy on test set\n",
      "Got 678 / 1000 correct (67.80)\n",
      "285 epoch,   500 iteration, loss:0.044\n",
      "Checking accuracy on test set\n",
      "Got 690 / 1000 correct (69.00)\n",
      "285 epoch,  1000 iteration, loss:0.038\n",
      " num 284 epoch \n",
      "####### Training Loss #######\n",
      "[0.0846844]\n",
      "Checking accuracy on test set\n",
      "Got 694 / 1000 correct (69.40)\n",
      "286 epoch,   500 iteration, loss:0.046\n",
      "Checking accuracy on test set\n",
      "Got 625 / 1000 correct (62.50)\n",
      "286 epoch,  1000 iteration, loss:0.036\n",
      " num 285 epoch \n",
      "####### Training Loss #######\n",
      "[0.08131567]\n",
      "Checking accuracy on test set\n",
      "Got 661 / 1000 correct (66.10)\n",
      "287 epoch,   500 iteration, loss:0.063\n",
      "Checking accuracy on test set\n",
      "Got 692 / 1000 correct (69.20)\n",
      "287 epoch,  1000 iteration, loss:0.054\n",
      " num 286 epoch \n",
      "####### Training Loss #######\n",
      "[0.11911737]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking accuracy on test set\n",
      "Got 691 / 1000 correct (69.10)\n",
      "288 epoch,   500 iteration, loss:0.049\n",
      "Checking accuracy on test set\n",
      "Got 677 / 1000 correct (67.70)\n",
      "288 epoch,  1000 iteration, loss:0.042\n",
      " num 287 epoch \n",
      "####### Training Loss #######\n",
      "[0.09384851]\n",
      "Checking accuracy on test set\n",
      "Got 682 / 1000 correct (68.20)\n",
      "289 epoch,   500 iteration, loss:0.040\n",
      "Checking accuracy on test set\n",
      "Got 670 / 1000 correct (67.00)\n",
      "289 epoch,  1000 iteration, loss:0.048\n",
      " num 288 epoch \n",
      "####### Training Loss #######\n",
      "[0.09083735]\n",
      "Checking accuracy on test set\n",
      "Got 683 / 1000 correct (68.30)\n",
      "290 epoch,   500 iteration, loss:0.040\n",
      "Checking accuracy on test set\n",
      "Got 684 / 1000 correct (68.40)\n",
      "290 epoch,  1000 iteration, loss:0.061\n",
      " num 289 epoch \n",
      "####### Training Loss #######\n",
      "[0.09964341]\n",
      "Checking accuracy on test set\n",
      "Got 692 / 1000 correct (69.20)\n",
      "291 epoch,   500 iteration, loss:0.037\n",
      "Checking accuracy on test set\n",
      "Got 683 / 1000 correct (68.30)\n",
      "291 epoch,  1000 iteration, loss:0.043\n",
      " num 290 epoch \n",
      "####### Training Loss #######\n",
      "[0.07831195]\n",
      "Checking accuracy on test set\n",
      "Got 672 / 1000 correct (67.20)\n",
      "292 epoch,   500 iteration, loss:0.044\n",
      "Checking accuracy on test set\n",
      "Got 680 / 1000 correct (68.00)\n",
      "292 epoch,  1000 iteration, loss:0.051\n",
      " num 291 epoch \n",
      "####### Training Loss #######\n",
      "[0.09425053]\n",
      "Checking accuracy on test set\n",
      "Got 689 / 1000 correct (68.90)\n",
      "293 epoch,   500 iteration, loss:0.049\n",
      "Checking accuracy on test set\n",
      "Got 687 / 1000 correct (68.70)\n",
      "293 epoch,  1000 iteration, loss:0.050\n",
      " num 292 epoch \n",
      "####### Training Loss #######\n",
      "[0.09436119]\n",
      "Checking accuracy on test set\n",
      "Got 677 / 1000 correct (67.70)\n",
      "294 epoch,   500 iteration, loss:0.051\n",
      "Checking accuracy on test set\n",
      "Got 688 / 1000 correct (68.80)\n",
      "294 epoch,  1000 iteration, loss:0.035\n",
      " num 293 epoch \n",
      "####### Training Loss #######\n",
      "[0.09038846]\n",
      "Checking accuracy on test set\n",
      "Got 681 / 1000 correct (68.10)\n",
      "295 epoch,   500 iteration, loss:0.044\n",
      "Checking accuracy on test set\n",
      "Got 681 / 1000 correct (68.10)\n",
      "295 epoch,  1000 iteration, loss:0.046\n",
      " num 294 epoch \n",
      "####### Training Loss #######\n",
      "[0.09013375]\n",
      "Checking accuracy on test set\n",
      "Got 671 / 1000 correct (67.10)\n",
      "296 epoch,   500 iteration, loss:0.044\n",
      "Checking accuracy on test set\n",
      "Got 635 / 1000 correct (63.50)\n",
      "296 epoch,  1000 iteration, loss:0.052\n",
      " num 295 epoch \n",
      "####### Training Loss #######\n",
      "[0.09650814]\n",
      "Checking accuracy on test set\n",
      "Got 676 / 1000 correct (67.60)\n",
      "297 epoch,   500 iteration, loss:0.058\n",
      "Checking accuracy on test set\n",
      "Got 695 / 1000 correct (69.50)\n",
      "297 epoch,  1000 iteration, loss:0.035\n",
      " num 296 epoch \n",
      "####### Training Loss #######\n",
      "[0.09285989]\n",
      "Checking accuracy on test set\n",
      "Got 692 / 1000 correct (69.20)\n",
      "298 epoch,   500 iteration, loss:0.040\n",
      "Checking accuracy on test set\n",
      "Got 672 / 1000 correct (67.20)\n",
      "298 epoch,  1000 iteration, loss:0.043\n",
      " num 297 epoch \n",
      "####### Training Loss #######\n",
      "[0.08420827]\n",
      "Checking accuracy on test set\n",
      "Got 703 / 1000 correct (70.30)\n",
      "299 epoch,   500 iteration, loss:0.041\n",
      "Checking accuracy on test set\n",
      "Got 689 / 1000 correct (68.90)\n",
      "299 epoch,  1000 iteration, loss:0.046\n",
      " num 298 epoch \n",
      "####### Training Loss #######\n",
      "[0.08023956]\n",
      "Checking accuracy on test set\n",
      "Got 694 / 1000 correct (69.40)\n",
      "300 epoch,   500 iteration, loss:0.055\n",
      "Checking accuracy on test set\n",
      "Got 675 / 1000 correct (67.50)\n",
      "300 epoch,  1000 iteration, loss:0.049\n",
      " num 299 epoch \n",
      "####### Training Loss #######\n",
      "[0.10272019]\n",
      "Checking accuracy on test set\n",
      "Got 699 / 1000 correct (69.90)\n",
      "301 epoch,   500 iteration, loss:0.053\n",
      "Checking accuracy on test set\n",
      "Got 681 / 1000 correct (68.10)\n",
      "301 epoch,  1000 iteration, loss:0.052\n",
      " num 300 epoch \n",
      "####### Training Loss #######\n",
      "[0.10404984]\n",
      "Checking accuracy on test set\n",
      "Got 681 / 1000 correct (68.10)\n",
      "302 epoch,   500 iteration, loss:0.044\n",
      "Checking accuracy on test set\n",
      "Got 679 / 1000 correct (67.90)\n",
      "302 epoch,  1000 iteration, loss:0.047\n",
      " num 301 epoch \n",
      "####### Training Loss #######\n",
      "[0.09250204]\n",
      "Checking accuracy on test set\n",
      "Got 686 / 1000 correct (68.60)\n",
      "303 epoch,   500 iteration, loss:0.053\n",
      "Checking accuracy on test set\n",
      "Got 706 / 1000 correct (70.60)\n",
      "303 epoch,  1000 iteration, loss:0.030\n",
      " num 302 epoch \n",
      "####### Training Loss #######\n",
      "[0.08702041]\n",
      "Checking accuracy on test set\n",
      "Got 667 / 1000 correct (66.70)\n",
      "304 epoch,   500 iteration, loss:0.044\n",
      "Checking accuracy on test set\n",
      "Got 685 / 1000 correct (68.50)\n",
      "304 epoch,  1000 iteration, loss:0.058\n",
      " num 303 epoch \n",
      "####### Training Loss #######\n",
      "[0.09691075]\n",
      "Checking accuracy on test set\n",
      "Got 688 / 1000 correct (68.80)\n",
      "305 epoch,   500 iteration, loss:0.050\n",
      "Checking accuracy on test set\n",
      "Got 681 / 1000 correct (68.10)\n",
      "305 epoch,  1000 iteration, loss:0.058\n",
      " num 304 epoch \n",
      "####### Training Loss #######\n",
      "[0.10279669]\n",
      "Checking accuracy on test set\n",
      "Got 663 / 1000 correct (66.30)\n",
      "306 epoch,   500 iteration, loss:0.055\n",
      "Checking accuracy on test set\n",
      "Got 673 / 1000 correct (67.30)\n",
      "306 epoch,  1000 iteration, loss:0.038\n",
      " num 305 epoch \n",
      "####### Training Loss #######\n",
      "[0.09248899]\n",
      "Checking accuracy on test set\n",
      "Got 692 / 1000 correct (69.20)\n",
      "307 epoch,   500 iteration, loss:0.045\n",
      "Checking accuracy on test set\n",
      "Got 689 / 1000 correct (68.90)\n",
      "307 epoch,  1000 iteration, loss:0.045\n",
      " num 306 epoch \n",
      "####### Training Loss #######\n",
      "[0.08661239]\n",
      "Checking accuracy on test set\n",
      "Got 687 / 1000 correct (68.70)\n",
      "308 epoch,   500 iteration, loss:0.040\n",
      "Checking accuracy on test set\n",
      "Got 706 / 1000 correct (70.60)\n",
      "308 epoch,  1000 iteration, loss:0.043\n",
      " num 307 epoch \n",
      "####### Training Loss #######\n",
      "[0.08237584]\n",
      "Checking accuracy on test set\n",
      "Got 687 / 1000 correct (68.70)\n",
      "309 epoch,   500 iteration, loss:0.049\n",
      "Checking accuracy on test set\n",
      "Got 650 / 1000 correct (65.00)\n",
      "309 epoch,  1000 iteration, loss:0.044\n",
      " num 308 epoch \n",
      "####### Training Loss #######\n",
      "[0.09610886]\n",
      "Checking accuracy on test set\n",
      "Got 686 / 1000 correct (68.60)\n",
      "310 epoch,   500 iteration, loss:0.046\n",
      "Checking accuracy on test set\n",
      "Got 678 / 1000 correct (67.80)\n",
      "310 epoch,  1000 iteration, loss:0.044\n",
      " num 309 epoch \n",
      "####### Training Loss #######\n",
      "[0.09931345]\n",
      "Checking accuracy on test set\n",
      "Got 699 / 1000 correct (69.90)\n",
      "311 epoch,   500 iteration, loss:0.036\n",
      "Checking accuracy on test set\n",
      "Got 687 / 1000 correct (68.70)\n",
      "311 epoch,  1000 iteration, loss:0.043\n",
      " num 310 epoch \n",
      "####### Training Loss #######\n",
      "[0.0866566]\n",
      "Checking accuracy on test set\n",
      "Got 698 / 1000 correct (69.80)\n",
      "312 epoch,   500 iteration, loss:0.045\n",
      "Checking accuracy on test set\n",
      "Got 678 / 1000 correct (67.80)\n",
      "312 epoch,  1000 iteration, loss:0.043\n",
      " num 311 epoch \n",
      "####### Training Loss #######\n",
      "[0.08841271]\n",
      "Checking accuracy on test set\n",
      "Got 698 / 1000 correct (69.80)\n",
      "313 epoch,   500 iteration, loss:0.045\n",
      "Checking accuracy on test set\n",
      "Got 693 / 1000 correct (69.30)\n",
      "313 epoch,  1000 iteration, loss:0.045\n",
      " num 312 epoch \n",
      "####### Training Loss #######\n",
      "[0.08600151]\n",
      "Checking accuracy on test set\n",
      "Got 703 / 1000 correct (70.30)\n",
      "314 epoch,   500 iteration, loss:0.037\n",
      "Checking accuracy on test set\n",
      "Got 601 / 1000 correct (60.10)\n",
      "314 epoch,  1000 iteration, loss:0.040\n",
      " num 313 epoch \n",
      "####### Training Loss #######\n",
      "[0.08071345]\n",
      "Checking accuracy on test set\n",
      "Got 707 / 1000 correct (70.70)\n",
      "315 epoch,   500 iteration, loss:0.050\n",
      "Checking accuracy on test set\n",
      "Got 693 / 1000 correct (69.30)\n",
      "315 epoch,  1000 iteration, loss:0.042\n",
      " num 314 epoch \n",
      "####### Training Loss #######\n",
      "[0.09053191]\n",
      "Checking accuracy on test set\n",
      "Got 689 / 1000 correct (68.90)\n",
      "316 epoch,   500 iteration, loss:0.049\n",
      "Checking accuracy on test set\n",
      "Got 690 / 1000 correct (69.00)\n",
      "316 epoch,  1000 iteration, loss:0.049\n",
      " num 315 epoch \n",
      "####### Training Loss #######\n",
      "[0.09347444]\n",
      "Checking accuracy on test set\n",
      "Got 701 / 1000 correct (70.10)\n",
      "317 epoch,   500 iteration, loss:0.052\n",
      "Checking accuracy on test set\n",
      "Got 706 / 1000 correct (70.60)\n",
      "317 epoch,  1000 iteration, loss:0.049\n",
      " num 316 epoch \n",
      "####### Training Loss #######\n",
      "[0.10309956]\n",
      "Checking accuracy on test set\n",
      "Got 681 / 1000 correct (68.10)\n",
      "318 epoch,   500 iteration, loss:0.042\n",
      "Checking accuracy on test set\n",
      "Got 661 / 1000 correct (66.10)\n",
      "318 epoch,  1000 iteration, loss:0.047\n",
      " num 317 epoch \n",
      "####### Training Loss #######\n",
      "[0.09133516]\n",
      "Checking accuracy on test set\n",
      "Got 684 / 1000 correct (68.40)\n",
      "319 epoch,   500 iteration, loss:0.046\n",
      "Checking accuracy on test set\n",
      "Got 683 / 1000 correct (68.30)\n",
      "319 epoch,  1000 iteration, loss:0.043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " num 318 epoch \n",
      "####### Training Loss #######\n",
      "[0.08865211]\n",
      "Checking accuracy on test set\n",
      "Got 694 / 1000 correct (69.40)\n",
      "320 epoch,   500 iteration, loss:0.041\n",
      "Checking accuracy on test set\n",
      "Got 671 / 1000 correct (67.10)\n",
      "320 epoch,  1000 iteration, loss:0.048\n",
      " num 319 epoch \n",
      "####### Training Loss #######\n",
      "[0.08891309]\n",
      "Checking accuracy on test set\n",
      "Got 688 / 1000 correct (68.80)\n",
      "321 epoch,   500 iteration, loss:0.047\n",
      "Checking accuracy on test set\n",
      "Got 680 / 1000 correct (68.00)\n",
      "321 epoch,  1000 iteration, loss:0.045\n",
      " num 320 epoch \n",
      "####### Training Loss #######\n",
      "[0.09591152]\n",
      "Checking accuracy on test set\n",
      "Got 698 / 1000 correct (69.80)\n",
      "322 epoch,   500 iteration, loss:0.041\n",
      "Checking accuracy on test set\n",
      "Got 657 / 1000 correct (65.70)\n",
      "322 epoch,  1000 iteration, loss:0.052\n",
      " num 321 epoch \n",
      "####### Training Loss #######\n",
      "[0.09472168]\n",
      "Checking accuracy on test set\n",
      "Got 684 / 1000 correct (68.40)\n",
      "323 epoch,   500 iteration, loss:0.033\n",
      "Checking accuracy on test set\n",
      "Got 681 / 1000 correct (68.10)\n",
      "323 epoch,  1000 iteration, loss:0.048\n",
      " num 322 epoch \n",
      "####### Training Loss #######\n",
      "[0.08261166]\n",
      "Checking accuracy on test set\n",
      "Got 693 / 1000 correct (69.30)\n",
      "324 epoch,   500 iteration, loss:0.048\n",
      "Checking accuracy on test set\n",
      "Got 642 / 1000 correct (64.20)\n",
      "324 epoch,  1000 iteration, loss:0.049\n",
      " num 323 epoch \n",
      "####### Training Loss #######\n",
      "[0.09309217]\n",
      "Checking accuracy on test set\n",
      "Got 692 / 1000 correct (69.20)\n",
      "325 epoch,   500 iteration, loss:0.043\n",
      "Checking accuracy on test set\n",
      "Got 679 / 1000 correct (67.90)\n",
      "325 epoch,  1000 iteration, loss:0.051\n",
      " num 324 epoch \n",
      "####### Training Loss #######\n",
      "[0.09213423]\n",
      "Checking accuracy on test set\n",
      "Got 674 / 1000 correct (67.40)\n",
      "326 epoch,   500 iteration, loss:0.041\n",
      "Checking accuracy on test set\n",
      "Got 678 / 1000 correct (67.80)\n",
      "326 epoch,  1000 iteration, loss:0.039\n",
      " num 325 epoch \n",
      "####### Training Loss #######\n",
      "[0.08652718]\n",
      "Checking accuracy on test set\n",
      "Got 691 / 1000 correct (69.10)\n",
      "327 epoch,   500 iteration, loss:0.039\n",
      "Checking accuracy on test set\n",
      "Got 638 / 1000 correct (63.80)\n",
      "327 epoch,  1000 iteration, loss:0.038\n",
      " num 326 epoch \n",
      "####### Training Loss #######\n",
      "[0.07881631]\n",
      "Checking accuracy on test set\n",
      "Got 701 / 1000 correct (70.10)\n",
      "328 epoch,   500 iteration, loss:0.043\n",
      "Checking accuracy on test set\n",
      "Got 652 / 1000 correct (65.20)\n",
      "328 epoch,  1000 iteration, loss:0.048\n",
      " num 327 epoch \n",
      "####### Training Loss #######\n",
      "[0.09678794]\n",
      "Checking accuracy on test set\n",
      "Got 684 / 1000 correct (68.40)\n",
      "329 epoch,   500 iteration, loss:0.046\n",
      "Checking accuracy on test set\n",
      "Got 662 / 1000 correct (66.20)\n",
      "329 epoch,  1000 iteration, loss:0.053\n",
      " num 328 epoch \n",
      "####### Training Loss #######\n",
      "[0.10048091]\n",
      "Checking accuracy on test set\n",
      "Got 683 / 1000 correct (68.30)\n",
      "330 epoch,   500 iteration, loss:0.055\n",
      "Checking accuracy on test set\n",
      "Got 687 / 1000 correct (68.70)\n",
      "330 epoch,  1000 iteration, loss:0.033\n",
      " num 329 epoch \n",
      "####### Training Loss #######\n",
      "[0.07839632]\n",
      "Checking accuracy on test set\n",
      "Got 682 / 1000 correct (68.20)\n",
      "331 epoch,   500 iteration, loss:0.040\n",
      "Checking accuracy on test set\n",
      "Got 692 / 1000 correct (69.20)\n",
      "331 epoch,  1000 iteration, loss:0.044\n",
      " num 330 epoch \n",
      "####### Training Loss #######\n",
      "[0.08696245]\n",
      "Checking accuracy on test set\n",
      "Got 679 / 1000 correct (67.90)\n",
      "332 epoch,   500 iteration, loss:0.038\n",
      "Checking accuracy on test set\n",
      "Got 679 / 1000 correct (67.90)\n",
      "332 epoch,  1000 iteration, loss:0.055\n",
      " num 331 epoch \n",
      "####### Training Loss #######\n",
      "[0.09639431]\n",
      "Checking accuracy on test set\n",
      "Got 673 / 1000 correct (67.30)\n",
      "333 epoch,   500 iteration, loss:0.045\n",
      "Checking accuracy on test set\n",
      "Got 691 / 1000 correct (69.10)\n",
      "333 epoch,  1000 iteration, loss:0.049\n",
      " num 332 epoch \n",
      "####### Training Loss #######\n",
      "[0.08803328]\n",
      "Checking accuracy on test set\n",
      "Got 665 / 1000 correct (66.50)\n",
      "334 epoch,   500 iteration, loss:0.056\n",
      "Checking accuracy on test set\n",
      "Got 685 / 1000 correct (68.50)\n",
      "334 epoch,  1000 iteration, loss:0.054\n",
      " num 333 epoch \n",
      "####### Training Loss #######\n",
      "[0.11170498]\n",
      "Checking accuracy on test set\n",
      "Got 674 / 1000 correct (67.40)\n",
      "335 epoch,   500 iteration, loss:0.049\n",
      "Checking accuracy on test set\n",
      "Got 669 / 1000 correct (66.90)\n",
      "335 epoch,  1000 iteration, loss:0.050\n",
      " num 334 epoch \n",
      "####### Training Loss #######\n",
      "[0.09549082]\n",
      "Checking accuracy on test set\n",
      "Got 689 / 1000 correct (68.90)\n",
      "336 epoch,   500 iteration, loss:0.057\n",
      "Checking accuracy on test set\n",
      "Got 697 / 1000 correct (69.70)\n",
      "336 epoch,  1000 iteration, loss:0.041\n",
      " num 335 epoch \n",
      "####### Training Loss #######\n",
      "[0.09607092]\n",
      "Checking accuracy on test set\n",
      "Got 677 / 1000 correct (67.70)\n",
      "337 epoch,   500 iteration, loss:0.047\n",
      "Checking accuracy on test set\n",
      "Got 676 / 1000 correct (67.60)\n",
      "337 epoch,  1000 iteration, loss:0.042\n",
      " num 336 epoch \n",
      "####### Training Loss #######\n",
      "[0.09225581]\n",
      "Checking accuracy on test set\n",
      "Got 701 / 1000 correct (70.10)\n",
      "338 epoch,   500 iteration, loss:0.052\n",
      "Checking accuracy on test set\n",
      "Got 689 / 1000 correct (68.90)\n",
      "338 epoch,  1000 iteration, loss:0.053\n",
      " num 337 epoch \n",
      "####### Training Loss #######\n",
      "[0.09719787]\n",
      "Checking accuracy on test set\n",
      "Got 682 / 1000 correct (68.20)\n",
      "339 epoch,   500 iteration, loss:0.047\n",
      "Checking accuracy on test set\n",
      "Got 659 / 1000 correct (65.90)\n",
      "339 epoch,  1000 iteration, loss:0.046\n",
      " num 338 epoch \n",
      "####### Training Loss #######\n",
      "[0.09413122]\n",
      "Checking accuracy on test set\n",
      "Got 677 / 1000 correct (67.70)\n",
      "340 epoch,   500 iteration, loss:0.042\n",
      "Checking accuracy on test set\n",
      "Got 687 / 1000 correct (68.70)\n",
      "340 epoch,  1000 iteration, loss:0.049\n",
      " num 339 epoch \n",
      "####### Training Loss #######\n",
      "[0.08669501]\n",
      "Checking accuracy on test set\n",
      "Got 670 / 1000 correct (67.00)\n",
      "341 epoch,   500 iteration, loss:0.051\n",
      "Checking accuracy on test set\n",
      "Got 687 / 1000 correct (68.70)\n",
      "341 epoch,  1000 iteration, loss:0.047\n",
      " num 340 epoch \n",
      "####### Training Loss #######\n",
      "[0.09592965]\n",
      "Checking accuracy on test set\n",
      "Got 672 / 1000 correct (67.20)\n",
      "342 epoch,   500 iteration, loss:0.045\n",
      "Checking accuracy on test set\n",
      "Got 686 / 1000 correct (68.60)\n",
      "342 epoch,  1000 iteration, loss:0.049\n",
      " num 341 epoch \n",
      "####### Training Loss #######\n",
      "[0.09664189]\n",
      "Checking accuracy on test set\n",
      "Got 675 / 1000 correct (67.50)\n",
      "343 epoch,   500 iteration, loss:0.045\n",
      "Checking accuracy on test set\n",
      "Got 679 / 1000 correct (67.90)\n",
      "343 epoch,  1000 iteration, loss:0.049\n",
      " num 342 epoch \n",
      "####### Training Loss #######\n",
      "[0.09294496]\n",
      "Checking accuracy on test set\n",
      "Got 670 / 1000 correct (67.00)\n",
      "344 epoch,   500 iteration, loss:0.041\n",
      "Checking accuracy on test set\n",
      "Got 689 / 1000 correct (68.90)\n",
      "344 epoch,  1000 iteration, loss:0.047\n",
      " num 343 epoch \n",
      "####### Training Loss #######\n",
      "[0.08911871]\n",
      "Checking accuracy on test set\n",
      "Got 687 / 1000 correct (68.70)\n",
      "345 epoch,   500 iteration, loss:0.049\n",
      "Checking accuracy on test set\n",
      "Got 683 / 1000 correct (68.30)\n",
      "345 epoch,  1000 iteration, loss:0.060\n",
      " num 344 epoch \n",
      "####### Training Loss #######\n",
      "[0.10566721]\n",
      "Checking accuracy on test set\n",
      "Got 682 / 1000 correct (68.20)\n",
      "346 epoch,   500 iteration, loss:0.043\n",
      "Checking accuracy on test set\n",
      "Got 702 / 1000 correct (70.20)\n",
      "346 epoch,  1000 iteration, loss:0.054\n",
      " num 345 epoch \n",
      "####### Training Loss #######\n",
      "[0.09012077]\n",
      "Checking accuracy on test set\n",
      "Got 702 / 1000 correct (70.20)\n",
      "347 epoch,   500 iteration, loss:0.055\n",
      "Checking accuracy on test set\n",
      "Got 674 / 1000 correct (67.40)\n",
      "347 epoch,  1000 iteration, loss:0.037\n",
      " num 346 epoch \n",
      "####### Training Loss #######\n",
      "[0.08996581]\n",
      "Checking accuracy on test set\n",
      "Got 679 / 1000 correct (67.90)\n",
      "348 epoch,   500 iteration, loss:0.034\n",
      "Checking accuracy on test set\n",
      "Got 692 / 1000 correct (69.20)\n",
      "348 epoch,  1000 iteration, loss:0.040\n",
      " num 347 epoch \n",
      "####### Training Loss #######\n",
      "[0.07641738]\n",
      "Checking accuracy on test set\n",
      "Got 686 / 1000 correct (68.60)\n",
      "349 epoch,   500 iteration, loss:0.058\n",
      "Checking accuracy on test set\n",
      "Got 675 / 1000 correct (67.50)\n",
      "349 epoch,  1000 iteration, loss:0.058\n",
      " num 348 epoch \n",
      "####### Training Loss #######\n",
      "[0.11005427]\n",
      "Checking accuracy on test set\n",
      "Got 692 / 1000 correct (69.20)\n",
      "350 epoch,   500 iteration, loss:0.053\n",
      "Checking accuracy on test set\n",
      "Got 711 / 1000 correct (71.10)\n",
      "350 epoch,  1000 iteration, loss:0.034\n",
      " num 349 epoch \n",
      "####### Training Loss #######\n",
      "[0.08241343]\n",
      "finish training \n",
      "\n",
      "now begin saving datum for next step plotting\n",
      "now plotting accuracies and losses\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (1,) and (700,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-cc4199ef2a5e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mstride_cnn_c_class1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_ALL_Conv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStride_CNN_C\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m stride_cnn_c_class1 = running_model_B(run_num, stride_cnn_c_class1, net_name1, lr, epoch, \n\u001b[0;32m---> 10\u001b[0;31m                         loaderA_train, loaderA_test)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-f9018b16c1a9>\u001b[0m in \u001b[0;36mrunning_model_B\u001b[0;34m(run_num, net, net_name, lr_list, epoch_list, loader_train, loader_test)\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray_epoch_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'--r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'iteration'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Anacondas/anaconda3/envs/cs231n/lib/python3.6/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2747\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2748\u001b[0m     return gca().plot(\n\u001b[0;32m-> 2749\u001b[0;31m         *args, scalex=scalex, scaley=scaley, data=data, **kwargs)\n\u001b[0m\u001b[1;32m   2750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2751\u001b[0m \u001b[0;31m# Autogenerated by boilerplate.py.  Do not edit as changes will be lost.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Anacondas/anaconda3/envs/cs231n/lib/python3.6/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1783\u001b[0m                         \u001b[0;34m\"the Matplotlib list!)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlabel_namer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1784\u001b[0m                         RuntimeWarning, stacklevel=2)\n\u001b[0;32m-> 1785\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1787\u001b[0m         inner.__doc__ = _add_data_doc(inner.__doc__,\n",
      "\u001b[0;32m~/Anacondas/anaconda3/envs/cs231n/lib/python3.6/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1602\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_alias_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1604\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1605\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1606\u001b[0m             \u001b[0mlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Anacondas/anaconda3/envs/cs231n/lib/python3.6/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_grab_next_args\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    391\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Anacondas/anaconda3/envs/cs231n/lib/python3.6/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_of\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_xy_from_xy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'plot'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Anacondas/anaconda3/envs/cs231n/lib/python3.6/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_xy_from_xy\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m             raise ValueError(\"x and y must have same first dimension, but \"\n\u001b[0;32m--> 231\u001b[0;31m                              \"have shapes {} and {}\".format(x.shape, y.shape))\n\u001b[0m\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             raise ValueError(\"x and y can be no greater than 2-D, but have \"\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (1,) and (700,)"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr = [0.01, 0.005, 0.001, 0.0005]\n",
    "epoch = [200, 250, 300] # first20\n",
    "\n",
    "run_num = 10\n",
    "net_name1 = 'Stride_CNN_C_Class1'\n",
    "\n",
    "torch.cuda.set_device(1)\n",
    "stride_cnn_c_class1 = new_ALL_Conv.Stride_CNN_C()\n",
    "stride_cnn_c_class1 = running_model_B(run_num, stride_cnn_c_class1, net_name1, lr, epoch, \n",
    "                        loaderA_train, loaderA_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n",
      "begin training\n",
      "Checking accuracy on test set\n",
      "Got 170 / 1000 correct (17.00)\n",
      "1 epoch,   500 iteration, loss:1.134\n",
      "Checking accuracy on test set\n",
      "Got 284 / 1000 correct (28.40)\n",
      "1 epoch,  1000 iteration, loss:1.045\n",
      " num 0 epoch \n",
      "####### Training Loss #######\n",
      "[2.13606516]\n",
      "Checking accuracy on test set\n",
      "Got 335 / 1000 correct (33.50)\n",
      "2 epoch,   500 iteration, loss:0.953\n",
      "Checking accuracy on test set\n",
      "Got 375 / 1000 correct (37.50)\n",
      "2 epoch,  1000 iteration, loss:0.896\n",
      " num 1 epoch \n",
      "####### Training Loss #######\n",
      "[1.8232647]\n",
      "Checking accuracy on test set\n",
      "Got 429 / 1000 correct (42.90)\n",
      "3 epoch,   500 iteration, loss:0.850\n",
      "Checking accuracy on test set\n",
      "Got 444 / 1000 correct (44.40)\n",
      "3 epoch,  1000 iteration, loss:0.817\n",
      " num 2 epoch \n",
      "####### Training Loss #######\n",
      "[1.64857017]\n",
      "Checking accuracy on test set\n",
      "Got 484 / 1000 correct (48.40)\n",
      "4 epoch,   500 iteration, loss:0.793\n",
      "Checking accuracy on test set\n",
      "Got 462 / 1000 correct (46.20)\n",
      "4 epoch,  1000 iteration, loss:0.768\n",
      " num 3 epoch \n",
      "####### Training Loss #######\n",
      "[1.54764523]\n",
      "Checking accuracy on test set\n",
      "Got 509 / 1000 correct (50.90)\n",
      "5 epoch,   500 iteration, loss:0.760\n",
      "Checking accuracy on test set\n",
      "Got 477 / 1000 correct (47.70)\n",
      "5 epoch,  1000 iteration, loss:0.741\n",
      " num 4 epoch \n",
      "####### Training Loss #######\n",
      "[1.48739368]\n",
      "Checking accuracy on test set\n",
      "Got 514 / 1000 correct (51.40)\n",
      "6 epoch,   500 iteration, loss:0.726\n",
      "Checking accuracy on test set\n",
      "Got 502 / 1000 correct (50.20)\n",
      "6 epoch,  1000 iteration, loss:0.703\n",
      " num 5 epoch \n",
      "####### Training Loss #######\n",
      "[1.41256179]\n",
      "Checking accuracy on test set\n",
      "Got 518 / 1000 correct (51.80)\n",
      "7 epoch,   500 iteration, loss:0.706\n",
      "Checking accuracy on test set\n",
      "Got 503 / 1000 correct (50.30)\n",
      "7 epoch,  1000 iteration, loss:0.683\n",
      " num 6 epoch \n",
      "####### Training Loss #######\n",
      "[1.37817002]\n",
      "Checking accuracy on test set\n",
      "Got 530 / 1000 correct (53.00)\n",
      "8 epoch,   500 iteration, loss:0.678\n",
      "Checking accuracy on test set\n",
      "Got 530 / 1000 correct (53.00)\n",
      "8 epoch,  1000 iteration, loss:0.653\n",
      " num 7 epoch \n",
      "####### Training Loss #######\n",
      "[1.3229237]\n",
      "Checking accuracy on test set\n",
      "Got 540 / 1000 correct (54.00)\n",
      "9 epoch,   500 iteration, loss:0.665\n",
      "Checking accuracy on test set\n",
      "Got 531 / 1000 correct (53.10)\n",
      "9 epoch,  1000 iteration, loss:0.640\n",
      " num 8 epoch \n",
      "####### Training Loss #######\n",
      "[1.29895857]\n",
      "Checking accuracy on test set\n",
      "Got 562 / 1000 correct (56.20)\n",
      "10 epoch,   500 iteration, loss:0.644\n",
      "Checking accuracy on test set\n",
      "Got 542 / 1000 correct (54.20)\n",
      "10 epoch,  1000 iteration, loss:0.616\n",
      " num 9 epoch \n",
      "####### Training Loss #######\n",
      "[1.25188967]\n",
      "Checking accuracy on test set\n",
      "Got 564 / 1000 correct (56.40)\n",
      "11 epoch,   500 iteration, loss:0.641\n",
      "Checking accuracy on test set\n",
      "Got 535 / 1000 correct (53.50)\n",
      "11 epoch,  1000 iteration, loss:0.605\n",
      " num 10 epoch \n",
      "####### Training Loss #######\n",
      "[1.23684911]\n",
      "Checking accuracy on test set\n",
      "Got 581 / 1000 correct (58.10)\n",
      "12 epoch,   500 iteration, loss:0.625\n",
      "Checking accuracy on test set\n",
      "Got 550 / 1000 correct (55.00)\n",
      "12 epoch,  1000 iteration, loss:0.594\n",
      " num 11 epoch \n",
      "####### Training Loss #######\n",
      "[1.20424479]\n",
      "Checking accuracy on test set\n",
      "Got 575 / 1000 correct (57.50)\n",
      "13 epoch,   500 iteration, loss:0.601\n",
      "Checking accuracy on test set\n",
      "Got 560 / 1000 correct (56.00)\n",
      "13 epoch,  1000 iteration, loss:0.572\n",
      " num 12 epoch \n",
      "####### Training Loss #######\n",
      "[1.16644317]\n",
      "Checking accuracy on test set\n",
      "Got 608 / 1000 correct (60.80)\n",
      "14 epoch,   500 iteration, loss:0.584\n",
      "Checking accuracy on test set\n",
      "Got 572 / 1000 correct (57.20)\n",
      "14 epoch,  1000 iteration, loss:0.562\n",
      " num 13 epoch \n",
      "####### Training Loss #######\n",
      "[1.13564827]\n",
      "Checking accuracy on test set\n",
      "Got 587 / 1000 correct (58.70)\n",
      "15 epoch,   500 iteration, loss:0.578\n",
      "Checking accuracy on test set\n",
      "Got 595 / 1000 correct (59.50)\n",
      "15 epoch,  1000 iteration, loss:0.544\n",
      " num 14 epoch \n",
      "####### Training Loss #######\n",
      "[1.1140794]\n",
      "Checking accuracy on test set\n",
      "Got 593 / 1000 correct (59.30)\n",
      "16 epoch,   500 iteration, loss:0.565\n",
      "Checking accuracy on test set\n",
      "Got 596 / 1000 correct (59.60)\n",
      "16 epoch,  1000 iteration, loss:0.529\n",
      " num 15 epoch \n",
      "####### Training Loss #######\n",
      "[1.0821195]\n",
      "Checking accuracy on test set\n",
      "Got 613 / 1000 correct (61.30)\n",
      "17 epoch,   500 iteration, loss:0.555\n",
      "Checking accuracy on test set\n",
      "Got 589 / 1000 correct (58.90)\n",
      "17 epoch,  1000 iteration, loss:0.511\n",
      " num 16 epoch \n",
      "####### Training Loss #######\n",
      "[1.05814005]\n",
      "Checking accuracy on test set\n",
      "Got 606 / 1000 correct (60.60)\n",
      "18 epoch,   500 iteration, loss:0.544\n",
      "Checking accuracy on test set\n",
      "Got 587 / 1000 correct (58.70)\n",
      "18 epoch,  1000 iteration, loss:0.502\n",
      " num 17 epoch \n",
      "####### Training Loss #######\n",
      "[1.03712291]\n",
      "Checking accuracy on test set\n",
      "Got 614 / 1000 correct (61.40)\n",
      "19 epoch,   500 iteration, loss:0.524\n",
      "Checking accuracy on test set\n",
      "Got 595 / 1000 correct (59.50)\n",
      "19 epoch,  1000 iteration, loss:0.506\n",
      " num 18 epoch \n",
      "####### Training Loss #######\n",
      "[1.02227643]\n",
      "Checking accuracy on test set\n",
      "Got 598 / 1000 correct (59.80)\n",
      "20 epoch,   500 iteration, loss:0.515\n",
      "Checking accuracy on test set\n",
      "Got 610 / 1000 correct (61.00)\n",
      "20 epoch,  1000 iteration, loss:0.476\n",
      " num 19 epoch \n",
      "####### Training Loss #######\n",
      "[0.98987997]\n",
      "Checking accuracy on test set\n",
      "Got 633 / 1000 correct (63.30)\n",
      "21 epoch,   500 iteration, loss:0.514\n",
      "Checking accuracy on test set\n",
      "Got 608 / 1000 correct (60.80)\n",
      "21 epoch,  1000 iteration, loss:0.469\n",
      " num 20 epoch \n",
      "####### Training Loss #######\n",
      "[0.97104111]\n",
      "Checking accuracy on test set\n",
      "Got 628 / 1000 correct (62.80)\n",
      "22 epoch,   500 iteration, loss:0.493\n",
      "Checking accuracy on test set\n",
      "Got 607 / 1000 correct (60.70)\n",
      "22 epoch,  1000 iteration, loss:0.455\n",
      " num 21 epoch \n",
      "####### Training Loss #######\n",
      "[0.94236827]\n",
      "Checking accuracy on test set\n",
      "Got 629 / 1000 correct (62.90)\n",
      "23 epoch,   500 iteration, loss:0.481\n",
      "Checking accuracy on test set\n",
      "Got 627 / 1000 correct (62.70)\n",
      "23 epoch,  1000 iteration, loss:0.444\n",
      " num 22 epoch \n",
      "####### Training Loss #######\n",
      "[0.92250239]\n",
      "Checking accuracy on test set\n",
      "Got 634 / 1000 correct (63.40)\n",
      "24 epoch,   500 iteration, loss:0.475\n",
      "Checking accuracy on test set\n",
      "Got 633 / 1000 correct (63.30)\n",
      "24 epoch,  1000 iteration, loss:0.436\n",
      " num 23 epoch \n",
      "####### Training Loss #######\n",
      "[0.91109507]\n",
      "Checking accuracy on test set\n",
      "Got 629 / 1000 correct (62.90)\n",
      "25 epoch,   500 iteration, loss:0.457\n",
      "Checking accuracy on test set\n",
      "Got 611 / 1000 correct (61.10)\n",
      "25 epoch,  1000 iteration, loss:0.427\n",
      " num 24 epoch \n",
      "####### Training Loss #######\n",
      "[0.87803208]\n",
      "Checking accuracy on test set\n",
      "Got 629 / 1000 correct (62.90)\n",
      "26 epoch,   500 iteration, loss:0.452\n",
      "Checking accuracy on test set\n",
      "Got 625 / 1000 correct (62.50)\n",
      "26 epoch,  1000 iteration, loss:0.420\n",
      " num 25 epoch \n",
      "####### Training Loss #######\n",
      "[0.86645522]\n",
      "Checking accuracy on test set\n",
      "Got 632 / 1000 correct (63.20)\n",
      "27 epoch,   500 iteration, loss:0.448\n",
      "Checking accuracy on test set\n",
      "Got 616 / 1000 correct (61.60)\n",
      "27 epoch,  1000 iteration, loss:0.410\n",
      " num 26 epoch \n",
      "####### Training Loss #######\n",
      "[0.84827542]\n",
      "Checking accuracy on test set\n",
      "Got 654 / 1000 correct (65.40)\n",
      "28 epoch,   500 iteration, loss:0.431\n",
      "Checking accuracy on test set\n",
      "Got 638 / 1000 correct (63.80)\n",
      "28 epoch,  1000 iteration, loss:0.404\n",
      " num 27 epoch \n",
      "####### Training Loss #######\n",
      "[0.82426369]\n",
      "Checking accuracy on test set\n",
      "Got 653 / 1000 correct (65.30)\n",
      "29 epoch,   500 iteration, loss:0.419\n",
      "Checking accuracy on test set\n",
      "Got 656 / 1000 correct (65.60)\n",
      "29 epoch,  1000 iteration, loss:0.403\n",
      " num 28 epoch \n",
      "####### Training Loss #######\n",
      "[0.8224273]\n",
      "Checking accuracy on test set\n",
      "Got 653 / 1000 correct (65.30)\n",
      "30 epoch,   500 iteration, loss:0.423\n",
      "Checking accuracy on test set\n",
      "Got 640 / 1000 correct (64.00)\n",
      "30 epoch,  1000 iteration, loss:0.393\n",
      " num 29 epoch \n",
      "####### Training Loss #######\n",
      "[0.8078431]\n",
      "Checking accuracy on test set\n",
      "Got 639 / 1000 correct (63.90)\n",
      "31 epoch,   500 iteration, loss:0.404\n",
      "Checking accuracy on test set\n",
      "Got 657 / 1000 correct (65.70)\n",
      "31 epoch,  1000 iteration, loss:0.385\n",
      " num 30 epoch \n",
      "####### Training Loss #######\n",
      "[0.78607427]\n",
      "Checking accuracy on test set\n",
      "Got 645 / 1000 correct (64.50)\n",
      "32 epoch,   500 iteration, loss:0.406\n",
      "Checking accuracy on test set\n",
      "Got 627 / 1000 correct (62.70)\n",
      "32 epoch,  1000 iteration, loss:0.369\n",
      " num 31 epoch \n",
      "####### Training Loss #######\n",
      "[0.76829402]\n",
      "Checking accuracy on test set\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 649 / 1000 correct (64.90)\n",
      "33 epoch,   500 iteration, loss:0.386\n",
      "Checking accuracy on test set\n",
      "Got 650 / 1000 correct (65.00)\n",
      "33 epoch,  1000 iteration, loss:0.360\n",
      " num 32 epoch \n",
      "####### Training Loss #######\n",
      "[0.74567557]\n",
      "Checking accuracy on test set\n",
      "Got 662 / 1000 correct (66.20)\n",
      "34 epoch,   500 iteration, loss:0.392\n",
      "Checking accuracy on test set\n",
      "Got 630 / 1000 correct (63.00)\n",
      "34 epoch,  1000 iteration, loss:0.359\n",
      " num 33 epoch \n",
      "####### Training Loss #######\n",
      "[0.74192553]\n",
      "Checking accuracy on test set\n",
      "Got 645 / 1000 correct (64.50)\n",
      "35 epoch,   500 iteration, loss:0.377\n",
      "Checking accuracy on test set\n",
      "Got 661 / 1000 correct (66.10)\n",
      "35 epoch,  1000 iteration, loss:0.364\n",
      " num 34 epoch \n",
      "####### Training Loss #######\n",
      "[0.7319085]\n",
      "Checking accuracy on test set\n",
      "Got 651 / 1000 correct (65.10)\n",
      "36 epoch,   500 iteration, loss:0.375\n",
      "Checking accuracy on test set\n",
      "Got 668 / 1000 correct (66.80)\n",
      "36 epoch,  1000 iteration, loss:0.347\n",
      " num 35 epoch \n",
      "####### Training Loss #######\n",
      "[0.71906193]\n",
      "Checking accuracy on test set\n",
      "Got 655 / 1000 correct (65.50)\n",
      "37 epoch,   500 iteration, loss:0.361\n",
      "Checking accuracy on test set\n",
      "Got 633 / 1000 correct (63.30)\n",
      "37 epoch,  1000 iteration, loss:0.327\n",
      " num 36 epoch \n",
      "####### Training Loss #######\n",
      "[0.69063104]\n",
      "Checking accuracy on test set\n",
      "Got 658 / 1000 correct (65.80)\n",
      "38 epoch,   500 iteration, loss:0.354\n",
      "Checking accuracy on test set\n",
      "Got 640 / 1000 correct (64.00)\n",
      "38 epoch,  1000 iteration, loss:0.321\n",
      " num 37 epoch \n",
      "####### Training Loss #######\n",
      "[0.67714055]\n",
      "Checking accuracy on test set\n",
      "Got 666 / 1000 correct (66.60)\n",
      "39 epoch,   500 iteration, loss:0.343\n",
      "Checking accuracy on test set\n",
      "Got 659 / 1000 correct (65.90)\n",
      "39 epoch,  1000 iteration, loss:0.318\n",
      " num 38 epoch \n",
      "####### Training Loss #######\n",
      "[0.66285001]\n",
      "Checking accuracy on test set\n",
      "Got 652 / 1000 correct (65.20)\n",
      "40 epoch,   500 iteration, loss:0.338\n",
      "Checking accuracy on test set\n",
      "Got 667 / 1000 correct (66.70)\n",
      "40 epoch,  1000 iteration, loss:0.302\n",
      " num 39 epoch \n",
      "####### Training Loss #######\n",
      "[0.63338397]\n",
      "Checking accuracy on test set\n",
      "Got 657 / 1000 correct (65.70)\n",
      "41 epoch,   500 iteration, loss:0.333\n",
      "Checking accuracy on test set\n",
      "Got 661 / 1000 correct (66.10)\n",
      "41 epoch,  1000 iteration, loss:0.306\n",
      " num 40 epoch \n",
      "####### Training Loss #######\n",
      "[0.63542029]\n",
      "Checking accuracy on test set\n",
      "Got 658 / 1000 correct (65.80)\n",
      "42 epoch,   500 iteration, loss:0.318\n",
      "Checking accuracy on test set\n",
      "Got 640 / 1000 correct (64.00)\n",
      "42 epoch,  1000 iteration, loss:0.295\n",
      " num 41 epoch \n",
      "####### Training Loss #######\n",
      "[0.61445871]\n",
      "Checking accuracy on test set\n",
      "Got 665 / 1000 correct (66.50)\n",
      "43 epoch,   500 iteration, loss:0.309\n",
      "Checking accuracy on test set\n",
      "Got 662 / 1000 correct (66.20)\n",
      "43 epoch,  1000 iteration, loss:0.296\n",
      " num 42 epoch \n",
      "####### Training Loss #######\n",
      "[0.59526757]\n",
      "Checking accuracy on test set\n",
      "Got 663 / 1000 correct (66.30)\n",
      "44 epoch,   500 iteration, loss:0.309\n",
      "Checking accuracy on test set\n",
      "Got 650 / 1000 correct (65.00)\n",
      "44 epoch,  1000 iteration, loss:0.275\n",
      " num 43 epoch \n",
      "####### Training Loss #######\n",
      "[0.58122474]\n",
      "Checking accuracy on test set\n",
      "Got 646 / 1000 correct (64.60)\n",
      "45 epoch,   500 iteration, loss:0.294\n",
      "Checking accuracy on test set\n",
      "Got 672 / 1000 correct (67.20)\n",
      "45 epoch,  1000 iteration, loss:0.272\n",
      " num 44 epoch \n",
      "####### Training Loss #######\n",
      "[0.56450456]\n",
      "Checking accuracy on test set\n",
      "Got 649 / 1000 correct (64.90)\n",
      "46 epoch,   500 iteration, loss:0.291\n",
      "Checking accuracy on test set\n",
      "Got 652 / 1000 correct (65.20)\n",
      "46 epoch,  1000 iteration, loss:0.270\n",
      " num 45 epoch \n",
      "####### Training Loss #######\n",
      "[0.55106085]\n",
      "Checking accuracy on test set\n",
      "Got 652 / 1000 correct (65.20)\n",
      "47 epoch,   500 iteration, loss:0.284\n",
      "Checking accuracy on test set\n",
      "Got 664 / 1000 correct (66.40)\n",
      "47 epoch,  1000 iteration, loss:0.266\n",
      " num 46 epoch \n",
      "####### Training Loss #######\n",
      "[0.55696403]\n",
      "Checking accuracy on test set\n",
      "Got 661 / 1000 correct (66.10)\n",
      "48 epoch,   500 iteration, loss:0.268\n",
      "Checking accuracy on test set\n",
      "Got 664 / 1000 correct (66.40)\n",
      "48 epoch,  1000 iteration, loss:0.246\n",
      " num 47 epoch \n",
      "####### Training Loss #######\n",
      "[0.51637157]\n",
      "Checking accuracy on test set\n",
      "Got 657 / 1000 correct (65.70)\n",
      "49 epoch,   500 iteration, loss:0.257\n",
      "Checking accuracy on test set\n",
      "Got 669 / 1000 correct (66.90)\n",
      "49 epoch,  1000 iteration, loss:0.242\n",
      " num 48 epoch \n",
      "####### Training Loss #######\n",
      "[0.49401458]\n",
      "Checking accuracy on test set\n",
      "Got 677 / 1000 correct (67.70)\n",
      "50 epoch,   500 iteration, loss:0.244\n",
      "Checking accuracy on test set\n",
      "Got 659 / 1000 correct (65.90)\n",
      "50 epoch,  1000 iteration, loss:0.250\n",
      " num 49 epoch \n",
      "####### Training Loss #######\n",
      "[0.49814874]\n",
      "Checking accuracy on test set\n",
      "Got 671 / 1000 correct (67.10)\n",
      "51 epoch,   500 iteration, loss:0.258\n",
      "Checking accuracy on test set\n",
      "Got 656 / 1000 correct (65.60)\n",
      "51 epoch,  1000 iteration, loss:0.238\n",
      " num 50 epoch \n",
      "####### Training Loss #######\n",
      "[0.48896789]\n",
      "Checking accuracy on test set\n",
      "Got 651 / 1000 correct (65.10)\n",
      "52 epoch,   500 iteration, loss:0.243\n",
      "Checking accuracy on test set\n",
      "Got 659 / 1000 correct (65.90)\n",
      "52 epoch,  1000 iteration, loss:0.228\n",
      " num 51 epoch \n",
      "####### Training Loss #######\n",
      "[0.46708747]\n",
      "Checking accuracy on test set\n",
      "Got 667 / 1000 correct (66.70)\n",
      "53 epoch,   500 iteration, loss:0.250\n",
      "Checking accuracy on test set\n",
      "Got 667 / 1000 correct (66.70)\n",
      "53 epoch,  1000 iteration, loss:0.212\n",
      " num 52 epoch \n",
      "####### Training Loss #######\n",
      "[0.46471565]\n",
      "Checking accuracy on test set\n",
      "Got 670 / 1000 correct (67.00)\n",
      "54 epoch,   500 iteration, loss:0.219\n",
      "Checking accuracy on test set\n",
      "Got 655 / 1000 correct (65.50)\n",
      "54 epoch,  1000 iteration, loss:0.215\n",
      " num 53 epoch \n",
      "####### Training Loss #######\n",
      "[0.4346146]\n",
      "Checking accuracy on test set\n",
      "Got 670 / 1000 correct (67.00)\n",
      "55 epoch,   500 iteration, loss:0.228\n",
      "Checking accuracy on test set\n",
      "Got 670 / 1000 correct (67.00)\n",
      "55 epoch,  1000 iteration, loss:0.210\n",
      " num 54 epoch \n",
      "####### Training Loss #######\n",
      "[0.43849524]\n",
      "Checking accuracy on test set\n",
      "Got 661 / 1000 correct (66.10)\n",
      "56 epoch,   500 iteration, loss:0.229\n",
      "Checking accuracy on test set\n",
      "Got 661 / 1000 correct (66.10)\n",
      "56 epoch,  1000 iteration, loss:0.210\n",
      " num 55 epoch \n",
      "####### Training Loss #######\n",
      "[0.43303431]\n",
      "Checking accuracy on test set\n",
      "Got 640 / 1000 correct (64.00)\n",
      "57 epoch,   500 iteration, loss:0.213\n",
      "Checking accuracy on test set\n",
      "Got 663 / 1000 correct (66.30)\n",
      "57 epoch,  1000 iteration, loss:0.208\n",
      " num 56 epoch \n",
      "####### Training Loss #######\n",
      "[0.42122191]\n",
      "Checking accuracy on test set\n",
      "Got 683 / 1000 correct (68.30)\n",
      "58 epoch,   500 iteration, loss:0.213\n",
      "Checking accuracy on test set\n",
      "Got 668 / 1000 correct (66.80)\n",
      "58 epoch,  1000 iteration, loss:0.194\n",
      " num 57 epoch \n",
      "####### Training Loss #######\n",
      "[0.39791659]\n",
      "Checking accuracy on test set\n",
      "Got 669 / 1000 correct (66.90)\n",
      "59 epoch,   500 iteration, loss:0.202\n",
      "Checking accuracy on test set\n",
      "Got 675 / 1000 correct (67.50)\n",
      "59 epoch,  1000 iteration, loss:0.197\n",
      " num 58 epoch \n",
      "####### Training Loss #######\n",
      "[0.39526732]\n",
      "Checking accuracy on test set\n",
      "Got 669 / 1000 correct (66.90)\n",
      "60 epoch,   500 iteration, loss:0.200\n",
      "Checking accuracy on test set\n",
      "Got 660 / 1000 correct (66.00)\n",
      "60 epoch,  1000 iteration, loss:0.183\n",
      " num 59 epoch \n",
      "####### Training Loss #######\n",
      "[0.38062316]\n",
      "Checking accuracy on test set\n",
      "Got 678 / 1000 correct (67.80)\n",
      "61 epoch,   500 iteration, loss:0.181\n",
      "Checking accuracy on test set\n",
      "Got 653 / 1000 correct (65.30)\n",
      "61 epoch,  1000 iteration, loss:0.180\n",
      " num 60 epoch \n",
      "####### Training Loss #######\n",
      "[0.36499112]\n",
      "Checking accuracy on test set\n",
      "Got 670 / 1000 correct (67.00)\n",
      "62 epoch,   500 iteration, loss:0.177\n",
      "Checking accuracy on test set\n",
      "Got 644 / 1000 correct (64.40)\n",
      "62 epoch,  1000 iteration, loss:0.172\n",
      " num 61 epoch \n",
      "####### Training Loss #######\n",
      "[0.35044458]\n",
      "Checking accuracy on test set\n",
      "Got 670 / 1000 correct (67.00)\n",
      "63 epoch,   500 iteration, loss:0.191\n",
      "Checking accuracy on test set\n",
      "Got 682 / 1000 correct (68.20)\n",
      "63 epoch,  1000 iteration, loss:0.165\n",
      " num 62 epoch \n",
      "####### Training Loss #######\n",
      "[0.35915982]\n",
      "Checking accuracy on test set\n",
      "Got 682 / 1000 correct (68.20)\n",
      "64 epoch,   500 iteration, loss:0.169\n",
      "Checking accuracy on test set\n",
      "Got 677 / 1000 correct (67.70)\n",
      "64 epoch,  1000 iteration, loss:0.155\n",
      " num 63 epoch \n",
      "####### Training Loss #######\n",
      "[0.32407432]\n",
      "Checking accuracy on test set\n",
      "Got 688 / 1000 correct (68.80)\n",
      "65 epoch,   500 iteration, loss:0.171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking accuracy on test set\n",
      "Got 674 / 1000 correct (67.40)\n",
      "65 epoch,  1000 iteration, loss:0.165\n",
      " num 64 epoch \n",
      "####### Training Loss #######\n",
      "[0.3368234]\n",
      "Checking accuracy on test set\n",
      "Got 671 / 1000 correct (67.10)\n",
      "66 epoch,   500 iteration, loss:0.157\n",
      "Checking accuracy on test set\n",
      "Got 660 / 1000 correct (66.00)\n",
      "66 epoch,  1000 iteration, loss:0.171\n",
      " num 65 epoch \n",
      "####### Training Loss #######\n",
      "[0.33417476]\n",
      "Checking accuracy on test set\n",
      "Got 676 / 1000 correct (67.60)\n",
      "67 epoch,   500 iteration, loss:0.162\n",
      "Checking accuracy on test set\n",
      "Got 678 / 1000 correct (67.80)\n",
      "67 epoch,  1000 iteration, loss:0.164\n",
      " num 66 epoch \n",
      "####### Training Loss #######\n",
      "[0.32018879]\n",
      "Checking accuracy on test set\n",
      "Got 654 / 1000 correct (65.40)\n",
      "68 epoch,   500 iteration, loss:0.154\n",
      "Checking accuracy on test set\n",
      "Got 675 / 1000 correct (67.50)\n",
      "68 epoch,  1000 iteration, loss:0.144\n",
      " num 67 epoch \n",
      "####### Training Loss #######\n",
      "[0.30660885]\n",
      "Checking accuracy on test set\n",
      "Got 678 / 1000 correct (67.80)\n",
      "69 epoch,   500 iteration, loss:0.167\n",
      "Checking accuracy on test set\n",
      "Got 675 / 1000 correct (67.50)\n",
      "69 epoch,  1000 iteration, loss:0.154\n",
      " num 68 epoch \n",
      "####### Training Loss #######\n",
      "[0.32870205]\n",
      "Checking accuracy on test set\n",
      "Got 668 / 1000 correct (66.80)\n",
      "70 epoch,   500 iteration, loss:0.156\n",
      "Checking accuracy on test set\n",
      "Got 655 / 1000 correct (65.50)\n",
      "70 epoch,  1000 iteration, loss:0.147\n",
      " num 69 epoch \n",
      "####### Training Loss #######\n",
      "[0.31095247]\n",
      "Checking accuracy on test set\n",
      "Got 676 / 1000 correct (67.60)\n",
      "71 epoch,   500 iteration, loss:0.142\n",
      "Checking accuracy on test set\n",
      "Got 652 / 1000 correct (65.20)\n",
      "71 epoch,  1000 iteration, loss:0.143\n",
      " num 70 epoch \n",
      "####### Training Loss #######\n",
      "[0.28480585]\n",
      "Checking accuracy on test set\n",
      "Got 661 / 1000 correct (66.10)\n",
      "72 epoch,   500 iteration, loss:0.142\n",
      "Checking accuracy on test set\n",
      "Got 685 / 1000 correct (68.50)\n",
      "72 epoch,  1000 iteration, loss:0.137\n",
      " num 71 epoch \n",
      "####### Training Loss #######\n",
      "[0.27979351]\n",
      "Checking accuracy on test set\n",
      "Got 678 / 1000 correct (67.80)\n",
      "73 epoch,   500 iteration, loss:0.151\n",
      "Checking accuracy on test set\n",
      "Got 679 / 1000 correct (67.90)\n",
      "73 epoch,  1000 iteration, loss:0.137\n",
      " num 72 epoch \n",
      "####### Training Loss #######\n",
      "[0.28819834]\n",
      "Checking accuracy on test set\n",
      "Got 667 / 1000 correct (66.70)\n",
      "74 epoch,   500 iteration, loss:0.135\n",
      "Checking accuracy on test set\n",
      "Got 674 / 1000 correct (67.40)\n",
      "74 epoch,  1000 iteration, loss:0.149\n",
      " num 73 epoch \n",
      "####### Training Loss #######\n",
      "[0.27664929]\n",
      "Checking accuracy on test set\n",
      "Got 677 / 1000 correct (67.70)\n",
      "75 epoch,   500 iteration, loss:0.131\n",
      "Checking accuracy on test set\n",
      "Got 681 / 1000 correct (68.10)\n",
      "75 epoch,  1000 iteration, loss:0.132\n",
      " num 74 epoch \n",
      "####### Training Loss #######\n",
      "[0.26313715]\n",
      "Checking accuracy on test set\n",
      "Got 653 / 1000 correct (65.30)\n",
      "76 epoch,   500 iteration, loss:0.150\n",
      "Checking accuracy on test set\n",
      "Got 688 / 1000 correct (68.80)\n",
      "76 epoch,  1000 iteration, loss:0.137\n",
      " num 75 epoch \n",
      "####### Training Loss #######\n",
      "[0.28118077]\n",
      "Checking accuracy on test set\n",
      "Got 678 / 1000 correct (67.80)\n",
      "77 epoch,   500 iteration, loss:0.126\n",
      "Checking accuracy on test set\n",
      "Got 666 / 1000 correct (66.60)\n",
      "77 epoch,  1000 iteration, loss:0.133\n",
      " num 76 epoch \n",
      "####### Training Loss #######\n",
      "[0.26137852]\n",
      "Checking accuracy on test set\n",
      "Got 678 / 1000 correct (67.80)\n",
      "78 epoch,   500 iteration, loss:0.114\n",
      "Checking accuracy on test set\n",
      "Got 672 / 1000 correct (67.20)\n",
      "78 epoch,  1000 iteration, loss:0.132\n",
      " num 77 epoch \n",
      "####### Training Loss #######\n",
      "[0.2414799]\n",
      "Checking accuracy on test set\n",
      "Got 654 / 1000 correct (65.40)\n",
      "79 epoch,   500 iteration, loss:0.131\n",
      "Checking accuracy on test set\n",
      "Got 666 / 1000 correct (66.60)\n",
      "79 epoch,  1000 iteration, loss:0.117\n",
      " num 78 epoch \n",
      "####### Training Loss #######\n",
      "[0.25172383]\n",
      "Checking accuracy on test set\n",
      "Got 652 / 1000 correct (65.20)\n",
      "80 epoch,   500 iteration, loss:0.135\n",
      "Checking accuracy on test set\n",
      "Got 680 / 1000 correct (68.00)\n",
      "80 epoch,  1000 iteration, loss:0.124\n",
      " num 79 epoch \n",
      "####### Training Loss #######\n",
      "[0.25553348]\n",
      "Checking accuracy on test set\n",
      "Got 678 / 1000 correct (67.80)\n",
      "81 epoch,   500 iteration, loss:0.128\n",
      "Checking accuracy on test set\n",
      "Got 686 / 1000 correct (68.60)\n",
      "81 epoch,  1000 iteration, loss:0.101\n",
      " num 80 epoch \n",
      "####### Training Loss #######\n",
      "[0.23617802]\n",
      "Checking accuracy on test set\n",
      "Got 688 / 1000 correct (68.80)\n",
      "82 epoch,   500 iteration, loss:0.116\n",
      "Checking accuracy on test set\n",
      "Got 673 / 1000 correct (67.30)\n",
      "82 epoch,  1000 iteration, loss:0.121\n",
      " num 81 epoch \n",
      "####### Training Loss #######\n",
      "[0.24030185]\n",
      "Checking accuracy on test set\n",
      "Got 691 / 1000 correct (69.10)\n",
      "83 epoch,   500 iteration, loss:0.111\n",
      "Checking accuracy on test set\n",
      "Got 692 / 1000 correct (69.20)\n",
      "83 epoch,  1000 iteration, loss:0.105\n",
      " num 82 epoch \n",
      "####### Training Loss #######\n",
      "[0.22284333]\n",
      "Checking accuracy on test set\n",
      "Got 671 / 1000 correct (67.10)\n",
      "84 epoch,   500 iteration, loss:0.112\n",
      "Checking accuracy on test set\n",
      "Got 669 / 1000 correct (66.90)\n",
      "84 epoch,  1000 iteration, loss:0.103\n",
      " num 83 epoch \n",
      "####### Training Loss #######\n",
      "[0.22053312]\n",
      "Checking accuracy on test set\n",
      "Got 683 / 1000 correct (68.30)\n",
      "85 epoch,   500 iteration, loss:0.117\n",
      "Checking accuracy on test set\n",
      "Got 690 / 1000 correct (69.00)\n",
      "85 epoch,  1000 iteration, loss:0.114\n",
      " num 84 epoch \n",
      "####### Training Loss #######\n",
      "[0.2383702]\n",
      "Checking accuracy on test set\n",
      "Got 679 / 1000 correct (67.90)\n",
      "86 epoch,   500 iteration, loss:0.098\n",
      "Checking accuracy on test set\n",
      "Got 695 / 1000 correct (69.50)\n",
      "86 epoch,  1000 iteration, loss:0.106\n",
      " num 85 epoch \n",
      "####### Training Loss #######\n",
      "[0.20216848]\n",
      "Checking accuracy on test set\n",
      "Got 660 / 1000 correct (66.00)\n",
      "87 epoch,   500 iteration, loss:0.108\n",
      "Checking accuracy on test set\n",
      "Got 692 / 1000 correct (69.20)\n",
      "87 epoch,  1000 iteration, loss:0.103\n",
      " num 86 epoch \n",
      "####### Training Loss #######\n",
      "[0.20972841]\n",
      "Checking accuracy on test set\n",
      "Got 681 / 1000 correct (68.10)\n",
      "88 epoch,   500 iteration, loss:0.092\n",
      "Checking accuracy on test set\n",
      "Got 661 / 1000 correct (66.10)\n",
      "88 epoch,  1000 iteration, loss:0.099\n",
      " num 87 epoch \n",
      "####### Training Loss #######\n",
      "[0.19935449]\n",
      "Checking accuracy on test set\n",
      "Got 669 / 1000 correct (66.90)\n",
      "89 epoch,   500 iteration, loss:0.109\n",
      "Checking accuracy on test set\n",
      "Got 701 / 1000 correct (70.10)\n",
      "89 epoch,  1000 iteration, loss:0.105\n",
      " num 88 epoch \n",
      "####### Training Loss #######\n",
      "[0.21088951]\n",
      "Checking accuracy on test set\n",
      "Got 658 / 1000 correct (65.80)\n",
      "90 epoch,   500 iteration, loss:0.098\n",
      "Checking accuracy on test set\n",
      "Got 689 / 1000 correct (68.90)\n",
      "90 epoch,  1000 iteration, loss:0.106\n",
      " num 89 epoch \n",
      "####### Training Loss #######\n",
      "[0.20646266]\n",
      "Checking accuracy on test set\n",
      "Got 668 / 1000 correct (66.80)\n",
      "91 epoch,   500 iteration, loss:0.109\n",
      "Checking accuracy on test set\n",
      "Got 666 / 1000 correct (66.60)\n",
      "91 epoch,  1000 iteration, loss:0.115\n",
      " num 90 epoch \n",
      "####### Training Loss #######\n",
      "[0.22381105]\n",
      "Checking accuracy on test set\n",
      "Got 673 / 1000 correct (67.30)\n",
      "92 epoch,   500 iteration, loss:0.097\n",
      "Checking accuracy on test set\n",
      "Got 668 / 1000 correct (66.80)\n",
      "92 epoch,  1000 iteration, loss:0.096\n",
      " num 91 epoch \n",
      "####### Training Loss #######\n",
      "[0.19330897]\n",
      "Checking accuracy on test set\n",
      "Got 670 / 1000 correct (67.00)\n",
      "93 epoch,   500 iteration, loss:0.089\n",
      "Checking accuracy on test set\n",
      "Got 669 / 1000 correct (66.90)\n",
      "93 epoch,  1000 iteration, loss:0.081\n",
      " num 92 epoch \n",
      "####### Training Loss #######\n",
      "[0.17735728]\n",
      "Checking accuracy on test set\n",
      "Got 672 / 1000 correct (67.20)\n",
      "94 epoch,   500 iteration, loss:0.098\n",
      "Checking accuracy on test set\n",
      "Got 682 / 1000 correct (68.20)\n",
      "94 epoch,  1000 iteration, loss:0.101\n",
      " num 93 epoch \n",
      "####### Training Loss #######\n",
      "[0.19875595]\n",
      "Checking accuracy on test set\n",
      "Got 688 / 1000 correct (68.80)\n",
      "95 epoch,   500 iteration, loss:0.097\n",
      "Checking accuracy on test set\n",
      "Got 710 / 1000 correct (71.00)\n",
      "95 epoch,  1000 iteration, loss:0.090\n",
      " num 94 epoch \n",
      "####### Training Loss #######\n",
      "[0.18385772]\n",
      "Checking accuracy on test set\n",
      "Got 665 / 1000 correct (66.50)\n",
      "96 epoch,   500 iteration, loss:0.084\n",
      "Checking accuracy on test set\n",
      "Got 703 / 1000 correct (70.30)\n",
      "96 epoch,  1000 iteration, loss:0.092\n",
      " num 95 epoch \n",
      "####### Training Loss #######\n",
      "[0.17847256]\n",
      "Checking accuracy on test set\n",
      "Got 682 / 1000 correct (68.20)\n",
      "97 epoch,   500 iteration, loss:0.099\n",
      "Checking accuracy on test set\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 690 / 1000 correct (69.00)\n",
      "97 epoch,  1000 iteration, loss:0.096\n",
      " num 96 epoch \n",
      "####### Training Loss #######\n",
      "[0.18861012]\n",
      "Checking accuracy on test set\n",
      "Got 687 / 1000 correct (68.70)\n",
      "98 epoch,   500 iteration, loss:0.077\n",
      "Checking accuracy on test set\n",
      "Got 691 / 1000 correct (69.10)\n",
      "98 epoch,  1000 iteration, loss:0.095\n",
      " num 97 epoch \n",
      "####### Training Loss #######\n",
      "[0.17498713]\n",
      "Checking accuracy on test set\n",
      "Got 681 / 1000 correct (68.10)\n",
      "99 epoch,   500 iteration, loss:0.088\n",
      "Checking accuracy on test set\n",
      "Got 707 / 1000 correct (70.70)\n",
      "99 epoch,  1000 iteration, loss:0.088\n",
      " num 98 epoch \n",
      "####### Training Loss #######\n",
      "[0.1785242]\n",
      "Checking accuracy on test set\n",
      "Got 686 / 1000 correct (68.60)\n",
      "100 epoch,   500 iteration, loss:0.085\n",
      "Checking accuracy on test set\n",
      "Got 682 / 1000 correct (68.20)\n",
      "100 epoch,  1000 iteration, loss:0.092\n",
      " num 99 epoch \n",
      "####### Training Loss #######\n",
      "[0.17084036]\n",
      "Checking accuracy on test set\n",
      "Got 675 / 1000 correct (67.50)\n",
      "101 epoch,   500 iteration, loss:0.079\n",
      "Checking accuracy on test set\n",
      "Got 695 / 1000 correct (69.50)\n",
      "101 epoch,  1000 iteration, loss:0.084\n",
      " num 100 epoch \n",
      "####### Training Loss #######\n",
      "[0.16614054]\n",
      "Checking accuracy on test set\n",
      "Got 676 / 1000 correct (67.60)\n",
      "102 epoch,   500 iteration, loss:0.083\n",
      "Checking accuracy on test set\n",
      "Got 671 / 1000 correct (67.10)\n",
      "102 epoch,  1000 iteration, loss:0.095\n",
      " num 101 epoch \n",
      "####### Training Loss #######\n",
      "[0.17275922]\n",
      "Checking accuracy on test set\n",
      "Got 647 / 1000 correct (64.70)\n",
      "103 epoch,   500 iteration, loss:0.079\n",
      "Checking accuracy on test set\n",
      "Got 689 / 1000 correct (68.90)\n",
      "103 epoch,  1000 iteration, loss:0.077\n",
      " num 102 epoch \n",
      "####### Training Loss #######\n",
      "[0.15985325]\n",
      "Checking accuracy on test set\n",
      "Got 685 / 1000 correct (68.50)\n",
      "104 epoch,   500 iteration, loss:0.079\n",
      "Checking accuracy on test set\n",
      "Got 673 / 1000 correct (67.30)\n",
      "104 epoch,  1000 iteration, loss:0.078\n",
      " num 103 epoch \n",
      "####### Training Loss #######\n",
      "[0.15663505]\n",
      "Checking accuracy on test set\n",
      "Got 676 / 1000 correct (67.60)\n",
      "105 epoch,   500 iteration, loss:0.081\n",
      "Checking accuracy on test set\n",
      "Got 703 / 1000 correct (70.30)\n",
      "105 epoch,  1000 iteration, loss:0.086\n",
      " num 104 epoch \n",
      "####### Training Loss #######\n",
      "[0.16333748]\n",
      "Checking accuracy on test set\n",
      "Got 697 / 1000 correct (69.70)\n",
      "106 epoch,   500 iteration, loss:0.089\n",
      "Checking accuracy on test set\n",
      "Got 700 / 1000 correct (70.00)\n",
      "106 epoch,  1000 iteration, loss:0.076\n",
      " num 105 epoch \n",
      "####### Training Loss #######\n",
      "[0.17150668]\n",
      "Checking accuracy on test set\n",
      "Got 687 / 1000 correct (68.70)\n",
      "107 epoch,   500 iteration, loss:0.086\n",
      "Checking accuracy on test set\n",
      "Got 690 / 1000 correct (69.00)\n",
      "107 epoch,  1000 iteration, loss:0.097\n",
      " num 106 epoch \n",
      "####### Training Loss #######\n",
      "[0.17923752]\n",
      "Checking accuracy on test set\n",
      "Got 661 / 1000 correct (66.10)\n",
      "108 epoch,   500 iteration, loss:0.082\n",
      "Checking accuracy on test set\n",
      "Got 705 / 1000 correct (70.50)\n",
      "108 epoch,  1000 iteration, loss:0.081\n",
      " num 107 epoch \n",
      "####### Training Loss #######\n",
      "[0.16667913]\n",
      "Checking accuracy on test set\n",
      "Got 692 / 1000 correct (69.20)\n",
      "109 epoch,   500 iteration, loss:0.061\n",
      "Checking accuracy on test set\n",
      "Got 699 / 1000 correct (69.90)\n",
      "109 epoch,  1000 iteration, loss:0.072\n",
      " num 108 epoch \n",
      "####### Training Loss #######\n",
      "[0.14394939]\n",
      "Checking accuracy on test set\n",
      "Got 679 / 1000 correct (67.90)\n",
      "110 epoch,   500 iteration, loss:0.082\n",
      "Checking accuracy on test set\n",
      "Got 685 / 1000 correct (68.50)\n",
      "110 epoch,  1000 iteration, loss:0.091\n",
      " num 109 epoch \n",
      "####### Training Loss #######\n",
      "[0.17101306]\n",
      "Checking accuracy on test set\n",
      "Got 687 / 1000 correct (68.70)\n",
      "111 epoch,   500 iteration, loss:0.067\n",
      "Checking accuracy on test set\n",
      "Got 697 / 1000 correct (69.70)\n",
      "111 epoch,  1000 iteration, loss:0.072\n",
      " num 110 epoch \n",
      "####### Training Loss #######\n",
      "[0.15075315]\n",
      "Checking accuracy on test set\n",
      "Got 660 / 1000 correct (66.00)\n",
      "112 epoch,   500 iteration, loss:0.080\n",
      "Checking accuracy on test set\n",
      "Got 670 / 1000 correct (67.00)\n",
      "112 epoch,  1000 iteration, loss:0.082\n",
      " num 111 epoch \n",
      "####### Training Loss #######\n",
      "[0.16368696]\n",
      "Checking accuracy on test set\n",
      "Got 681 / 1000 correct (68.10)\n",
      "113 epoch,   500 iteration, loss:0.071\n",
      "Checking accuracy on test set\n",
      "Got 670 / 1000 correct (67.00)\n",
      "113 epoch,  1000 iteration, loss:0.065\n",
      " num 112 epoch \n",
      "####### Training Loss #######\n",
      "[0.14181162]\n",
      "Checking accuracy on test set\n",
      "Got 652 / 1000 correct (65.20)\n",
      "114 epoch,   500 iteration, loss:0.072\n",
      "Checking accuracy on test set\n",
      "Got 678 / 1000 correct (67.80)\n",
      "114 epoch,  1000 iteration, loss:0.074\n",
      " num 113 epoch \n",
      "####### Training Loss #######\n",
      "[0.14768385]\n",
      "Checking accuracy on test set\n",
      "Got 675 / 1000 correct (67.50)\n",
      "115 epoch,   500 iteration, loss:0.075\n",
      "Checking accuracy on test set\n",
      "Got 695 / 1000 correct (69.50)\n",
      "115 epoch,  1000 iteration, loss:0.079\n",
      " num 114 epoch \n",
      "####### Training Loss #######\n",
      "[0.15998762]\n",
      "Checking accuracy on test set\n",
      "Got 698 / 1000 correct (69.80)\n",
      "116 epoch,   500 iteration, loss:0.078\n",
      "Checking accuracy on test set\n",
      "Got 702 / 1000 correct (70.20)\n",
      "116 epoch,  1000 iteration, loss:0.064\n",
      " num 115 epoch \n",
      "####### Training Loss #######\n",
      "[0.14109867]\n",
      "Checking accuracy on test set\n",
      "Got 654 / 1000 correct (65.40)\n",
      "117 epoch,   500 iteration, loss:0.072\n",
      "Checking accuracy on test set\n",
      "Got 711 / 1000 correct (71.10)\n",
      "117 epoch,  1000 iteration, loss:0.090\n",
      " num 116 epoch \n",
      "####### Training Loss #######\n",
      "[0.15593679]\n",
      "Checking accuracy on test set\n",
      "Got 680 / 1000 correct (68.00)\n",
      "118 epoch,   500 iteration, loss:0.075\n",
      "Checking accuracy on test set\n",
      "Got 700 / 1000 correct (70.00)\n",
      "118 epoch,  1000 iteration, loss:0.072\n",
      " num 117 epoch \n",
      "####### Training Loss #######\n",
      "[0.14326278]\n",
      "Checking accuracy on test set\n",
      "Got 669 / 1000 correct (66.90)\n",
      "119 epoch,   500 iteration, loss:0.076\n",
      "Checking accuracy on test set\n",
      "Got 694 / 1000 correct (69.40)\n",
      "119 epoch,  1000 iteration, loss:0.093\n",
      " num 118 epoch \n",
      "####### Training Loss #######\n",
      "[0.16563257]\n",
      "Checking accuracy on test set\n",
      "Got 680 / 1000 correct (68.00)\n",
      "120 epoch,   500 iteration, loss:0.068\n",
      "Checking accuracy on test set\n",
      "Got 704 / 1000 correct (70.40)\n",
      "120 epoch,  1000 iteration, loss:0.073\n",
      " num 119 epoch \n",
      "####### Training Loss #######\n",
      "[0.1373654]\n",
      "Checking accuracy on test set\n",
      "Got 691 / 1000 correct (69.10)\n",
      "121 epoch,   500 iteration, loss:0.064\n",
      "Checking accuracy on test set\n",
      "Got 703 / 1000 correct (70.30)\n",
      "121 epoch,  1000 iteration, loss:0.067\n",
      " num 120 epoch \n",
      "####### Training Loss #######\n",
      "[0.1368349]\n",
      "Checking accuracy on test set\n",
      "Got 673 / 1000 correct (67.30)\n",
      "122 epoch,   500 iteration, loss:0.064\n",
      "Checking accuracy on test set\n",
      "Got 696 / 1000 correct (69.60)\n",
      "122 epoch,  1000 iteration, loss:0.078\n",
      " num 121 epoch \n",
      "####### Training Loss #######\n",
      "[0.14276535]\n",
      "Checking accuracy on test set\n",
      "Got 690 / 1000 correct (69.00)\n",
      "123 epoch,   500 iteration, loss:0.057\n",
      "Checking accuracy on test set\n",
      "Got 695 / 1000 correct (69.50)\n",
      "123 epoch,  1000 iteration, loss:0.071\n",
      " num 122 epoch \n",
      "####### Training Loss #######\n",
      "[0.12690651]\n",
      "Checking accuracy on test set\n",
      "Got 677 / 1000 correct (67.70)\n",
      "124 epoch,   500 iteration, loss:0.074\n",
      "Checking accuracy on test set\n",
      "Got 672 / 1000 correct (67.20)\n",
      "124 epoch,  1000 iteration, loss:0.070\n",
      " num 123 epoch \n",
      "####### Training Loss #######\n",
      "[0.13576714]\n",
      "Checking accuracy on test set\n",
      "Got 674 / 1000 correct (67.40)\n",
      "125 epoch,   500 iteration, loss:0.083\n",
      "Checking accuracy on test set\n",
      "Got 690 / 1000 correct (69.00)\n",
      "125 epoch,  1000 iteration, loss:0.077\n",
      " num 124 epoch \n",
      "####### Training Loss #######\n",
      "[0.16066943]\n",
      "Checking accuracy on test set\n",
      "Got 691 / 1000 correct (69.10)\n",
      "126 epoch,   500 iteration, loss:0.069\n",
      "Checking accuracy on test set\n",
      "Got 697 / 1000 correct (69.70)\n",
      "126 epoch,  1000 iteration, loss:0.060\n",
      " num 125 epoch \n",
      "####### Training Loss #######\n",
      "[0.13444945]\n",
      "Checking accuracy on test set\n",
      "Got 670 / 1000 correct (67.00)\n",
      "127 epoch,   500 iteration, loss:0.071\n",
      "Checking accuracy on test set\n",
      "Got 683 / 1000 correct (68.30)\n",
      "127 epoch,  1000 iteration, loss:0.062\n",
      " num 126 epoch \n",
      "####### Training Loss #######\n",
      "[0.1286372]\n",
      "Checking accuracy on test set\n",
      "Got 664 / 1000 correct (66.40)\n",
      "128 epoch,   500 iteration, loss:0.074\n",
      "Checking accuracy on test set\n",
      "Got 710 / 1000 correct (71.00)\n",
      "128 epoch,  1000 iteration, loss:0.078\n",
      " num 127 epoch \n",
      "####### Training Loss #######\n",
      "[0.14404903]\n",
      "Checking accuracy on test set\n",
      "Got 687 / 1000 correct (68.70)\n",
      "129 epoch,   500 iteration, loss:0.072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking accuracy on test set\n",
      "Got 707 / 1000 correct (70.70)\n",
      "129 epoch,  1000 iteration, loss:0.064\n",
      " num 128 epoch \n",
      "####### Training Loss #######\n",
      "[0.14221816]\n",
      "Checking accuracy on test set\n",
      "Got 689 / 1000 correct (68.90)\n",
      "130 epoch,   500 iteration, loss:0.071\n",
      "Checking accuracy on test set\n",
      "Got 705 / 1000 correct (70.50)\n",
      "130 epoch,  1000 iteration, loss:0.063\n",
      " num 129 epoch \n",
      "####### Training Loss #######\n",
      "[0.13489307]\n",
      "Checking accuracy on test set\n",
      "Got 672 / 1000 correct (67.20)\n",
      "131 epoch,   500 iteration, loss:0.075\n",
      "Checking accuracy on test set\n",
      "Got 705 / 1000 correct (70.50)\n",
      "131 epoch,  1000 iteration, loss:0.064\n",
      " num 130 epoch \n",
      "####### Training Loss #######\n",
      "[0.14153645]\n",
      "Checking accuracy on test set\n",
      "Got 703 / 1000 correct (70.30)\n",
      "132 epoch,   500 iteration, loss:0.063\n",
      "Checking accuracy on test set\n",
      "Got 722 / 1000 correct (72.20)\n",
      "132 epoch,  1000 iteration, loss:0.078\n",
      " num 131 epoch \n",
      "####### Training Loss #######\n",
      "[0.13129379]\n",
      "Checking accuracy on test set\n",
      "Got 688 / 1000 correct (68.80)\n",
      "133 epoch,   500 iteration, loss:0.080\n",
      "Checking accuracy on test set\n",
      "Got 697 / 1000 correct (69.70)\n",
      "133 epoch,  1000 iteration, loss:0.063\n",
      " num 132 epoch \n",
      "####### Training Loss #######\n",
      "[0.14141599]\n",
      "Checking accuracy on test set\n",
      "Got 685 / 1000 correct (68.50)\n",
      "134 epoch,   500 iteration, loss:0.064\n",
      "Checking accuracy on test set\n",
      "Got 720 / 1000 correct (72.00)\n",
      "134 epoch,  1000 iteration, loss:0.062\n",
      " num 133 epoch \n",
      "####### Training Loss #######\n",
      "[0.12833514]\n",
      "Checking accuracy on test set\n",
      "Got 666 / 1000 correct (66.60)\n",
      "135 epoch,   500 iteration, loss:0.068\n",
      "Checking accuracy on test set\n",
      "Got 692 / 1000 correct (69.20)\n",
      "135 epoch,  1000 iteration, loss:0.076\n",
      " num 134 epoch \n",
      "####### Training Loss #######\n",
      "[0.14571608]\n",
      "Checking accuracy on test set\n",
      "Got 679 / 1000 correct (67.90)\n",
      "136 epoch,   500 iteration, loss:0.061\n",
      "Checking accuracy on test set\n",
      "Got 704 / 1000 correct (70.40)\n",
      "136 epoch,  1000 iteration, loss:0.064\n",
      " num 135 epoch \n",
      "####### Training Loss #######\n",
      "[0.12655971]\n",
      "Checking accuracy on test set\n",
      "Got 679 / 1000 correct (67.90)\n",
      "137 epoch,   500 iteration, loss:0.061\n",
      "Checking accuracy on test set\n",
      "Got 706 / 1000 correct (70.60)\n",
      "137 epoch,  1000 iteration, loss:0.073\n",
      " num 136 epoch \n",
      "####### Training Loss #######\n",
      "[0.13376942]\n",
      "Checking accuracy on test set\n",
      "Got 683 / 1000 correct (68.30)\n",
      "138 epoch,   500 iteration, loss:0.071\n",
      "Checking accuracy on test set\n",
      "Got 700 / 1000 correct (70.00)\n",
      "138 epoch,  1000 iteration, loss:0.075\n",
      " num 137 epoch \n",
      "####### Training Loss #######\n",
      "[0.1439264]\n",
      "Checking accuracy on test set\n",
      "Got 682 / 1000 correct (68.20)\n",
      "139 epoch,   500 iteration, loss:0.070\n",
      "Checking accuracy on test set\n",
      "Got 672 / 1000 correct (67.20)\n",
      "139 epoch,  1000 iteration, loss:0.062\n",
      " num 138 epoch \n",
      "####### Training Loss #######\n",
      "[0.13556855]\n",
      "Checking accuracy on test set\n",
      "Got 704 / 1000 correct (70.40)\n",
      "140 epoch,   500 iteration, loss:0.055\n",
      "Checking accuracy on test set\n",
      "Got 689 / 1000 correct (68.90)\n",
      "140 epoch,  1000 iteration, loss:0.056\n",
      " num 139 epoch \n",
      "####### Training Loss #######\n",
      "[0.11463573]\n",
      "Checking accuracy on test set\n",
      "Got 698 / 1000 correct (69.80)\n",
      "141 epoch,   500 iteration, loss:0.052\n",
      "Checking accuracy on test set\n",
      "Got 695 / 1000 correct (69.50)\n",
      "141 epoch,  1000 iteration, loss:0.062\n",
      " num 140 epoch \n",
      "####### Training Loss #######\n",
      "[0.11333235]\n",
      "Checking accuracy on test set\n",
      "Got 680 / 1000 correct (68.00)\n",
      "142 epoch,   500 iteration, loss:0.064\n",
      "Checking accuracy on test set\n",
      "Got 687 / 1000 correct (68.70)\n",
      "142 epoch,  1000 iteration, loss:0.061\n",
      " num 141 epoch \n",
      "####### Training Loss #######\n",
      "[0.12398191]\n",
      "Checking accuracy on test set\n",
      "Got 683 / 1000 correct (68.30)\n",
      "143 epoch,   500 iteration, loss:0.065\n",
      "Checking accuracy on test set\n",
      "Got 703 / 1000 correct (70.30)\n",
      "143 epoch,  1000 iteration, loss:0.061\n",
      " num 142 epoch \n",
      "####### Training Loss #######\n",
      "[0.1337926]\n",
      "Checking accuracy on test set\n",
      "Got 675 / 1000 correct (67.50)\n",
      "144 epoch,   500 iteration, loss:0.062\n",
      "Checking accuracy on test set\n",
      "Got 706 / 1000 correct (70.60)\n",
      "144 epoch,  1000 iteration, loss:0.061\n",
      " num 143 epoch \n",
      "####### Training Loss #######\n",
      "[0.12021155]\n",
      "Checking accuracy on test set\n",
      "Got 687 / 1000 correct (68.70)\n",
      "145 epoch,   500 iteration, loss:0.051\n",
      "Checking accuracy on test set\n",
      "Got 698 / 1000 correct (69.80)\n",
      "145 epoch,  1000 iteration, loss:0.073\n",
      " num 144 epoch \n",
      "####### Training Loss #######\n",
      "[0.13204849]\n",
      "Checking accuracy on test set\n",
      "Got 676 / 1000 correct (67.60)\n",
      "146 epoch,   500 iteration, loss:0.054\n",
      "Checking accuracy on test set\n",
      "Got 684 / 1000 correct (68.40)\n",
      "146 epoch,  1000 iteration, loss:0.062\n",
      " num 145 epoch \n",
      "####### Training Loss #######\n",
      "[0.11794104]\n",
      "Checking accuracy on test set\n",
      "Got 689 / 1000 correct (68.90)\n",
      "147 epoch,   500 iteration, loss:0.057\n",
      "Checking accuracy on test set\n",
      "Got 714 / 1000 correct (71.40)\n",
      "147 epoch,  1000 iteration, loss:0.068\n",
      " num 146 epoch \n",
      "####### Training Loss #######\n",
      "[0.12278301]\n",
      "Checking accuracy on test set\n",
      "Got 675 / 1000 correct (67.50)\n",
      "148 epoch,   500 iteration, loss:0.064\n",
      "Checking accuracy on test set\n",
      "Got 698 / 1000 correct (69.80)\n",
      "148 epoch,  1000 iteration, loss:0.057\n",
      " num 147 epoch \n",
      "####### Training Loss #######\n",
      "[0.12044534]\n",
      "Checking accuracy on test set\n",
      "Got 693 / 1000 correct (69.30)\n",
      "149 epoch,   500 iteration, loss:0.069\n",
      "Checking accuracy on test set\n",
      "Got 715 / 1000 correct (71.50)\n",
      "149 epoch,  1000 iteration, loss:0.051\n",
      " num 148 epoch \n",
      "####### Training Loss #######\n",
      "[0.12491143]\n",
      "Checking accuracy on test set\n",
      "Got 685 / 1000 correct (68.50)\n",
      "150 epoch,   500 iteration, loss:0.059\n",
      "Checking accuracy on test set\n",
      "Got 702 / 1000 correct (70.20)\n",
      "150 epoch,  1000 iteration, loss:0.053\n",
      " num 149 epoch \n",
      "####### Training Loss #######\n",
      "[0.11607]\n",
      "Checking accuracy on test set\n",
      "Got 682 / 1000 correct (68.20)\n",
      "151 epoch,   500 iteration, loss:0.055\n",
      "Checking accuracy on test set\n",
      "Got 688 / 1000 correct (68.80)\n",
      "151 epoch,  1000 iteration, loss:0.063\n",
      " num 150 epoch \n",
      "####### Training Loss #######\n",
      "[0.1169546]\n",
      "Checking accuracy on test set\n",
      "Got 695 / 1000 correct (69.50)\n",
      "152 epoch,   500 iteration, loss:0.046\n",
      "Checking accuracy on test set\n",
      "Got 678 / 1000 correct (67.80)\n",
      "152 epoch,  1000 iteration, loss:0.057\n",
      " num 151 epoch \n",
      "####### Training Loss #######\n",
      "[0.1089123]\n",
      "Checking accuracy on test set\n",
      "Got 662 / 1000 correct (66.20)\n",
      "153 epoch,   500 iteration, loss:0.063\n",
      "Checking accuracy on test set\n",
      "Got 713 / 1000 correct (71.30)\n",
      "153 epoch,  1000 iteration, loss:0.048\n",
      " num 152 epoch \n",
      "####### Training Loss #######\n",
      "[0.11277581]\n",
      "Checking accuracy on test set\n",
      "Got 678 / 1000 correct (67.80)\n",
      "154 epoch,   500 iteration, loss:0.065\n",
      "Checking accuracy on test set\n",
      "Got 682 / 1000 correct (68.20)\n",
      "154 epoch,  1000 iteration, loss:0.062\n",
      " num 153 epoch \n",
      "####### Training Loss #######\n",
      "[0.12476315]\n",
      "Checking accuracy on test set\n",
      "Got 694 / 1000 correct (69.40)\n",
      "155 epoch,   500 iteration, loss:0.052\n",
      "Checking accuracy on test set\n",
      "Got 693 / 1000 correct (69.30)\n",
      "155 epoch,  1000 iteration, loss:0.057\n",
      " num 154 epoch \n",
      "####### Training Loss #######\n",
      "[0.10937414]\n",
      "Checking accuracy on test set\n",
      "Got 683 / 1000 correct (68.30)\n",
      "156 epoch,   500 iteration, loss:0.065\n",
      "Checking accuracy on test set\n",
      "Got 690 / 1000 correct (69.00)\n",
      "156 epoch,  1000 iteration, loss:0.060\n",
      " num 155 epoch \n",
      "####### Training Loss #######\n",
      "[0.13565485]\n",
      "Checking accuracy on test set\n",
      "Got 685 / 1000 correct (68.50)\n",
      "157 epoch,   500 iteration, loss:0.055\n",
      "Checking accuracy on test set\n",
      "Got 698 / 1000 correct (69.80)\n",
      "157 epoch,  1000 iteration, loss:0.060\n",
      " num 156 epoch \n",
      "####### Training Loss #######\n",
      "[0.1157209]\n",
      "Checking accuracy on test set\n",
      "Got 677 / 1000 correct (67.70)\n",
      "158 epoch,   500 iteration, loss:0.055\n",
      "Checking accuracy on test set\n",
      "Got 691 / 1000 correct (69.10)\n",
      "158 epoch,  1000 iteration, loss:0.064\n",
      " num 157 epoch \n",
      "####### Training Loss #######\n",
      "[0.12282031]\n",
      "Checking accuracy on test set\n",
      "Got 693 / 1000 correct (69.30)\n",
      "159 epoch,   500 iteration, loss:0.058\n",
      "Checking accuracy on test set\n",
      "Got 699 / 1000 correct (69.90)\n",
      "159 epoch,  1000 iteration, loss:0.052\n",
      " num 158 epoch \n",
      "####### Training Loss #######\n",
      "[0.10703762]\n",
      "Checking accuracy on test set\n",
      "Got 663 / 1000 correct (66.30)\n",
      "160 epoch,   500 iteration, loss:0.058\n",
      "Checking accuracy on test set\n",
      "Got 703 / 1000 correct (70.30)\n",
      "160 epoch,  1000 iteration, loss:0.062\n",
      " num 159 epoch \n",
      "####### Training Loss #######\n",
      "[0.12115988]\n",
      "Checking accuracy on test set\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 689 / 1000 correct (68.90)\n",
      "161 epoch,   500 iteration, loss:0.059\n",
      "Checking accuracy on test set\n",
      "Got 683 / 1000 correct (68.30)\n",
      "161 epoch,  1000 iteration, loss:0.046\n",
      " num 160 epoch \n",
      "####### Training Loss #######\n",
      "[0.10506526]\n",
      "Checking accuracy on test set\n",
      "Got 672 / 1000 correct (67.20)\n",
      "162 epoch,   500 iteration, loss:0.060\n",
      "Checking accuracy on test set\n",
      "Got 696 / 1000 correct (69.60)\n",
      "162 epoch,  1000 iteration, loss:0.060\n",
      " num 161 epoch \n",
      "####### Training Loss #######\n",
      "[0.11258161]\n",
      "Checking accuracy on test set\n",
      "Got 675 / 1000 correct (67.50)\n",
      "163 epoch,   500 iteration, loss:0.059\n",
      "Checking accuracy on test set\n",
      "Got 680 / 1000 correct (68.00)\n",
      "163 epoch,  1000 iteration, loss:0.058\n",
      " num 162 epoch \n",
      "####### Training Loss #######\n",
      "[0.12478443]\n",
      "Checking accuracy on test set\n",
      "Got 680 / 1000 correct (68.00)\n",
      "164 epoch,   500 iteration, loss:0.052\n",
      "Checking accuracy on test set\n",
      "Got 681 / 1000 correct (68.10)\n",
      "164 epoch,  1000 iteration, loss:0.060\n",
      " num 163 epoch \n",
      "####### Training Loss #######\n",
      "[0.10994502]\n",
      "Checking accuracy on test set\n",
      "Got 678 / 1000 correct (67.80)\n",
      "165 epoch,   500 iteration, loss:0.050\n",
      "Checking accuracy on test set\n",
      "Got 704 / 1000 correct (70.40)\n",
      "165 epoch,  1000 iteration, loss:0.057\n",
      " num 164 epoch \n",
      "####### Training Loss #######\n",
      "[0.10797223]\n",
      "Checking accuracy on test set\n",
      "Got 677 / 1000 correct (67.70)\n",
      "166 epoch,   500 iteration, loss:0.047\n",
      "Checking accuracy on test set\n",
      "Got 701 / 1000 correct (70.10)\n",
      "166 epoch,  1000 iteration, loss:0.058\n",
      " num 165 epoch \n",
      "####### Training Loss #######\n",
      "[0.10775065]\n",
      "Checking accuracy on test set\n",
      "Got 678 / 1000 correct (67.80)\n",
      "167 epoch,   500 iteration, loss:0.058\n",
      "Checking accuracy on test set\n",
      "Got 685 / 1000 correct (68.50)\n",
      "167 epoch,  1000 iteration, loss:0.048\n",
      " num 166 epoch \n",
      "####### Training Loss #######\n",
      "[0.10704013]\n",
      "Checking accuracy on test set\n",
      "Got 666 / 1000 correct (66.60)\n",
      "168 epoch,   500 iteration, loss:0.065\n",
      "Checking accuracy on test set\n",
      "Got 685 / 1000 correct (68.50)\n",
      "168 epoch,  1000 iteration, loss:0.052\n",
      " num 167 epoch \n",
      "####### Training Loss #######\n",
      "[0.11530219]\n",
      "Checking accuracy on test set\n",
      "Got 676 / 1000 correct (67.60)\n",
      "169 epoch,   500 iteration, loss:0.049\n",
      "Checking accuracy on test set\n",
      "Got 662 / 1000 correct (66.20)\n",
      "169 epoch,  1000 iteration, loss:0.056\n",
      " num 168 epoch \n",
      "####### Training Loss #######\n",
      "[0.10670826]\n",
      "Checking accuracy on test set\n",
      "Got 675 / 1000 correct (67.50)\n",
      "170 epoch,   500 iteration, loss:0.053\n",
      "Checking accuracy on test set\n",
      "Got 687 / 1000 correct (68.70)\n",
      "170 epoch,  1000 iteration, loss:0.060\n",
      " num 169 epoch \n",
      "####### Training Loss #######\n",
      "[0.11561777]\n",
      "Checking accuracy on test set\n",
      "Got 675 / 1000 correct (67.50)\n",
      "171 epoch,   500 iteration, loss:0.054\n",
      "Checking accuracy on test set\n",
      "Got 691 / 1000 correct (69.10)\n",
      "171 epoch,  1000 iteration, loss:0.053\n",
      " num 170 epoch \n",
      "####### Training Loss #######\n",
      "[0.10692767]\n",
      "Checking accuracy on test set\n",
      "Got 686 / 1000 correct (68.60)\n",
      "172 epoch,   500 iteration, loss:0.062\n",
      "Checking accuracy on test set\n",
      "Got 687 / 1000 correct (68.70)\n",
      "172 epoch,  1000 iteration, loss:0.068\n",
      " num 171 epoch \n",
      "####### Training Loss #######\n",
      "[0.13187495]\n",
      "Checking accuracy on test set\n",
      "Got 680 / 1000 correct (68.00)\n",
      "173 epoch,   500 iteration, loss:0.050\n",
      "Checking accuracy on test set\n",
      "Got 694 / 1000 correct (69.40)\n",
      "173 epoch,  1000 iteration, loss:0.046\n",
      " num 172 epoch \n",
      "####### Training Loss #######\n",
      "[0.09560529]\n",
      "Checking accuracy on test set\n",
      "Got 691 / 1000 correct (69.10)\n",
      "174 epoch,   500 iteration, loss:0.058\n",
      "Checking accuracy on test set\n",
      "Got 683 / 1000 correct (68.30)\n",
      "174 epoch,  1000 iteration, loss:0.060\n",
      " num 173 epoch \n",
      "####### Training Loss #######\n",
      "[0.12203359]\n",
      "Checking accuracy on test set\n",
      "Got 669 / 1000 correct (66.90)\n",
      "175 epoch,   500 iteration, loss:0.067\n",
      "Checking accuracy on test set\n",
      "Got 687 / 1000 correct (68.70)\n",
      "175 epoch,  1000 iteration, loss:0.061\n",
      " num 174 epoch \n",
      "####### Training Loss #######\n",
      "[0.12291989]\n",
      "Checking accuracy on test set\n",
      "Got 703 / 1000 correct (70.30)\n",
      "176 epoch,   500 iteration, loss:0.064\n",
      "Checking accuracy on test set\n",
      "Got 682 / 1000 correct (68.20)\n",
      "176 epoch,  1000 iteration, loss:0.057\n",
      " num 175 epoch \n",
      "####### Training Loss #######\n",
      "[0.1181176]\n",
      "Checking accuracy on test set\n",
      "Got 661 / 1000 correct (66.10)\n",
      "177 epoch,   500 iteration, loss:0.061\n",
      "Checking accuracy on test set\n",
      "Got 695 / 1000 correct (69.50)\n",
      "177 epoch,  1000 iteration, loss:0.055\n",
      " num 176 epoch \n",
      "####### Training Loss #######\n",
      "[0.10616569]\n",
      "Checking accuracy on test set\n",
      "Got 678 / 1000 correct (67.80)\n",
      "178 epoch,   500 iteration, loss:0.045\n",
      "Checking accuracy on test set\n",
      "Got 680 / 1000 correct (68.00)\n",
      "178 epoch,  1000 iteration, loss:0.056\n",
      " num 177 epoch \n",
      "####### Training Loss #######\n",
      "[0.10368875]\n",
      "Checking accuracy on test set\n",
      "Got 687 / 1000 correct (68.70)\n",
      "179 epoch,   500 iteration, loss:0.054\n",
      "Checking accuracy on test set\n",
      "Got 711 / 1000 correct (71.10)\n",
      "179 epoch,  1000 iteration, loss:0.052\n",
      " num 178 epoch \n",
      "####### Training Loss #######\n",
      "[0.1077482]\n",
      "Checking accuracy on test set\n",
      "Got 692 / 1000 correct (69.20)\n",
      "180 epoch,   500 iteration, loss:0.060\n",
      "Checking accuracy on test set\n",
      "Got 697 / 1000 correct (69.70)\n",
      "180 epoch,  1000 iteration, loss:0.048\n",
      " num 179 epoch \n",
      "####### Training Loss #######\n",
      "[0.102477]\n",
      "Checking accuracy on test set\n",
      "Got 699 / 1000 correct (69.90)\n",
      "181 epoch,   500 iteration, loss:0.044\n",
      "Checking accuracy on test set\n",
      "Got 705 / 1000 correct (70.50)\n",
      "181 epoch,  1000 iteration, loss:0.060\n",
      " num 180 epoch \n",
      "####### Training Loss #######\n",
      "[0.10587291]\n",
      "Checking accuracy on test set\n",
      "Got 694 / 1000 correct (69.40)\n",
      "182 epoch,   500 iteration, loss:0.041\n",
      "Checking accuracy on test set\n",
      "Got 692 / 1000 correct (69.20)\n",
      "182 epoch,  1000 iteration, loss:0.059\n",
      " num 181 epoch \n",
      "####### Training Loss #######\n",
      "[0.10566161]\n",
      "Checking accuracy on test set\n",
      "Got 657 / 1000 correct (65.70)\n",
      "183 epoch,   500 iteration, loss:0.049\n",
      "Checking accuracy on test set\n",
      "Got 696 / 1000 correct (69.60)\n",
      "183 epoch,  1000 iteration, loss:0.066\n",
      " num 182 epoch \n",
      "####### Training Loss #######\n",
      "[0.11918048]\n",
      "Checking accuracy on test set\n",
      "Got 665 / 1000 correct (66.50)\n",
      "184 epoch,   500 iteration, loss:0.052\n",
      "Checking accuracy on test set\n",
      "Got 693 / 1000 correct (69.30)\n",
      "184 epoch,  1000 iteration, loss:0.062\n",
      " num 183 epoch \n",
      "####### Training Loss #######\n",
      "[0.10942257]\n",
      "Checking accuracy on test set\n",
      "Got 697 / 1000 correct (69.70)\n",
      "185 epoch,   500 iteration, loss:0.048\n",
      "Checking accuracy on test set\n",
      "Got 691 / 1000 correct (69.10)\n",
      "185 epoch,  1000 iteration, loss:0.056\n",
      " num 184 epoch \n",
      "####### Training Loss #######\n",
      "[0.10785091]\n",
      "Checking accuracy on test set\n",
      "Got 675 / 1000 correct (67.50)\n",
      "186 epoch,   500 iteration, loss:0.049\n",
      "Checking accuracy on test set\n",
      "Got 686 / 1000 correct (68.60)\n",
      "186 epoch,  1000 iteration, loss:0.053\n",
      " num 185 epoch \n",
      "####### Training Loss #######\n",
      "[0.10797648]\n",
      "Checking accuracy on test set\n",
      "Got 683 / 1000 correct (68.30)\n",
      "187 epoch,   500 iteration, loss:0.052\n",
      "Checking accuracy on test set\n",
      "Got 696 / 1000 correct (69.60)\n",
      "187 epoch,  1000 iteration, loss:0.059\n",
      " num 186 epoch \n",
      "####### Training Loss #######\n",
      "[0.10943481]\n",
      "Checking accuracy on test set\n",
      "Got 664 / 1000 correct (66.40)\n",
      "188 epoch,   500 iteration, loss:0.066\n",
      "Checking accuracy on test set\n",
      "Got 673 / 1000 correct (67.30)\n",
      "188 epoch,  1000 iteration, loss:0.055\n",
      " num 187 epoch \n",
      "####### Training Loss #######\n",
      "[0.11515556]\n",
      "Checking accuracy on test set\n",
      "Got 684 / 1000 correct (68.40)\n",
      "189 epoch,   500 iteration, loss:0.061\n",
      "Checking accuracy on test set\n",
      "Got 713 / 1000 correct (71.30)\n",
      "189 epoch,  1000 iteration, loss:0.055\n",
      " num 188 epoch \n",
      "####### Training Loss #######\n",
      "[0.11553258]\n",
      "Checking accuracy on test set\n",
      "Got 661 / 1000 correct (66.10)\n",
      "190 epoch,   500 iteration, loss:0.050\n",
      "Checking accuracy on test set\n",
      "Got 664 / 1000 correct (66.40)\n",
      "190 epoch,  1000 iteration, loss:0.051\n",
      " num 189 epoch \n",
      "####### Training Loss #######\n",
      "[0.09787566]\n",
      "Checking accuracy on test set\n",
      "Got 695 / 1000 correct (69.50)\n",
      "191 epoch,   500 iteration, loss:0.048\n",
      "Checking accuracy on test set\n",
      "Got 692 / 1000 correct (69.20)\n",
      "191 epoch,  1000 iteration, loss:0.042\n",
      " num 190 epoch \n",
      "####### Training Loss #######\n",
      "[0.09015953]\n",
      "Checking accuracy on test set\n",
      "Got 686 / 1000 correct (68.60)\n",
      "192 epoch,   500 iteration, loss:0.048\n",
      "Checking accuracy on test set\n",
      "Got 692 / 1000 correct (69.20)\n",
      "192 epoch,  1000 iteration, loss:0.042\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " num 191 epoch \n",
      "####### Training Loss #######\n",
      "[0.09565709]\n",
      "Checking accuracy on test set\n",
      "Got 670 / 1000 correct (67.00)\n",
      "193 epoch,   500 iteration, loss:0.058\n",
      "Checking accuracy on test set\n",
      "Got 691 / 1000 correct (69.10)\n",
      "193 epoch,  1000 iteration, loss:0.059\n",
      " num 192 epoch \n",
      "####### Training Loss #######\n",
      "[0.11791696]\n",
      "Checking accuracy on test set\n",
      "Got 669 / 1000 correct (66.90)\n",
      "194 epoch,   500 iteration, loss:0.049\n",
      "Checking accuracy on test set\n",
      "Got 688 / 1000 correct (68.80)\n",
      "194 epoch,  1000 iteration, loss:0.049\n",
      " num 193 epoch \n",
      "####### Training Loss #######\n",
      "[0.1000282]\n",
      "Checking accuracy on test set\n",
      "Got 675 / 1000 correct (67.50)\n",
      "195 epoch,   500 iteration, loss:0.057\n",
      "Checking accuracy on test set\n",
      "Got 702 / 1000 correct (70.20)\n",
      "195 epoch,  1000 iteration, loss:0.039\n",
      " num 194 epoch \n",
      "####### Training Loss #######\n",
      "[0.09834992]\n",
      "Checking accuracy on test set\n",
      "Got 703 / 1000 correct (70.30)\n",
      "196 epoch,   500 iteration, loss:0.048\n",
      "Checking accuracy on test set\n",
      "Got 688 / 1000 correct (68.80)\n",
      "196 epoch,  1000 iteration, loss:0.053\n",
      " num 195 epoch \n",
      "####### Training Loss #######\n",
      "[0.09795279]\n",
      "Checking accuracy on test set\n",
      "Got 697 / 1000 correct (69.70)\n",
      "197 epoch,   500 iteration, loss:0.052\n",
      "Checking accuracy on test set\n",
      "Got 689 / 1000 correct (68.90)\n",
      "197 epoch,  1000 iteration, loss:0.060\n",
      " num 196 epoch \n",
      "####### Training Loss #######\n",
      "[0.10396611]\n",
      "Checking accuracy on test set\n",
      "Got 693 / 1000 correct (69.30)\n",
      "198 epoch,   500 iteration, loss:0.057\n",
      "Checking accuracy on test set\n",
      "Got 682 / 1000 correct (68.20)\n",
      "198 epoch,  1000 iteration, loss:0.060\n",
      " num 197 epoch \n",
      "####### Training Loss #######\n",
      "[0.11587333]\n",
      "Checking accuracy on test set\n",
      "Got 680 / 1000 correct (68.00)\n",
      "199 epoch,   500 iteration, loss:0.049\n",
      "Checking accuracy on test set\n",
      "Got 693 / 1000 correct (69.30)\n",
      "199 epoch,  1000 iteration, loss:0.040\n",
      " num 198 epoch \n",
      "####### Training Loss #######\n",
      "[0.09639084]\n",
      "Checking accuracy on test set\n",
      "Got 673 / 1000 correct (67.30)\n",
      "200 epoch,   500 iteration, loss:0.061\n",
      "Checking accuracy on test set\n",
      "Got 684 / 1000 correct (68.40)\n",
      "200 epoch,  1000 iteration, loss:0.051\n",
      " num 199 epoch \n",
      "####### Training Loss #######\n",
      "[0.11161142]\n",
      "Checking accuracy on test set\n",
      "Got 688 / 1000 correct (68.80)\n",
      "201 epoch,   500 iteration, loss:0.045\n",
      "Checking accuracy on test set\n",
      "Got 704 / 1000 correct (70.40)\n",
      "201 epoch,  1000 iteration, loss:0.053\n",
      " num 200 epoch \n",
      "####### Training Loss #######\n",
      "[0.10211247]\n",
      "Checking accuracy on test set\n",
      "Got 693 / 1000 correct (69.30)\n",
      "202 epoch,   500 iteration, loss:0.054\n",
      "Checking accuracy on test set\n",
      "Got 664 / 1000 correct (66.40)\n",
      "202 epoch,  1000 iteration, loss:0.049\n",
      " num 201 epoch \n",
      "####### Training Loss #######\n",
      "[0.10336341]\n",
      "Checking accuracy on test set\n",
      "Got 685 / 1000 correct (68.50)\n",
      "203 epoch,   500 iteration, loss:0.039\n",
      "Checking accuracy on test set\n",
      "Got 694 / 1000 correct (69.40)\n",
      "203 epoch,  1000 iteration, loss:0.039\n",
      " num 202 epoch \n",
      "####### Training Loss #######\n",
      "[0.07990264]\n",
      "Checking accuracy on test set\n",
      "Got 702 / 1000 correct (70.20)\n",
      "204 epoch,   500 iteration, loss:0.051\n",
      "Checking accuracy on test set\n",
      "Got 697 / 1000 correct (69.70)\n",
      "204 epoch,  1000 iteration, loss:0.058\n",
      " num 203 epoch \n",
      "####### Training Loss #######\n",
      "[0.10187736]\n",
      "Checking accuracy on test set\n",
      "Got 690 / 1000 correct (69.00)\n",
      "205 epoch,   500 iteration, loss:0.044\n",
      "Checking accuracy on test set\n",
      "Got 691 / 1000 correct (69.10)\n",
      "205 epoch,  1000 iteration, loss:0.054\n",
      " num 204 epoch \n",
      "####### Training Loss #######\n",
      "[0.0984933]\n",
      "Checking accuracy on test set\n",
      "Got 666 / 1000 correct (66.60)\n",
      "206 epoch,   500 iteration, loss:0.053\n",
      "Checking accuracy on test set\n",
      "Got 689 / 1000 correct (68.90)\n",
      "206 epoch,  1000 iteration, loss:0.047\n",
      " num 205 epoch \n",
      "####### Training Loss #######\n",
      "[0.10225423]\n",
      "Checking accuracy on test set\n",
      "Got 669 / 1000 correct (66.90)\n",
      "207 epoch,   500 iteration, loss:0.045\n",
      "Checking accuracy on test set\n",
      "Got 693 / 1000 correct (69.30)\n",
      "207 epoch,  1000 iteration, loss:0.038\n",
      " num 206 epoch \n",
      "####### Training Loss #######\n",
      "[0.09134946]\n",
      "Checking accuracy on test set\n",
      "Got 686 / 1000 correct (68.60)\n",
      "208 epoch,   500 iteration, loss:0.045\n",
      "Checking accuracy on test set\n",
      "Got 695 / 1000 correct (69.50)\n",
      "208 epoch,  1000 iteration, loss:0.048\n",
      " num 207 epoch \n",
      "####### Training Loss #######\n",
      "[0.09831763]\n",
      "Checking accuracy on test set\n",
      "Got 694 / 1000 correct (69.40)\n",
      "209 epoch,   500 iteration, loss:0.043\n",
      "Checking accuracy on test set\n",
      "Got 692 / 1000 correct (69.20)\n",
      "209 epoch,  1000 iteration, loss:0.052\n",
      " num 208 epoch \n",
      "####### Training Loss #######\n",
      "[0.09445737]\n",
      "Checking accuracy on test set\n",
      "Got 667 / 1000 correct (66.70)\n",
      "210 epoch,   500 iteration, loss:0.057\n",
      "Checking accuracy on test set\n",
      "Got 668 / 1000 correct (66.80)\n",
      "210 epoch,  1000 iteration, loss:0.042\n",
      " num 209 epoch \n",
      "####### Training Loss #######\n",
      "[0.1035575]\n",
      "Checking accuracy on test set\n",
      "Got 683 / 1000 correct (68.30)\n",
      "211 epoch,   500 iteration, loss:0.051\n",
      "Checking accuracy on test set\n",
      "Got 675 / 1000 correct (67.50)\n",
      "211 epoch,  1000 iteration, loss:0.045\n",
      " num 210 epoch \n",
      "####### Training Loss #######\n",
      "[0.09939207]\n",
      "Checking accuracy on test set\n",
      "Got 686 / 1000 correct (68.60)\n",
      "212 epoch,   500 iteration, loss:0.043\n",
      "Checking accuracy on test set\n",
      "Got 685 / 1000 correct (68.50)\n",
      "212 epoch,  1000 iteration, loss:0.055\n",
      " num 211 epoch \n",
      "####### Training Loss #######\n",
      "[0.09990351]\n",
      "Checking accuracy on test set\n",
      "Got 694 / 1000 correct (69.40)\n",
      "213 epoch,   500 iteration, loss:0.056\n",
      "Checking accuracy on test set\n",
      "Got 689 / 1000 correct (68.90)\n",
      "213 epoch,  1000 iteration, loss:0.043\n",
      " num 212 epoch \n",
      "####### Training Loss #######\n",
      "[0.1013199]\n",
      "Checking accuracy on test set\n",
      "Got 667 / 1000 correct (66.70)\n",
      "214 epoch,   500 iteration, loss:0.045\n",
      "Checking accuracy on test set\n",
      "Got 685 / 1000 correct (68.50)\n",
      "214 epoch,  1000 iteration, loss:0.045\n",
      " num 213 epoch \n",
      "####### Training Loss #######\n",
      "[0.09418976]\n",
      "Checking accuracy on test set\n",
      "Got 677 / 1000 correct (67.70)\n",
      "215 epoch,   500 iteration, loss:0.049\n",
      "Checking accuracy on test set\n",
      "Got 697 / 1000 correct (69.70)\n",
      "215 epoch,  1000 iteration, loss:0.054\n",
      " num 214 epoch \n",
      "####### Training Loss #######\n",
      "[0.10468439]\n",
      "Checking accuracy on test set\n",
      "Got 700 / 1000 correct (70.00)\n",
      "216 epoch,   500 iteration, loss:0.053\n",
      "Checking accuracy on test set\n",
      "Got 676 / 1000 correct (67.60)\n",
      "216 epoch,  1000 iteration, loss:0.051\n",
      " num 215 epoch \n",
      "####### Training Loss #######\n",
      "[0.10688823]\n",
      "Checking accuracy on test set\n",
      "Got 686 / 1000 correct (68.60)\n",
      "217 epoch,   500 iteration, loss:0.057\n",
      "Checking accuracy on test set\n",
      "Got 672 / 1000 correct (67.20)\n",
      "217 epoch,  1000 iteration, loss:0.064\n",
      " num 216 epoch \n",
      "####### Training Loss #######\n",
      "[0.12524026]\n",
      "Checking accuracy on test set\n",
      "Got 672 / 1000 correct (67.20)\n",
      "218 epoch,   500 iteration, loss:0.053\n",
      "Checking accuracy on test set\n",
      "Got 703 / 1000 correct (70.30)\n",
      "218 epoch,  1000 iteration, loss:0.050\n",
      " num 217 epoch \n",
      "####### Training Loss #######\n",
      "[0.10142921]\n",
      "Checking accuracy on test set\n",
      "Got 674 / 1000 correct (67.40)\n",
      "219 epoch,   500 iteration, loss:0.046\n",
      "Checking accuracy on test set\n",
      "Got 709 / 1000 correct (70.90)\n",
      "219 epoch,  1000 iteration, loss:0.051\n",
      " num 218 epoch \n",
      "####### Training Loss #######\n",
      "[0.09928187]\n",
      "Checking accuracy on test set\n",
      "Got 688 / 1000 correct (68.80)\n",
      "220 epoch,   500 iteration, loss:0.040\n",
      "Checking accuracy on test set\n",
      "Got 696 / 1000 correct (69.60)\n",
      "220 epoch,  1000 iteration, loss:0.054\n",
      " num 219 epoch \n",
      "####### Training Loss #######\n",
      "[0.09933834]\n",
      "Checking accuracy on test set\n",
      "Got 681 / 1000 correct (68.10)\n",
      "221 epoch,   500 iteration, loss:0.047\n",
      "Checking accuracy on test set\n",
      "Got 717 / 1000 correct (71.70)\n",
      "221 epoch,  1000 iteration, loss:0.051\n",
      " num 220 epoch \n",
      "####### Training Loss #######\n",
      "[0.09133972]\n",
      "Checking accuracy on test set\n",
      "Got 699 / 1000 correct (69.90)\n",
      "222 epoch,   500 iteration, loss:0.051\n",
      "Checking accuracy on test set\n",
      "Got 692 / 1000 correct (69.20)\n",
      "222 epoch,  1000 iteration, loss:0.066\n",
      " num 221 epoch \n",
      "####### Training Loss #######\n",
      "[0.12550542]\n",
      "Checking accuracy on test set\n",
      "Got 694 / 1000 correct (69.40)\n",
      "223 epoch,   500 iteration, loss:0.056\n",
      "Checking accuracy on test set\n",
      "Got 678 / 1000 correct (67.80)\n",
      "223 epoch,  1000 iteration, loss:0.057\n",
      " num 222 epoch \n",
      "####### Training Loss #######\n",
      "[0.10993531]\n",
      "Checking accuracy on test set\n",
      "Got 672 / 1000 correct (67.20)\n",
      "224 epoch,   500 iteration, loss:0.046\n",
      "Checking accuracy on test set\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 695 / 1000 correct (69.50)\n",
      "224 epoch,  1000 iteration, loss:0.049\n",
      " num 223 epoch \n",
      "####### Training Loss #######\n",
      "[0.09500859]\n",
      "Checking accuracy on test set\n",
      "Got 692 / 1000 correct (69.20)\n",
      "225 epoch,   500 iteration, loss:0.044\n",
      "Checking accuracy on test set\n",
      "Got 694 / 1000 correct (69.40)\n",
      "225 epoch,  1000 iteration, loss:0.045\n",
      " num 224 epoch \n",
      "####### Training Loss #######\n",
      "[0.09280657]\n",
      "Checking accuracy on test set\n",
      "Got 700 / 1000 correct (70.00)\n",
      "226 epoch,   500 iteration, loss:0.047\n",
      "Checking accuracy on test set\n",
      "Got 701 / 1000 correct (70.10)\n",
      "226 epoch,  1000 iteration, loss:0.047\n",
      " num 225 epoch \n",
      "####### Training Loss #######\n",
      "[0.09552701]\n",
      "Checking accuracy on test set\n",
      "Got 668 / 1000 correct (66.80)\n",
      "227 epoch,   500 iteration, loss:0.062\n",
      "Checking accuracy on test set\n",
      "Got 695 / 1000 correct (69.50)\n",
      "227 epoch,  1000 iteration, loss:0.061\n",
      " num 226 epoch \n",
      "####### Training Loss #######\n",
      "[0.11311798]\n",
      "Checking accuracy on test set\n",
      "Got 676 / 1000 correct (67.60)\n",
      "228 epoch,   500 iteration, loss:0.048\n",
      "Checking accuracy on test set\n",
      "Got 698 / 1000 correct (69.80)\n",
      "228 epoch,  1000 iteration, loss:0.047\n",
      " num 227 epoch \n",
      "####### Training Loss #######\n",
      "[0.09890903]\n",
      "Checking accuracy on test set\n",
      "Got 701 / 1000 correct (70.10)\n",
      "229 epoch,   500 iteration, loss:0.050\n",
      "Checking accuracy on test set\n",
      "Got 690 / 1000 correct (69.00)\n",
      "229 epoch,  1000 iteration, loss:0.059\n",
      " num 228 epoch \n",
      "####### Training Loss #######\n",
      "[0.10776035]\n",
      "Checking accuracy on test set\n",
      "Got 686 / 1000 correct (68.60)\n",
      "230 epoch,   500 iteration, loss:0.059\n",
      "Checking accuracy on test set\n",
      "Got 698 / 1000 correct (69.80)\n",
      "230 epoch,  1000 iteration, loss:0.038\n",
      " num 229 epoch \n",
      "####### Training Loss #######\n",
      "[0.09510173]\n",
      "Checking accuracy on test set\n",
      "Got 695 / 1000 correct (69.50)\n",
      "231 epoch,   500 iteration, loss:0.036\n",
      "Checking accuracy on test set\n",
      "Got 670 / 1000 correct (67.00)\n",
      "231 epoch,  1000 iteration, loss:0.049\n",
      " num 230 epoch \n",
      "####### Training Loss #######\n",
      "[0.08670033]\n",
      "Checking accuracy on test set\n",
      "Got 677 / 1000 correct (67.70)\n",
      "232 epoch,   500 iteration, loss:0.061\n",
      "Checking accuracy on test set\n",
      "Got 689 / 1000 correct (68.90)\n",
      "232 epoch,  1000 iteration, loss:0.038\n",
      " num 231 epoch \n",
      "####### Training Loss #######\n",
      "[0.10529041]\n",
      "Checking accuracy on test set\n",
      "Got 666 / 1000 correct (66.60)\n",
      "233 epoch,   500 iteration, loss:0.047\n",
      "Checking accuracy on test set\n",
      "Got 688 / 1000 correct (68.80)\n",
      "233 epoch,  1000 iteration, loss:0.051\n",
      " num 232 epoch \n",
      "####### Training Loss #######\n",
      "[0.10362531]\n",
      "Checking accuracy on test set\n",
      "Got 683 / 1000 correct (68.30)\n",
      "234 epoch,   500 iteration, loss:0.039\n",
      "Checking accuracy on test set\n",
      "Got 679 / 1000 correct (67.90)\n",
      "234 epoch,  1000 iteration, loss:0.043\n",
      " num 233 epoch \n",
      "####### Training Loss #######\n",
      "[0.09272688]\n",
      "Checking accuracy on test set\n",
      "Got 692 / 1000 correct (69.20)\n",
      "235 epoch,   500 iteration, loss:0.047\n",
      "Checking accuracy on test set\n",
      "Got 707 / 1000 correct (70.70)\n",
      "235 epoch,  1000 iteration, loss:0.049\n",
      " num 234 epoch \n",
      "####### Training Loss #######\n",
      "[0.0925314]\n",
      "Checking accuracy on test set\n",
      "Got 687 / 1000 correct (68.70)\n",
      "236 epoch,   500 iteration, loss:0.041\n",
      "Checking accuracy on test set\n",
      "Got 701 / 1000 correct (70.10)\n",
      "236 epoch,  1000 iteration, loss:0.050\n",
      " num 235 epoch \n",
      "####### Training Loss #######\n",
      "[0.08883406]\n",
      "Checking accuracy on test set\n",
      "Got 689 / 1000 correct (68.90)\n",
      "237 epoch,   500 iteration, loss:0.046\n",
      "Checking accuracy on test set\n",
      "Got 691 / 1000 correct (69.10)\n",
      "237 epoch,  1000 iteration, loss:0.056\n",
      " num 236 epoch \n",
      "####### Training Loss #######\n",
      "[0.09748253]\n",
      "Checking accuracy on test set\n",
      "Got 649 / 1000 correct (64.90)\n",
      "238 epoch,   500 iteration, loss:0.060\n",
      "Checking accuracy on test set\n",
      "Got 694 / 1000 correct (69.40)\n",
      "238 epoch,  1000 iteration, loss:0.050\n",
      " num 237 epoch \n",
      "####### Training Loss #######\n",
      "[0.10297666]\n",
      "Checking accuracy on test set\n",
      "Got 691 / 1000 correct (69.10)\n",
      "239 epoch,   500 iteration, loss:0.040\n",
      "Checking accuracy on test set\n",
      "Got 690 / 1000 correct (69.00)\n",
      "239 epoch,  1000 iteration, loss:0.050\n",
      " num 238 epoch \n",
      "####### Training Loss #######\n",
      "[0.09473803]\n",
      "Checking accuracy on test set\n",
      "Got 680 / 1000 correct (68.00)\n",
      "240 epoch,   500 iteration, loss:0.047\n",
      "Checking accuracy on test set\n",
      "Got 713 / 1000 correct (71.30)\n",
      "240 epoch,  1000 iteration, loss:0.044\n",
      " num 239 epoch \n",
      "####### Training Loss #######\n",
      "[0.08688719]\n",
      "Checking accuracy on test set\n",
      "Got 680 / 1000 correct (68.00)\n",
      "241 epoch,   500 iteration, loss:0.045\n",
      "Checking accuracy on test set\n",
      "Got 674 / 1000 correct (67.40)\n",
      "241 epoch,  1000 iteration, loss:0.064\n",
      " num 240 epoch \n",
      "####### Training Loss #######\n",
      "[0.10848314]\n",
      "Checking accuracy on test set\n",
      "Got 704 / 1000 correct (70.40)\n",
      "242 epoch,   500 iteration, loss:0.050\n",
      "Checking accuracy on test set\n",
      "Got 695 / 1000 correct (69.50)\n",
      "242 epoch,  1000 iteration, loss:0.061\n",
      " num 241 epoch \n",
      "####### Training Loss #######\n",
      "[0.1117221]\n",
      "Checking accuracy on test set\n",
      "Got 696 / 1000 correct (69.60)\n",
      "243 epoch,   500 iteration, loss:0.048\n",
      "Checking accuracy on test set\n",
      "Got 711 / 1000 correct (71.10)\n",
      "243 epoch,  1000 iteration, loss:0.037\n",
      " num 242 epoch \n",
      "####### Training Loss #######\n",
      "[0.08638432]\n",
      "Checking accuracy on test set\n",
      "Got 685 / 1000 correct (68.50)\n",
      "244 epoch,   500 iteration, loss:0.053\n",
      "Checking accuracy on test set\n",
      "Got 689 / 1000 correct (68.90)\n",
      "244 epoch,  1000 iteration, loss:0.056\n",
      " num 243 epoch \n",
      "####### Training Loss #######\n",
      "[0.10766779]\n",
      "Checking accuracy on test set\n",
      "Got 684 / 1000 correct (68.40)\n",
      "245 epoch,   500 iteration, loss:0.040\n",
      "Checking accuracy on test set\n",
      "Got 695 / 1000 correct (69.50)\n",
      "245 epoch,  1000 iteration, loss:0.046\n",
      " num 244 epoch \n",
      "####### Training Loss #######\n",
      "[0.08974984]\n",
      "Checking accuracy on test set\n",
      "Got 666 / 1000 correct (66.60)\n",
      "246 epoch,   500 iteration, loss:0.052\n",
      "Checking accuracy on test set\n",
      "Got 689 / 1000 correct (68.90)\n",
      "246 epoch,  1000 iteration, loss:0.051\n",
      " num 245 epoch \n",
      "####### Training Loss #######\n",
      "[0.10272203]\n",
      "Checking accuracy on test set\n",
      "Got 671 / 1000 correct (67.10)\n",
      "247 epoch,   500 iteration, loss:0.052\n",
      "Checking accuracy on test set\n",
      "Got 694 / 1000 correct (69.40)\n",
      "247 epoch,  1000 iteration, loss:0.040\n",
      " num 246 epoch \n",
      "####### Training Loss #######\n",
      "[0.09126145]\n",
      "Checking accuracy on test set\n",
      "Got 702 / 1000 correct (70.20)\n",
      "248 epoch,   500 iteration, loss:0.044\n",
      "Checking accuracy on test set\n",
      "Got 711 / 1000 correct (71.10)\n",
      "248 epoch,  1000 iteration, loss:0.035\n",
      " num 247 epoch \n",
      "####### Training Loss #######\n",
      "[0.07829729]\n",
      "Checking accuracy on test set\n",
      "Got 704 / 1000 correct (70.40)\n",
      "249 epoch,   500 iteration, loss:0.055\n",
      "Checking accuracy on test set\n",
      "Got 669 / 1000 correct (66.90)\n",
      "249 epoch,  1000 iteration, loss:0.048\n",
      " num 248 epoch \n",
      "####### Training Loss #######\n",
      "[0.09975085]\n",
      "Checking accuracy on test set\n",
      "Got 653 / 1000 correct (65.30)\n",
      "250 epoch,   500 iteration, loss:0.050\n",
      "Checking accuracy on test set\n",
      "Got 678 / 1000 correct (67.80)\n",
      "250 epoch,  1000 iteration, loss:0.054\n",
      " num 249 epoch \n",
      "####### Training Loss #######\n",
      "[0.10750385]\n",
      "Checking accuracy on test set\n",
      "Got 679 / 1000 correct (67.90)\n",
      "251 epoch,   500 iteration, loss:0.040\n",
      "Checking accuracy on test set\n",
      "Got 705 / 1000 correct (70.50)\n",
      "251 epoch,  1000 iteration, loss:0.047\n",
      " num 250 epoch \n",
      "####### Training Loss #######\n",
      "[0.08733363]\n",
      "Checking accuracy on test set\n",
      "Got 686 / 1000 correct (68.60)\n",
      "252 epoch,   500 iteration, loss:0.048\n",
      "Checking accuracy on test set\n",
      "Got 669 / 1000 correct (66.90)\n",
      "252 epoch,  1000 iteration, loss:0.054\n",
      " num 251 epoch \n",
      "####### Training Loss #######\n",
      "[0.10364983]\n",
      "Checking accuracy on test set\n",
      "Got 696 / 1000 correct (69.60)\n",
      "253 epoch,   500 iteration, loss:0.056\n",
      "Checking accuracy on test set\n",
      "Got 706 / 1000 correct (70.60)\n",
      "253 epoch,  1000 iteration, loss:0.040\n",
      " num 252 epoch \n",
      "####### Training Loss #######\n",
      "[0.09437012]\n",
      "Checking accuracy on test set\n",
      "Got 700 / 1000 correct (70.00)\n",
      "254 epoch,   500 iteration, loss:0.043\n",
      "Checking accuracy on test set\n",
      "Got 693 / 1000 correct (69.30)\n",
      "254 epoch,  1000 iteration, loss:0.054\n",
      " num 253 epoch \n",
      "####### Training Loss #######\n",
      "[0.10047905]\n",
      "Checking accuracy on test set\n",
      "Got 665 / 1000 correct (66.50)\n",
      "255 epoch,   500 iteration, loss:0.036\n",
      "Checking accuracy on test set\n",
      "Got 696 / 1000 correct (69.60)\n",
      "255 epoch,  1000 iteration, loss:0.037\n",
      " num 254 epoch \n",
      "####### Training Loss #######\n",
      "[0.07528856]\n",
      "Checking accuracy on test set\n",
      "Got 685 / 1000 correct (68.50)\n",
      "256 epoch,   500 iteration, loss:0.054\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking accuracy on test set\n",
      "Got 701 / 1000 correct (70.10)\n",
      "256 epoch,  1000 iteration, loss:0.050\n",
      " num 255 epoch \n",
      "####### Training Loss #######\n",
      "[0.10572838]\n",
      "Checking accuracy on test set\n",
      "Got 700 / 1000 correct (70.00)\n",
      "257 epoch,   500 iteration, loss:0.047\n",
      "Checking accuracy on test set\n",
      "Got 686 / 1000 correct (68.60)\n",
      "257 epoch,  1000 iteration, loss:0.044\n",
      " num 256 epoch \n",
      "####### Training Loss #######\n",
      "[0.09681078]\n",
      "Checking accuracy on test set\n",
      "Got 671 / 1000 correct (67.10)\n",
      "258 epoch,   500 iteration, loss:0.046\n",
      "Checking accuracy on test set\n",
      "Got 667 / 1000 correct (66.70)\n",
      "258 epoch,  1000 iteration, loss:0.048\n",
      " num 257 epoch \n",
      "####### Training Loss #######\n",
      "[0.09377345]\n",
      "Checking accuracy on test set\n",
      "Got 691 / 1000 correct (69.10)\n",
      "259 epoch,   500 iteration, loss:0.030\n",
      "Checking accuracy on test set\n",
      "Got 723 / 1000 correct (72.30)\n",
      "259 epoch,  1000 iteration, loss:0.038\n",
      " num 258 epoch \n",
      "####### Training Loss #######\n",
      "[0.07034246]\n",
      "Checking accuracy on test set\n",
      "Got 696 / 1000 correct (69.60)\n",
      "260 epoch,   500 iteration, loss:0.048\n",
      "Checking accuracy on test set\n",
      "Got 709 / 1000 correct (70.90)\n",
      "260 epoch,  1000 iteration, loss:0.053\n",
      " num 259 epoch \n",
      "####### Training Loss #######\n",
      "[0.09729107]\n",
      "Checking accuracy on test set\n",
      "Got 691 / 1000 correct (69.10)\n",
      "261 epoch,   500 iteration, loss:0.037\n",
      "Checking accuracy on test set\n",
      "Got 689 / 1000 correct (68.90)\n",
      "261 epoch,  1000 iteration, loss:0.049\n",
      " num 260 epoch \n",
      "####### Training Loss #######\n",
      "[0.08541569]\n",
      "Checking accuracy on test set\n",
      "Got 694 / 1000 correct (69.40)\n",
      "262 epoch,   500 iteration, loss:0.038\n",
      "Checking accuracy on test set\n",
      "Got 719 / 1000 correct (71.90)\n",
      "262 epoch,  1000 iteration, loss:0.037\n",
      " num 261 epoch \n",
      "####### Training Loss #######\n",
      "[0.08765405]\n",
      "Checking accuracy on test set\n",
      "Got 681 / 1000 correct (68.10)\n",
      "263 epoch,   500 iteration, loss:0.041\n",
      "Checking accuracy on test set\n",
      "Got 693 / 1000 correct (69.30)\n",
      "263 epoch,  1000 iteration, loss:0.043\n",
      " num 262 epoch \n",
      "####### Training Loss #######\n",
      "[0.08223727]\n",
      "Checking accuracy on test set\n",
      "Got 692 / 1000 correct (69.20)\n",
      "264 epoch,   500 iteration, loss:0.047\n",
      "Checking accuracy on test set\n",
      "Got 692 / 1000 correct (69.20)\n",
      "264 epoch,  1000 iteration, loss:0.056\n",
      " num 263 epoch \n",
      "####### Training Loss #######\n",
      "[0.10078653]\n",
      "Checking accuracy on test set\n",
      "Got 687 / 1000 correct (68.70)\n",
      "265 epoch,   500 iteration, loss:0.042\n",
      "Checking accuracy on test set\n",
      "Got 696 / 1000 correct (69.60)\n",
      "265 epoch,  1000 iteration, loss:0.057\n",
      " num 264 epoch \n",
      "####### Training Loss #######\n",
      "[0.09605183]\n",
      "Checking accuracy on test set\n",
      "Got 680 / 1000 correct (68.00)\n",
      "266 epoch,   500 iteration, loss:0.038\n",
      "Checking accuracy on test set\n",
      "Got 691 / 1000 correct (69.10)\n",
      "266 epoch,  1000 iteration, loss:0.049\n",
      " num 265 epoch \n",
      "####### Training Loss #######\n",
      "[0.09209509]\n",
      "Checking accuracy on test set\n",
      "Got 695 / 1000 correct (69.50)\n",
      "267 epoch,   500 iteration, loss:0.055\n",
      "Checking accuracy on test set\n",
      "Got 679 / 1000 correct (67.90)\n",
      "267 epoch,  1000 iteration, loss:0.043\n",
      " num 266 epoch \n",
      "####### Training Loss #######\n",
      "[0.09612979]\n",
      "Checking accuracy on test set\n",
      "Got 702 / 1000 correct (70.20)\n",
      "268 epoch,   500 iteration, loss:0.048\n",
      "Checking accuracy on test set\n",
      "Got 681 / 1000 correct (68.10)\n",
      "268 epoch,  1000 iteration, loss:0.042\n",
      " num 267 epoch \n",
      "####### Training Loss #######\n",
      "[0.09127306]\n",
      "Checking accuracy on test set\n",
      "Got 655 / 1000 correct (65.50)\n",
      "269 epoch,   500 iteration, loss:0.053\n",
      "Checking accuracy on test set\n",
      "Got 692 / 1000 correct (69.20)\n",
      "269 epoch,  1000 iteration, loss:0.041\n",
      " num 268 epoch \n",
      "####### Training Loss #######\n",
      "[0.09768133]\n",
      "Checking accuracy on test set\n",
      "Got 720 / 1000 correct (72.00)\n",
      "270 epoch,   500 iteration, loss:0.054\n",
      "Checking accuracy on test set\n",
      "Got 667 / 1000 correct (66.70)\n",
      "270 epoch,  1000 iteration, loss:0.046\n",
      " num 269 epoch \n",
      "####### Training Loss #######\n",
      "[0.09684905]\n",
      "Checking accuracy on test set\n",
      "Got 693 / 1000 correct (69.30)\n",
      "271 epoch,   500 iteration, loss:0.033\n",
      "Checking accuracy on test set\n",
      "Got 691 / 1000 correct (69.10)\n",
      "271 epoch,  1000 iteration, loss:0.053\n",
      " num 270 epoch \n",
      "####### Training Loss #######\n",
      "[0.08823292]\n",
      "Checking accuracy on test set\n",
      "Got 676 / 1000 correct (67.60)\n",
      "272 epoch,   500 iteration, loss:0.044\n",
      "Checking accuracy on test set\n",
      "Got 699 / 1000 correct (69.90)\n",
      "272 epoch,  1000 iteration, loss:0.036\n",
      " num 271 epoch \n",
      "####### Training Loss #######\n",
      "[0.08092985]\n",
      "Checking accuracy on test set\n",
      "Got 684 / 1000 correct (68.40)\n",
      "273 epoch,   500 iteration, loss:0.045\n",
      "Checking accuracy on test set\n",
      "Got 692 / 1000 correct (69.20)\n",
      "273 epoch,  1000 iteration, loss:0.052\n",
      " num 272 epoch \n",
      "####### Training Loss #######\n",
      "[0.09137509]\n",
      "Checking accuracy on test set\n",
      "Got 704 / 1000 correct (70.40)\n",
      "274 epoch,   500 iteration, loss:0.045\n",
      "Checking accuracy on test set\n",
      "Got 708 / 1000 correct (70.80)\n",
      "274 epoch,  1000 iteration, loss:0.035\n",
      " num 273 epoch \n",
      "####### Training Loss #######\n",
      "[0.08008117]\n",
      "Checking accuracy on test set\n",
      "Got 704 / 1000 correct (70.40)\n",
      "275 epoch,   500 iteration, loss:0.033\n",
      "Checking accuracy on test set\n",
      "Got 701 / 1000 correct (70.10)\n",
      "275 epoch,  1000 iteration, loss:0.059\n",
      " num 274 epoch \n",
      "####### Training Loss #######\n",
      "[0.09787231]\n",
      "Checking accuracy on test set\n",
      "Got 658 / 1000 correct (65.80)\n",
      "276 epoch,   500 iteration, loss:0.053\n",
      "Checking accuracy on test set\n",
      "Got 684 / 1000 correct (68.40)\n",
      "276 epoch,  1000 iteration, loss:0.053\n",
      " num 275 epoch \n",
      "####### Training Loss #######\n",
      "[0.10200076]\n",
      "Checking accuracy on test set\n",
      "Got 678 / 1000 correct (67.80)\n",
      "277 epoch,   500 iteration, loss:0.037\n",
      "Checking accuracy on test set\n",
      "Got 693 / 1000 correct (69.30)\n",
      "277 epoch,  1000 iteration, loss:0.045\n",
      " num 276 epoch \n",
      "####### Training Loss #######\n",
      "[0.08996976]\n",
      "Checking accuracy on test set\n",
      "Got 697 / 1000 correct (69.70)\n",
      "278 epoch,   500 iteration, loss:0.050\n",
      "Checking accuracy on test set\n",
      "Got 687 / 1000 correct (68.70)\n",
      "278 epoch,  1000 iteration, loss:0.041\n",
      " num 277 epoch \n",
      "####### Training Loss #######\n",
      "[0.09089097]\n",
      "Checking accuracy on test set\n",
      "Got 674 / 1000 correct (67.40)\n",
      "279 epoch,   500 iteration, loss:0.047\n",
      "Checking accuracy on test set\n",
      "Got 704 / 1000 correct (70.40)\n",
      "279 epoch,  1000 iteration, loss:0.040\n",
      " num 278 epoch \n",
      "####### Training Loss #######\n",
      "[0.08135034]\n",
      "Checking accuracy on test set\n",
      "Got 692 / 1000 correct (69.20)\n",
      "280 epoch,   500 iteration, loss:0.045\n",
      "Checking accuracy on test set\n",
      "Got 687 / 1000 correct (68.70)\n",
      "280 epoch,  1000 iteration, loss:0.045\n",
      " num 279 epoch \n",
      "####### Training Loss #######\n",
      "[0.08988329]\n",
      "Checking accuracy on test set\n",
      "Got 667 / 1000 correct (66.70)\n",
      "281 epoch,   500 iteration, loss:0.046\n",
      "Checking accuracy on test set\n",
      "Got 704 / 1000 correct (70.40)\n",
      "281 epoch,  1000 iteration, loss:0.046\n",
      " num 280 epoch \n",
      "####### Training Loss #######\n",
      "[0.09365816]\n",
      "Checking accuracy on test set\n",
      "Got 690 / 1000 correct (69.00)\n",
      "282 epoch,   500 iteration, loss:0.055\n",
      "Checking accuracy on test set\n",
      "Got 718 / 1000 correct (71.80)\n",
      "282 epoch,  1000 iteration, loss:0.039\n",
      " num 281 epoch \n",
      "####### Training Loss #######\n",
      "[0.09198204]\n",
      "Checking accuracy on test set\n",
      "Got 697 / 1000 correct (69.70)\n",
      "283 epoch,   500 iteration, loss:0.038\n",
      "Checking accuracy on test set\n",
      "Got 692 / 1000 correct (69.20)\n",
      "283 epoch,  1000 iteration, loss:0.047\n",
      " num 282 epoch \n",
      "####### Training Loss #######\n",
      "[0.08884451]\n",
      "Checking accuracy on test set\n",
      "Got 676 / 1000 correct (67.60)\n",
      "284 epoch,   500 iteration, loss:0.044\n",
      "Checking accuracy on test set\n",
      "Got 706 / 1000 correct (70.60)\n",
      "284 epoch,  1000 iteration, loss:0.048\n",
      " num 283 epoch \n",
      "####### Training Loss #######\n",
      "[0.08546246]\n",
      "Checking accuracy on test set\n",
      "Got 688 / 1000 correct (68.80)\n",
      "285 epoch,   500 iteration, loss:0.048\n",
      "Checking accuracy on test set\n",
      "Got 705 / 1000 correct (70.50)\n",
      "285 epoch,  1000 iteration, loss:0.040\n",
      " num 284 epoch \n",
      "####### Training Loss #######\n",
      "[0.08996278]\n",
      "Checking accuracy on test set\n",
      "Got 693 / 1000 correct (69.30)\n",
      "286 epoch,   500 iteration, loss:0.038\n",
      "Checking accuracy on test set\n",
      "Got 705 / 1000 correct (70.50)\n",
      "286 epoch,  1000 iteration, loss:0.038\n",
      " num 285 epoch \n",
      "####### Training Loss #######\n",
      "[0.07211741]\n",
      "Checking accuracy on test set\n",
      "Got 683 / 1000 correct (68.30)\n",
      "287 epoch,   500 iteration, loss:0.045\n",
      "Checking accuracy on test set\n",
      "Got 708 / 1000 correct (70.80)\n",
      "287 epoch,  1000 iteration, loss:0.041\n",
      " num 286 epoch \n",
      "####### Training Loss #######\n",
      "[0.08789319]\n",
      "Checking accuracy on test set\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 677 / 1000 correct (67.70)\n",
      "288 epoch,   500 iteration, loss:0.044\n",
      "Checking accuracy on test set\n",
      "Got 693 / 1000 correct (69.30)\n",
      "288 epoch,  1000 iteration, loss:0.048\n",
      " num 287 epoch \n",
      "####### Training Loss #######\n",
      "[0.08915511]\n",
      "Checking accuracy on test set\n",
      "Got 687 / 1000 correct (68.70)\n",
      "289 epoch,   500 iteration, loss:0.050\n",
      "Checking accuracy on test set\n",
      "Got 667 / 1000 correct (66.70)\n",
      "289 epoch,  1000 iteration, loss:0.042\n",
      " num 288 epoch \n",
      "####### Training Loss #######\n",
      "[0.08780626]\n",
      "Checking accuracy on test set\n",
      "Got 683 / 1000 correct (68.30)\n",
      "290 epoch,   500 iteration, loss:0.040\n",
      "Checking accuracy on test set\n",
      "Got 713 / 1000 correct (71.30)\n",
      "290 epoch,  1000 iteration, loss:0.047\n",
      " num 289 epoch \n",
      "####### Training Loss #######\n",
      "[0.09052121]\n",
      "Checking accuracy on test set\n",
      "Got 698 / 1000 correct (69.80)\n",
      "291 epoch,   500 iteration, loss:0.047\n",
      "Checking accuracy on test set\n",
      "Got 692 / 1000 correct (69.20)\n",
      "291 epoch,  1000 iteration, loss:0.048\n",
      " num 290 epoch \n",
      "####### Training Loss #######\n",
      "[0.09471121]\n",
      "Checking accuracy on test set\n",
      "Got 679 / 1000 correct (67.90)\n",
      "292 epoch,   500 iteration, loss:0.049\n",
      "Checking accuracy on test set\n",
      "Got 688 / 1000 correct (68.80)\n",
      "292 epoch,  1000 iteration, loss:0.049\n",
      " num 291 epoch \n",
      "####### Training Loss #######\n",
      "[0.09900468]\n",
      "Checking accuracy on test set\n",
      "Got 678 / 1000 correct (67.80)\n",
      "293 epoch,   500 iteration, loss:0.050\n",
      "Checking accuracy on test set\n",
      "Got 684 / 1000 correct (68.40)\n",
      "293 epoch,  1000 iteration, loss:0.055\n",
      " num 292 epoch \n",
      "####### Training Loss #######\n",
      "[0.10308202]\n",
      "Checking accuracy on test set\n",
      "Got 685 / 1000 correct (68.50)\n",
      "294 epoch,   500 iteration, loss:0.046\n",
      "Checking accuracy on test set\n",
      "Got 685 / 1000 correct (68.50)\n",
      "294 epoch,  1000 iteration, loss:0.038\n",
      " num 293 epoch \n",
      "####### Training Loss #######\n",
      "[0.0813484]\n",
      "Checking accuracy on test set\n",
      "Got 676 / 1000 correct (67.60)\n",
      "295 epoch,   500 iteration, loss:0.031\n",
      "Checking accuracy on test set\n",
      "Got 706 / 1000 correct (70.60)\n",
      "295 epoch,  1000 iteration, loss:0.043\n",
      " num 294 epoch \n",
      "####### Training Loss #######\n",
      "[0.07668161]\n",
      "Checking accuracy on test set\n",
      "Got 676 / 1000 correct (67.60)\n",
      "296 epoch,   500 iteration, loss:0.045\n",
      "Checking accuracy on test set\n",
      "Got 690 / 1000 correct (69.00)\n",
      "296 epoch,  1000 iteration, loss:0.044\n",
      " num 295 epoch \n",
      "####### Training Loss #######\n",
      "[0.08798456]\n",
      "Checking accuracy on test set\n",
      "Got 679 / 1000 correct (67.90)\n",
      "297 epoch,   500 iteration, loss:0.042\n",
      "Checking accuracy on test set\n",
      "Got 712 / 1000 correct (71.20)\n",
      "297 epoch,  1000 iteration, loss:0.050\n",
      " num 296 epoch \n",
      "####### Training Loss #######\n",
      "[0.09279765]\n",
      "Checking accuracy on test set\n",
      "Got 703 / 1000 correct (70.30)\n",
      "298 epoch,   500 iteration, loss:0.043\n",
      "Checking accuracy on test set\n",
      "Got 703 / 1000 correct (70.30)\n",
      "298 epoch,  1000 iteration, loss:0.048\n",
      " num 297 epoch \n",
      "####### Training Loss #######\n",
      "[0.09758643]\n",
      "Checking accuracy on test set\n",
      "Got 679 / 1000 correct (67.90)\n",
      "299 epoch,   500 iteration, loss:0.047\n",
      "Checking accuracy on test set\n",
      "Got 690 / 1000 correct (69.00)\n",
      "299 epoch,  1000 iteration, loss:0.045\n",
      " num 298 epoch \n",
      "####### Training Loss #######\n",
      "[0.08757756]\n",
      "Checking accuracy on test set\n",
      "Got 691 / 1000 correct (69.10)\n",
      "300 epoch,   500 iteration, loss:0.031\n",
      "Checking accuracy on test set\n",
      "Got 697 / 1000 correct (69.70)\n",
      "300 epoch,  1000 iteration, loss:0.059\n",
      " num 299 epoch \n",
      "####### Training Loss #######\n",
      "[0.09675232]\n",
      "Checking accuracy on test set\n",
      "Got 680 / 1000 correct (68.00)\n",
      "301 epoch,   500 iteration, loss:0.038\n",
      "Checking accuracy on test set\n",
      "Got 691 / 1000 correct (69.10)\n",
      "301 epoch,  1000 iteration, loss:0.036\n",
      " num 300 epoch \n",
      "####### Training Loss #######\n",
      "[0.07966688]\n",
      "Checking accuracy on test set\n",
      "Got 661 / 1000 correct (66.10)\n",
      "302 epoch,   500 iteration, loss:0.051\n",
      "Checking accuracy on test set\n",
      "Got 682 / 1000 correct (68.20)\n",
      "302 epoch,  1000 iteration, loss:0.046\n",
      " num 301 epoch \n",
      "####### Training Loss #######\n",
      "[0.09458204]\n",
      "Checking accuracy on test set\n",
      "Got 685 / 1000 correct (68.50)\n",
      "303 epoch,   500 iteration, loss:0.041\n",
      "Checking accuracy on test set\n",
      "Got 686 / 1000 correct (68.60)\n",
      "303 epoch,  1000 iteration, loss:0.046\n",
      " num 302 epoch \n",
      "####### Training Loss #######\n",
      "[0.08790363]\n",
      "Checking accuracy on test set\n",
      "Got 693 / 1000 correct (69.30)\n",
      "304 epoch,   500 iteration, loss:0.039\n",
      "Checking accuracy on test set\n",
      "Got 707 / 1000 correct (70.70)\n",
      "304 epoch,  1000 iteration, loss:0.027\n",
      " num 303 epoch \n",
      "####### Training Loss #######\n",
      "[0.0728639]\n",
      "Checking accuracy on test set\n",
      "Got 679 / 1000 correct (67.90)\n",
      "305 epoch,   500 iteration, loss:0.026\n",
      "Checking accuracy on test set\n",
      "Got 675 / 1000 correct (67.50)\n",
      "305 epoch,  1000 iteration, loss:0.045\n",
      " num 304 epoch \n",
      "####### Training Loss #######\n",
      "[0.08142832]\n",
      "Checking accuracy on test set\n",
      "Got 688 / 1000 correct (68.80)\n",
      "306 epoch,   500 iteration, loss:0.035\n",
      "Checking accuracy on test set\n",
      "Got 695 / 1000 correct (69.50)\n",
      "306 epoch,  1000 iteration, loss:0.056\n",
      " num 305 epoch \n",
      "####### Training Loss #######\n",
      "[0.0900034]\n",
      "Checking accuracy on test set\n",
      "Got 699 / 1000 correct (69.90)\n",
      "307 epoch,   500 iteration, loss:0.040\n",
      "Checking accuracy on test set\n",
      "Got 696 / 1000 correct (69.60)\n",
      "307 epoch,  1000 iteration, loss:0.041\n",
      " num 306 epoch \n",
      "####### Training Loss #######\n",
      "[0.08507921]\n",
      "Checking accuracy on test set\n",
      "Got 679 / 1000 correct (67.90)\n",
      "308 epoch,   500 iteration, loss:0.049\n",
      "Checking accuracy on test set\n",
      "Got 696 / 1000 correct (69.60)\n",
      "308 epoch,  1000 iteration, loss:0.045\n",
      " num 307 epoch \n",
      "####### Training Loss #######\n",
      "[0.09336853]\n",
      "Checking accuracy on test set\n",
      "Got 692 / 1000 correct (69.20)\n",
      "309 epoch,   500 iteration, loss:0.041\n",
      "Checking accuracy on test set\n",
      "Got 704 / 1000 correct (70.40)\n",
      "309 epoch,  1000 iteration, loss:0.039\n",
      " num 308 epoch \n",
      "####### Training Loss #######\n",
      "[0.07926607]\n",
      "Checking accuracy on test set\n",
      "Got 679 / 1000 correct (67.90)\n",
      "310 epoch,   500 iteration, loss:0.046\n",
      "Checking accuracy on test set\n",
      "Got 699 / 1000 correct (69.90)\n",
      "310 epoch,  1000 iteration, loss:0.043\n",
      " num 309 epoch \n",
      "####### Training Loss #######\n",
      "[0.08765454]\n",
      "Checking accuracy on test set\n",
      "Got 691 / 1000 correct (69.10)\n",
      "311 epoch,   500 iteration, loss:0.046\n",
      "Checking accuracy on test set\n",
      "Got 692 / 1000 correct (69.20)\n",
      "311 epoch,  1000 iteration, loss:0.034\n",
      " num 310 epoch \n",
      "####### Training Loss #######\n",
      "[0.07712939]\n",
      "Checking accuracy on test set\n",
      "Got 686 / 1000 correct (68.60)\n",
      "312 epoch,   500 iteration, loss:0.047\n",
      "Checking accuracy on test set\n",
      "Got 676 / 1000 correct (67.60)\n",
      "312 epoch,  1000 iteration, loss:0.047\n",
      " num 311 epoch \n",
      "####### Training Loss #######\n",
      "[0.09053467]\n",
      "Checking accuracy on test set\n",
      "Got 672 / 1000 correct (67.20)\n",
      "313 epoch,   500 iteration, loss:0.058\n",
      "Checking accuracy on test set\n",
      "Got 698 / 1000 correct (69.80)\n",
      "313 epoch,  1000 iteration, loss:0.060\n",
      " num 312 epoch \n",
      "####### Training Loss #######\n",
      "[0.10713521]\n",
      "Checking accuracy on test set\n",
      "Got 689 / 1000 correct (68.90)\n",
      "314 epoch,   500 iteration, loss:0.055\n",
      "Checking accuracy on test set\n",
      "Got 716 / 1000 correct (71.60)\n",
      "314 epoch,  1000 iteration, loss:0.037\n",
      " num 313 epoch \n",
      "####### Training Loss #######\n",
      "[0.08763551]\n",
      "Checking accuracy on test set\n",
      "Got 695 / 1000 correct (69.50)\n",
      "315 epoch,   500 iteration, loss:0.050\n",
      "Checking accuracy on test set\n",
      "Got 708 / 1000 correct (70.80)\n",
      "315 epoch,  1000 iteration, loss:0.038\n",
      " num 314 epoch \n",
      "####### Training Loss #######\n",
      "[0.08637452]\n",
      "Checking accuracy on test set\n",
      "Got 680 / 1000 correct (68.00)\n",
      "316 epoch,   500 iteration, loss:0.042\n",
      "Checking accuracy on test set\n",
      "Got 688 / 1000 correct (68.80)\n",
      "316 epoch,  1000 iteration, loss:0.060\n",
      " num 315 epoch \n",
      "####### Training Loss #######\n",
      "[0.10297924]\n",
      "Checking accuracy on test set\n",
      "Got 687 / 1000 correct (68.70)\n",
      "317 epoch,   500 iteration, loss:0.046\n",
      "Checking accuracy on test set\n",
      "Got 677 / 1000 correct (67.70)\n",
      "317 epoch,  1000 iteration, loss:0.043\n",
      " num 316 epoch \n",
      "####### Training Loss #######\n",
      "[0.09026336]\n",
      "Checking accuracy on test set\n",
      "Got 696 / 1000 correct (69.60)\n",
      "318 epoch,   500 iteration, loss:0.047\n",
      "Checking accuracy on test set\n",
      "Got 708 / 1000 correct (70.80)\n",
      "318 epoch,  1000 iteration, loss:0.040\n",
      " num 317 epoch \n",
      "####### Training Loss #######\n",
      "[0.08358395]\n",
      "Checking accuracy on test set\n",
      "Got 634 / 1000 correct (63.40)\n",
      "319 epoch,   500 iteration, loss:0.042\n",
      "Checking accuracy on test set\n",
      "Got 689 / 1000 correct (68.90)\n",
      "319 epoch,  1000 iteration, loss:0.040\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " num 318 epoch \n",
      "####### Training Loss #######\n",
      "[0.0857946]\n",
      "Checking accuracy on test set\n",
      "Got 679 / 1000 correct (67.90)\n",
      "320 epoch,   500 iteration, loss:0.042\n",
      "Checking accuracy on test set\n",
      "Got 685 / 1000 correct (68.50)\n",
      "320 epoch,  1000 iteration, loss:0.039\n",
      " num 319 epoch \n",
      "####### Training Loss #######\n",
      "[0.07852017]\n",
      "Checking accuracy on test set\n",
      "Got 691 / 1000 correct (69.10)\n",
      "321 epoch,   500 iteration, loss:0.044\n",
      "Checking accuracy on test set\n",
      "Got 705 / 1000 correct (70.50)\n",
      "321 epoch,  1000 iteration, loss:0.039\n",
      " num 320 epoch \n",
      "####### Training Loss #######\n",
      "[0.0837071]\n",
      "Checking accuracy on test set\n",
      "Got 691 / 1000 correct (69.10)\n",
      "322 epoch,   500 iteration, loss:0.037\n",
      "Checking accuracy on test set\n",
      "Got 691 / 1000 correct (69.10)\n",
      "322 epoch,  1000 iteration, loss:0.046\n",
      " num 321 epoch \n",
      "####### Training Loss #######\n",
      "[0.08167249]\n",
      "Checking accuracy on test set\n",
      "Got 692 / 1000 correct (69.20)\n",
      "323 epoch,   500 iteration, loss:0.044\n",
      "Checking accuracy on test set\n",
      "Got 701 / 1000 correct (70.10)\n",
      "323 epoch,  1000 iteration, loss:0.057\n",
      " num 322 epoch \n",
      "####### Training Loss #######\n",
      "[0.09767531]\n",
      "Checking accuracy on test set\n",
      "Got 672 / 1000 correct (67.20)\n",
      "324 epoch,   500 iteration, loss:0.045\n",
      "Checking accuracy on test set\n",
      "Got 708 / 1000 correct (70.80)\n",
      "324 epoch,  1000 iteration, loss:0.041\n",
      " num 323 epoch \n",
      "####### Training Loss #######\n",
      "[0.09069385]\n",
      "Checking accuracy on test set\n",
      "Got 679 / 1000 correct (67.90)\n",
      "325 epoch,   500 iteration, loss:0.050\n",
      "Checking accuracy on test set\n",
      "Got 715 / 1000 correct (71.50)\n",
      "325 epoch,  1000 iteration, loss:0.044\n",
      " num 324 epoch \n",
      "####### Training Loss #######\n",
      "[0.09817267]\n",
      "Checking accuracy on test set\n",
      "Got 649 / 1000 correct (64.90)\n",
      "326 epoch,   500 iteration, loss:0.036\n",
      "Checking accuracy on test set\n",
      "Got 672 / 1000 correct (67.20)\n",
      "326 epoch,  1000 iteration, loss:0.043\n",
      " num 325 epoch \n",
      "####### Training Loss #######\n",
      "[0.08681755]\n",
      "Checking accuracy on test set\n",
      "Got 654 / 1000 correct (65.40)\n",
      "327 epoch,   500 iteration, loss:0.045\n",
      "Checking accuracy on test set\n",
      "Got 658 / 1000 correct (65.80)\n",
      "327 epoch,  1000 iteration, loss:0.045\n",
      " num 326 epoch \n",
      "####### Training Loss #######\n",
      "[0.09785725]\n",
      "Checking accuracy on test set\n",
      "Got 696 / 1000 correct (69.60)\n",
      "328 epoch,   500 iteration, loss:0.040\n",
      "Checking accuracy on test set\n",
      "Got 692 / 1000 correct (69.20)\n",
      "328 epoch,  1000 iteration, loss:0.033\n",
      " num 327 epoch \n",
      "####### Training Loss #######\n",
      "[0.07176473]\n",
      "Checking accuracy on test set\n",
      "Got 676 / 1000 correct (67.60)\n",
      "329 epoch,   500 iteration, loss:0.040\n",
      "Checking accuracy on test set\n",
      "Got 711 / 1000 correct (71.10)\n",
      "329 epoch,  1000 iteration, loss:0.038\n",
      " num 328 epoch \n",
      "####### Training Loss #######\n",
      "[0.07902228]\n",
      "Checking accuracy on test set\n",
      "Got 691 / 1000 correct (69.10)\n",
      "330 epoch,   500 iteration, loss:0.051\n",
      "Checking accuracy on test set\n",
      "Got 718 / 1000 correct (71.80)\n",
      "330 epoch,  1000 iteration, loss:0.047\n",
      " num 329 epoch \n",
      "####### Training Loss #######\n",
      "[0.09013701]\n",
      "Checking accuracy on test set\n",
      "Got 684 / 1000 correct (68.40)\n",
      "331 epoch,   500 iteration, loss:0.038\n",
      "Checking accuracy on test set\n",
      "Got 674 / 1000 correct (67.40)\n",
      "331 epoch,  1000 iteration, loss:0.044\n",
      " num 330 epoch \n",
      "####### Training Loss #######\n",
      "[0.08011734]\n",
      "Checking accuracy on test set\n",
      "Got 699 / 1000 correct (69.90)\n",
      "332 epoch,   500 iteration, loss:0.055\n",
      "Checking accuracy on test set\n",
      "Got 700 / 1000 correct (70.00)\n",
      "332 epoch,  1000 iteration, loss:0.037\n",
      " num 331 epoch \n",
      "####### Training Loss #######\n",
      "[0.09400257]\n",
      "Checking accuracy on test set\n",
      "Got 682 / 1000 correct (68.20)\n",
      "333 epoch,   500 iteration, loss:0.042\n",
      "Checking accuracy on test set\n",
      "Got 677 / 1000 correct (67.70)\n",
      "333 epoch,  1000 iteration, loss:0.048\n",
      " num 332 epoch \n",
      "####### Training Loss #######\n",
      "[0.09139314]\n",
      "Checking accuracy on test set\n",
      "Got 667 / 1000 correct (66.70)\n",
      "334 epoch,   500 iteration, loss:0.039\n",
      "Checking accuracy on test set\n",
      "Got 687 / 1000 correct (68.70)\n",
      "334 epoch,  1000 iteration, loss:0.054\n",
      " num 333 epoch \n",
      "####### Training Loss #######\n",
      "[0.08911032]\n",
      "Checking accuracy on test set\n",
      "Got 695 / 1000 correct (69.50)\n",
      "335 epoch,   500 iteration, loss:0.044\n",
      "Checking accuracy on test set\n",
      "Got 687 / 1000 correct (68.70)\n",
      "335 epoch,  1000 iteration, loss:0.039\n",
      " num 334 epoch \n",
      "####### Training Loss #######\n",
      "[0.07854687]\n",
      "Checking accuracy on test set\n",
      "Got 704 / 1000 correct (70.40)\n",
      "336 epoch,   500 iteration, loss:0.044\n",
      "Checking accuracy on test set\n",
      "Got 676 / 1000 correct (67.60)\n",
      "336 epoch,  1000 iteration, loss:0.053\n",
      " num 335 epoch \n",
      "####### Training Loss #######\n",
      "[0.09974678]\n",
      "Checking accuracy on test set\n",
      "Got 677 / 1000 correct (67.70)\n",
      "337 epoch,   500 iteration, loss:0.047\n",
      "Checking accuracy on test set\n",
      "Got 670 / 1000 correct (67.00)\n",
      "337 epoch,  1000 iteration, loss:0.042\n",
      " num 336 epoch \n",
      "####### Training Loss #######\n",
      "[0.0844389]\n",
      "Checking accuracy on test set\n",
      "Got 670 / 1000 correct (67.00)\n",
      "338 epoch,   500 iteration, loss:0.041\n",
      "Checking accuracy on test set\n",
      "Got 698 / 1000 correct (69.80)\n",
      "338 epoch,  1000 iteration, loss:0.039\n",
      " num 337 epoch \n",
      "####### Training Loss #######\n",
      "[0.07890325]\n",
      "Checking accuracy on test set\n",
      "Got 703 / 1000 correct (70.30)\n",
      "339 epoch,   500 iteration, loss:0.031\n",
      "Checking accuracy on test set\n",
      "Got 697 / 1000 correct (69.70)\n",
      "339 epoch,  1000 iteration, loss:0.055\n",
      " num 338 epoch \n",
      "####### Training Loss #######\n",
      "[0.08849864]\n",
      "Checking accuracy on test set\n",
      "Got 680 / 1000 correct (68.00)\n",
      "340 epoch,   500 iteration, loss:0.037\n",
      "Checking accuracy on test set\n",
      "Got 709 / 1000 correct (70.90)\n",
      "340 epoch,  1000 iteration, loss:0.040\n",
      " num 339 epoch \n",
      "####### Training Loss #######\n",
      "[0.07918566]\n",
      "Checking accuracy on test set\n",
      "Got 665 / 1000 correct (66.50)\n",
      "341 epoch,   500 iteration, loss:0.039\n",
      "Checking accuracy on test set\n",
      "Got 697 / 1000 correct (69.70)\n",
      "341 epoch,  1000 iteration, loss:0.044\n",
      " num 340 epoch \n",
      "####### Training Loss #######\n",
      "[0.08185477]\n",
      "Checking accuracy on test set\n",
      "Got 654 / 1000 correct (65.40)\n",
      "342 epoch,   500 iteration, loss:0.042\n",
      "Checking accuracy on test set\n",
      "Got 692 / 1000 correct (69.20)\n",
      "342 epoch,  1000 iteration, loss:0.042\n",
      " num 341 epoch \n",
      "####### Training Loss #######\n",
      "[0.09196689]\n",
      "Checking accuracy on test set\n",
      "Got 686 / 1000 correct (68.60)\n",
      "343 epoch,   500 iteration, loss:0.042\n",
      "Checking accuracy on test set\n",
      "Got 700 / 1000 correct (70.00)\n",
      "343 epoch,  1000 iteration, loss:0.040\n",
      " num 342 epoch \n",
      "####### Training Loss #######\n",
      "[0.08386802]\n",
      "Checking accuracy on test set\n",
      "Got 651 / 1000 correct (65.10)\n",
      "344 epoch,   500 iteration, loss:0.042\n",
      "Checking accuracy on test set\n",
      "Got 705 / 1000 correct (70.50)\n",
      "344 epoch,  1000 iteration, loss:0.033\n",
      " num 343 epoch \n",
      "####### Training Loss #######\n",
      "[0.07226487]\n",
      "Checking accuracy on test set\n",
      "Got 681 / 1000 correct (68.10)\n",
      "345 epoch,   500 iteration, loss:0.052\n",
      "Checking accuracy on test set\n",
      "Got 709 / 1000 correct (70.90)\n",
      "345 epoch,  1000 iteration, loss:0.038\n",
      " num 344 epoch \n",
      "####### Training Loss #######\n",
      "[0.08780348]\n",
      "Checking accuracy on test set\n",
      "Got 691 / 1000 correct (69.10)\n",
      "346 epoch,   500 iteration, loss:0.038\n",
      "Checking accuracy on test set\n",
      "Got 689 / 1000 correct (68.90)\n",
      "346 epoch,  1000 iteration, loss:0.036\n",
      " num 345 epoch \n",
      "####### Training Loss #######\n",
      "[0.08737674]\n",
      "Checking accuracy on test set\n",
      "Got 684 / 1000 correct (68.40)\n",
      "347 epoch,   500 iteration, loss:0.049\n",
      "Checking accuracy on test set\n",
      "Got 710 / 1000 correct (71.00)\n",
      "347 epoch,  1000 iteration, loss:0.044\n",
      " num 346 epoch \n",
      "####### Training Loss #######\n",
      "[0.09235793]\n",
      "Checking accuracy on test set\n",
      "Got 700 / 1000 correct (70.00)\n",
      "348 epoch,   500 iteration, loss:0.034\n",
      "Checking accuracy on test set\n",
      "Got 699 / 1000 correct (69.90)\n",
      "348 epoch,  1000 iteration, loss:0.046\n",
      " num 347 epoch \n",
      "####### Training Loss #######\n",
      "[0.08602452]\n",
      "Checking accuracy on test set\n",
      "Got 687 / 1000 correct (68.70)\n",
      "349 epoch,   500 iteration, loss:0.039\n",
      "Checking accuracy on test set\n",
      "Got 698 / 1000 correct (69.80)\n",
      "349 epoch,  1000 iteration, loss:0.038\n",
      " num 348 epoch \n",
      "####### Training Loss #######\n",
      "[0.08034272]\n",
      "Checking accuracy on test set\n",
      "Got 696 / 1000 correct (69.60)\n",
      "350 epoch,   500 iteration, loss:0.039\n",
      "Checking accuracy on test set\n",
      "Got 701 / 1000 correct (70.10)\n",
      "350 epoch,  1000 iteration, loss:0.041\n",
      " num 349 epoch \n",
      "####### Training Loss #######\n",
      "[0.08245489]\n",
      "finish training \n",
      "\n",
      "now begin saving datum for next step plotting\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_acc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-a1c6831ed252>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnet_name2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'ConvPool_CNN_C_Class2'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m convpool_cnn_c_class2 = running_model_B(run_num, convpool_cnn_c_class2, net_name2, \n\u001b[0;32m----> 4\u001b[0;31m                         lr, epoch, loaderB_train, loaderB_test)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-d1f6db3a4a2c>\u001b[0m in \u001b[0;36mrunning_model_B\u001b[0;34m(run_num, net, net_name, lr_list, epoch_list, loader_train, loader_test)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/test_acc.save'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m     \u001b[0mcPickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcPickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHIGHEST_PROTOCOL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_acc' is not defined"
     ]
    }
   ],
   "source": [
    "convpool_cnn_c_class2 = new_ALL_Conv.Stride_CNN_C()\n",
    "net_name2 = 'ConvPool_CNN_C_Class2'\n",
    "convpool_cnn_c_class2 = running_model_B(run_num, convpool_cnn_c_class2, net_name2, \n",
    "                        lr, epoch, loaderB_train, loaderB_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from six.moves import cPickle\n",
    "\n",
    "save_path = '../datum_for_plotting/run_num_10/ConvPool_CNN_C_Class2'\n",
    "f =open(save_path + '/train_loss.save' , 'rb')\n",
    "loss = cPickle.load(f )\n",
    "f.close()\n",
    "f =open(save_path + '/array_epoch_acc.save' , 'rb')\n",
    "acc_array = cPickle.load(f )\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'loss')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "max_epoch = 350\n",
    "itern_axis_train = np.array(np.linspace(1,max_epoch,num=max_epoch))\n",
    "\n",
    "plt.plot(itern_axis_train, loss,'-b.', label='Train')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = np.concatenate(acc_array)\n",
    "length = a.shape[0]\n",
    "a /= 100\n",
    "acc_axis_test = np.array(np.linspace(1,length, num=length))\n",
    "plt.plot(acc_axis_test, a.reshape(-1,1), '--r', label='Test')\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('loss')\n",
    "plt.savefig(save_path + '/validation_accuracy' + str(max_epoch) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
