{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path\n",
    "import errno\n",
    "import sys\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import sampler\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "if sys.version_info[0] == 2:\n",
    "    import cPickle as pickle\n",
    "else:\n",
    "    import pickle\n",
    "\n",
    "transform = T.Compose([\n",
    "                T.ToTensor(),\n",
    "                T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "            ])\n",
    "NUM_TRAIN=49000\n",
    "train_batch_size=4\n",
    "test_batch_size=4\n",
    "\n",
    "\n",
    "\n",
    "cifar10_train = torchvision.datasets.CIFAR10('./cs231n/datasets', train=True, download=True,\n",
    "                             transform=transform)\n",
    "loader_train = torch.utils.data.DataLoader(cifar10_train, batch_size=train_batch_size, \n",
    "                          sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN)))\n",
    "\n",
    "cifar10_val = torchvision.datasets.CIFAR10('./cs231n/datasets', train=True, download=True,\n",
    "                           transform=transform)\n",
    "loader_val = torch.utils.data.DataLoader(cifar10_val, batch_size=train_batch_size, \n",
    "                        sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN, 50000)))\n",
    "\n",
    "cifar10_test = torchvision.datasets.CIFAR10('./cs231n/datasets', train=False, download=True, \n",
    "                            transform=transform)\n",
    "loader_test = torch.utils.data.DataLoader(cifar10_test, batch_size=test_batch_size)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 32, 32, 3)\n",
      "(1000, 32, 32, 3)\n",
      "(5000, 32, 32, 3)\n",
      "(1000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "import torchvision.datasets as dset\n",
    "from PIL import Image    \n",
    "import os\n",
    "import os.path\n",
    "import errno\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "class CIFAR100(dset.CIFAR10):\n",
    "\n",
    "    base_folder = '.'\n",
    "    train_list = [\n",
    "        ['train', '16019d7e3df5f24257cddd939b257f8d'],\n",
    "    ]\n",
    "\n",
    "    test_list = [\n",
    "        ['test', 'f0ef6b0ae62326f3e7ffdfab6717acfc'],\n",
    "    ]\n",
    "    \n",
    "    def __init__(self, root, train=True,\n",
    "                 transform=None, target_transform=None,\n",
    "                 download=False):\n",
    "        self.root = os.path.expanduser(root)\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.train = train  # training set or test set\n",
    "\n",
    "        if download:\n",
    "            self.download()\n",
    "\n",
    "        if not self._check_integrity():\n",
    "            raise RuntimeError('Dataset not found or corrupted.' +\n",
    "                               ' You can use download=True to download it')\n",
    "\n",
    "        # now load the picked numpy arrays\n",
    "        self.test_data = []\n",
    "        if self.train:\n",
    "            self.train_data = []\n",
    "            self.train_labels = []\n",
    "            for fentry in self.train_list:\n",
    "                f = fentry[0]\n",
    "                file = os.path.join(self.root, self.base_folder, f)\n",
    "                fo = open(file, 'rb')\n",
    "                if sys.version_info[0] == 2:\n",
    "                    entry = pickle.load(fo)\n",
    "                else:\n",
    "                    entry = pickle.load(fo, encoding='latin1')\n",
    "                self.train_data.append(entry['data'])\n",
    "                if 'labels' in entry:\n",
    "                    self.train_labels += entry['labels']\n",
    "                else:\n",
    "                    self.train_labels += entry['fine_labels']\n",
    "                fo.close()\n",
    "\n",
    "            self.train_data = np.concatenate(self.train_data)\n",
    "            self.train_data = self.train_data.reshape((5000, 3, 32, 32))\n",
    "            self.train_data = self.train_data.transpose((0, 2, 3, 1))  # convert to HWC\n",
    "            print(self.train_data.shape)\n",
    "        else:\n",
    "            f = 'test'\n",
    "            file = os.path.join(self.root, self.base_folder, f)\n",
    "            fo = open(file, 'rb')\n",
    "            if sys.version_info[0] == 2:\n",
    "                entry = pickle.load(fo)\n",
    "            else:\n",
    "                entry = pickle.load(fo, encoding='latin1')\n",
    "            self.test_data = np.array(entry['data'])\n",
    "            if 'labels' in entry:\n",
    "                self.test_labels = entry['data']\n",
    "            else:\n",
    "                self.test_labels = entry['fine_labels']\n",
    "            fo.close()\n",
    "            self.test_data = self.test_data.reshape((1000, 3, 32, 32))\n",
    "            self.test_data = self.test_data.transpose((0, 2, 3, 1))  # convert to HWC\n",
    "            print(self.test_data.shape)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is index of the target class.\n",
    "        \"\"\"\n",
    "        if self.train:\n",
    "            img, target = self.train_data[index], self.train_labels[index]\n",
    "        else:\n",
    "            img, target = self.test_data[index], self.test_labels[index]\n",
    "\n",
    "        # doing this so that it is consistent with all other datasets\n",
    "        # to return a PIL Image\n",
    "        img = Image.fromarray(img.astype('uint8')).convert('RGB')\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.train:\n",
    "            return len(self.train_data)\n",
    "        else:\n",
    "            return len(self.test_data)                \n",
    "            \n",
    "    def _check_integrity(self):\n",
    "        return True\n",
    "    \n",
    "\n",
    "\n",
    "if sys.version_info[0] == 2:\n",
    "    import cPickle as pickle\n",
    "else:\n",
    "    import pickle\n",
    "\n",
    "transform = T.Compose([\n",
    "                T.ToTensor(),\n",
    "                T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "            ])\n",
    "\n",
    "A_train = CIFAR100('./class1', train=True, download=False,\n",
    "                             transform=transform)\n",
    "loaderA_train = torch.utils.data.DataLoader(A_train, batch_size=4)\n",
    "\n",
    "\n",
    "\n",
    "A_test = CIFAR100('./class1', train=False, download=False,\n",
    "                             transform=transform)\n",
    "loaderA_test = torch.utils.data.DataLoader(A_test, batch_size=4)\n",
    "\n",
    "\n",
    "B_train = CIFAR100('./class2', train=True, download=False,\n",
    "                             transform=transform)\n",
    "loaderB_train = torch.utils.data.DataLoader(B_train, batch_size=4)\n",
    "\n",
    "\n",
    "B_test = CIFAR100('./class2', train=False, download=False,\n",
    "                             transform=transform)\n",
    "loaderB_test = torch.utils.data.DataLoader(B_test, batch_size=4)\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right\n"
     ]
    }
   ],
   "source": [
    "class ALL_CNN_C(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super (ALL_CNN_C, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 96, kernel_size=3, stride=1, padding=1)\n",
    "        nn.init.kaiming_normal_(self.conv1.weight)\n",
    "        nn.init.constant_(self.conv1.bias, 0)\n",
    "        \n",
    "        self.dropout1 = nn.Dropout2d(0.2)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(96, 96, kernel_size=3, stride=1, padding=1)\n",
    "        nn.init.kaiming_normal_(self.conv2.weight)\n",
    "        nn.init.constant_(self.conv2.bias, 0)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(96, 96, kernel_size=3, stride=2, padding=0)\n",
    "        nn.init.kaiming_normal_(self.conv3.weight)\n",
    "        nn.init.constant_(self.conv3.bias, 0)\n",
    "        \n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(96, 192, kernel_size=3, stride=1, padding=1)\n",
    "        nn.init.kaiming_normal_(self.conv3.weight)\n",
    "        nn.init.constant_(self.conv3.bias, 0)\n",
    "        \n",
    "        self.conv5 = nn.Conv2d(192, 192, kernel_size=3, stride=1, padding=1)\n",
    "        nn.init.kaiming_normal_(self.conv5.weight)\n",
    "        nn.init.constant_(self.conv5.bias, 0)\n",
    "        \n",
    "        self.conv6 = nn.Conv2d(192, 192, kernel_size=3, stride=2, padding=0)\n",
    "        nn.init.kaiming_normal_(self.conv6.weight)\n",
    "        nn.init.constant_(self.conv6.bias, 0)\n",
    "        \n",
    "        self.dropout3 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.conv7 = nn.Conv2d(192, 192, kernel_size=3, padding=3)\n",
    "        nn.init.kaiming_normal_(self.conv7.weight)\n",
    "        nn.init.constant_(self.conv7.bias, 0)\n",
    "        \n",
    "        self.conv8 = nn.Conv2d(192, 192, kernel_size=1)\n",
    "        nn.init.kaiming_normal_(self.conv8.weight)\n",
    "        nn.init.constant_(self.conv8.bias, 0)\n",
    "        \n",
    "        self.conv9 = nn.Conv2d(192, self.num_classes, kernel_size=1)\n",
    "        nn.init.kaiming_normal_(self.conv9.weight)\n",
    "        nn.init.constant_(self.conv9.bias, 0)\n",
    "        \n",
    "        self.glb_avg = nn.AvgPool2d(6)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.conv1(x))\n",
    "        out = self.dropout1(out)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = F.relu(out)\n",
    "        \n",
    "        out = self.conv3(out)\n",
    "        out = F.relu(out)\n",
    "        \n",
    "        out = self.dropout2(out)\n",
    "        \n",
    "        out = self.conv4(out)\n",
    "        out = F.relu(out)\n",
    "        \n",
    "        out = self.conv5(out)\n",
    "        out = F.relu(out)\n",
    "        \n",
    "        out = self.conv6(out)\n",
    "        out = F.relu(out)\n",
    "        \n",
    "        out = self.dropout3(out)\n",
    "        \n",
    "        out = self.conv7(out)\n",
    "        out = F.relu(out)\n",
    "        \n",
    "        out = self.conv8(out)\n",
    "        out = F.relu(out)\n",
    "        \n",
    "        out = self.conv9(out)\n",
    "        out = F.relu(out)\n",
    "        \n",
    "        out = self.glb_avg(out)\n",
    "        out = out.view(-1, self.num_classes)\n",
    "        return out\n",
    "import new_ALL_Conv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from six.moves import cPickle\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import sampler\n",
    "\n",
    "USE_GPU = True\n",
    "dtype = torch.float32 # we will be using float throughout this tutorial\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "def check_accuracy(loader, model):\n",
    "    if loader.dataset.train:\n",
    "        print('Checking accuracy on validation set')\n",
    "    else:\n",
    "        print('Checking accuracy on test set')   \n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "            scores = model(x)\n",
    "            _, preds = scores.max(1)\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_samples += preds.size(0)\n",
    "        acc = float(num_correct) / num_samples *100\n",
    "        print('Got %d / %d correct (%.2f)' % (num_correct, num_samples,  acc))\n",
    "    return acc*100\n",
    "\n",
    "def running_model_B(run_num, net, net_name, lr_list, epoch_list, loader_train, \n",
    "                   loader_test):\n",
    "    train_batch_size = 4\n",
    "    test_batch_size = 4\n",
    "    \n",
    "\n",
    "    # Constant to control how frequently we print train loss\n",
    "    print_every = 100\n",
    "\n",
    "    print('using device:', device)\n",
    "    \n",
    "    #net = BaseNet_A()\n",
    "    net = net.to(device=device)\n",
    "    criterion = nn.CrossEntropyLoss()        \n",
    "        \n",
    "    lr_1, lr_2, lr_3, lr_4 = lr_list[0], lr_list[1], lr_list[2], lr_list[3]\n",
    "    weight_decay = 0.001\n",
    "\n",
    "    max_epoch = 350\n",
    "    display_interval = 500\n",
    "\n",
    "    train_size = 5000\n",
    "    test_size = 1000\n",
    "\n",
    "    num_train_batch = train_size/train_batch_size\n",
    "    num_test_batch = test_size/test_batch_size\n",
    "\n",
    "    train_loss = np.zeros((max_epoch,1))\n",
    "    val_acc = np.zeros((max_epoch,1))\n",
    "    #train_acc = np.zeros((max_epoch,1))\n",
    "    #test_loss = np.zeros((max_epoch,1))\n",
    "    #test_acc = np.zeros((max_epoch,1))\n",
    "\n",
    "    epoch_acc = [] # max_epoch x num\n",
    "    print(\"begin training\")\n",
    "    for epoch in range(max_epoch):\n",
    "        if(epoch<epoch_list[0]):\n",
    "            lr = lr_1\n",
    "        elif(epoch<epoch_list[1]):\n",
    "            lr = lr_2\n",
    "        elif(epoch<epoch_list[2]):\n",
    "            lr = lr_3\n",
    "        else:\n",
    "            lr = lr_4\n",
    "    \n",
    "        optimizer = optim.SGD( net.parameters(), lr=0.001,\n",
    "                              momentum=0.9, weight_decay=weight_decay)\n",
    "    \n",
    "        running_epoch_loss = 0.\n",
    "        running_loss_print = 0.\n",
    "        epoch_total_num = 0\n",
    "        correct_num = 0\n",
    "    \n",
    "        i_acc = []\n",
    "        #for i, data in enumerate(trainloader):\n",
    "        for i, data in enumerate(loader_train):\n",
    "            net.train()\n",
    "        \n",
    "            inputs_data, labels_data = data\n",
    "            inputs, labels = Variable(inputs_data), Variable(labels_data)\n",
    "            inputs = inputs.to(device=device, dtype=dtype)\n",
    "            labels = labels.to(device=device, dtype=torch.long)\n",
    "        \n",
    "    \n",
    "        \n",
    "        \n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "        \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "            running_epoch_loss += loss.item()\n",
    "            running_loss_print += loss.item()\n",
    "            if i%500 == 499: #net a, b, c 500 print once\n",
    "                \n",
    "                acc = check_accuracy(loader_test, net)\n",
    "                i_acc.append(acc)\n",
    "                print('%d epoch, %5d iteration, loss:%.3f' \n",
    "                      %(epoch+1, i+1, running_loss_print/500) )\n",
    "                running_loss_print = 0.\n",
    "            \n",
    "            #_, pred = torch.max(outputs, 1)\n",
    "            #epoch_total_num += labels.size(0)\n",
    "            #correct_num += (pred==labels).sum()\n",
    "        \n",
    "        \n",
    "        train_loss[epoch] = running_epoch_loss/num_train_batch\n",
    "        epoch_acc.append(i_acc)\n",
    "        \n",
    "        val_acc[epoch] = np.sum(epoch_acc[epoch])/49\n",
    "        #val_acc[epoch] = np.sum(epoch_acc[epoch])\n",
    "        #train_acc[epoch] = correct_num/epoch_total_num*100\n",
    "    \n",
    "    \n",
    "        # test accuracy and loss\n",
    "        '''\n",
    "        ts_runningloss_epoch = 0.\n",
    "        ts_correct = 0\n",
    "        ts_epoch_num = 0\n",
    "        for data in testloader:\n",
    "            inputs_data, labels_data = data\n",
    "            inputs, labels = Variable(inputs_data), Variable(labels_data)\n",
    "            inputs = inputs.to(device=device, dtype=dtype)\n",
    "            labels = labels.to(device=device, dtype=torch.long)\n",
    "        \n",
    "            net.eval()\n",
    "            ts_outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "        \n",
    "            ts_runningloss_epoch += loss.item()\n",
    "            _, ts_pred = torch.max(ts_outputs, 1)\n",
    "        \n",
    "            ts_epoch_num += labels.size(0)\n",
    "            ts_correct += (ts_pred==labels).sum()\n",
    "        test_loss[epoch] = ts_runningloss_epoch/num_test_batch\n",
    "        test_acc[epoch] = ts_correct/ts_epoch_num*100\n",
    "        '''\n",
    "        print(\" num %d epoch \" %epoch)\n",
    "        print(\"####### Training Loss #######\")\n",
    "        print(train_loss[epoch])\n",
    "        #print(\"####### Validation Accuracy #######\")\n",
    "        #print(val_acc[epoch])\n",
    "        #print(\"####### Training Accuracy #######\")\n",
    "        #print(train_acc[epoch])\n",
    "        #print(\"####### Testing Loss #######\")\n",
    "        #print(test_loss[epoch])\n",
    "        #print(\"####### Testing Accuracy #######\")\n",
    "        #print(test_acc[epoch])\n",
    "    \n",
    "    print('finish training \\n')\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "##################################################################################################\n",
    "    print('now begin saving datum for next step plotting')\n",
    "    \n",
    "    save_path = '../datum_for_plotting/run_num_' + str(run_num)+'/'+ net_name\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    f = open(save_path + '/train_loss.save', 'wb')\n",
    "    cPickle.dump(train_loss, f, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "    f.close()\n",
    "\n",
    "    f = open(save_path + '/val_acc.save', 'wb')\n",
    "    cPickle.dump(val_acc, f, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "    f.close()\n",
    "\n",
    "    f = open(save_path + '/epoch_acc.save', 'wb')\n",
    "    cPickle.dump(epoch_acc, f, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "    f.close()\n",
    "\n",
    "    array_epoch_acc = np.array(epoch_acc)\n",
    "    f = open(save_path + '/array_epoch_acc.save', 'wb')\n",
    "    cPickle.dump(array_epoch_acc, f, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "    f.close()\n",
    "\n",
    "    #f = open(save_path + '/test_acc.save', 'wb')\n",
    "    #cPickle.dump(test_acc, f, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "    #f.close()\n",
    "\n",
    "\n",
    "\n",
    "    torch.save(net, save_path+'/'+ net_name +'.pkl') # save whole net structure and params\n",
    "    torch.save(net.state_dict, save_path+'/'+ net_name +'_params.pkl') # only save model params\n",
    " \n",
    "    \n",
    "    \n",
    "    \n",
    "##################################################################################################    \n",
    "    print(\"now plotting accuracies and losses\")  \n",
    "    itern_axis_train = np.array(np.linspace(1,max_epoch,num=max_epoch))\n",
    "    itern_axis_test = np.array(np.linspace(1,max_epoch, num=max_epoch))\n",
    "\n",
    "    plt.plot(itern_axis_train, train_loss,'-b.', label='Train')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.savefig(save_path + '/train_loss' + str(max_epoch) + '.png')\n",
    "    \n",
    "    a = np.concatenate(array_epoch_acc)\n",
    "    length = a.shape[0]\n",
    "    plt.plot(length, a, '--r', label='Test')\n",
    "    plt.xlabel('iteration')\n",
    "    plt.ylabel('loss')\n",
    "    plt.savefig(save_path + '/testing_accuracy' + str(max_epoch) + '.png')\n",
    "    \n",
    "    return net\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n",
      "begin training\n",
      "Checking accuracy on test set\n",
      "Got 199 / 1000 correct (19.90)\n",
      "1 epoch,   500 iteration, loss:2.288\n",
      "Checking accuracy on test set\n",
      "Got 258 / 1000 correct (25.80)\n",
      "1 epoch,  1000 iteration, loss:2.245\n",
      " num 0 epoch \n",
      "####### Training Loss #######\n",
      "[2.2496775]\n",
      "Checking accuracy on test set\n",
      "Got 266 / 1000 correct (26.60)\n",
      "2 epoch,   500 iteration, loss:2.107\n",
      "Checking accuracy on test set\n",
      "Got 309 / 1000 correct (30.90)\n",
      "2 epoch,  1000 iteration, loss:2.067\n",
      " num 1 epoch \n",
      "####### Training Loss #######\n",
      "[2.0645971]\n",
      "Checking accuracy on test set\n",
      "Got 306 / 1000 correct (30.60)\n",
      "3 epoch,   500 iteration, loss:1.978\n",
      "Checking accuracy on test set\n",
      "Got 360 / 1000 correct (36.00)\n",
      "3 epoch,  1000 iteration, loss:1.931\n",
      " num 2 epoch \n",
      "####### Training Loss #######\n",
      "[1.93003855]\n",
      "Checking accuracy on test set\n",
      "Got 337 / 1000 correct (33.70)\n",
      "4 epoch,   500 iteration, loss:1.854\n",
      "Checking accuracy on test set\n",
      "Got 394 / 1000 correct (39.40)\n",
      "4 epoch,  1000 iteration, loss:1.827\n",
      " num 3 epoch \n",
      "####### Training Loss #######\n",
      "[1.81071767]\n",
      "Checking accuracy on test set\n",
      "Got 390 / 1000 correct (39.00)\n",
      "5 epoch,   500 iteration, loss:1.755\n",
      "Checking accuracy on test set\n",
      "Got 423 / 1000 correct (42.30)\n",
      "5 epoch,  1000 iteration, loss:1.726\n",
      " num 4 epoch \n",
      "####### Training Loss #######\n",
      "[1.71458024]\n",
      "Checking accuracy on test set\n",
      "Got 387 / 1000 correct (38.70)\n",
      "6 epoch,   500 iteration, loss:1.691\n",
      "Checking accuracy on test set\n",
      "Got 441 / 1000 correct (44.10)\n",
      "6 epoch,  1000 iteration, loss:1.642\n",
      " num 5 epoch \n",
      "####### Training Loss #######\n",
      "[1.63853075]\n",
      "Checking accuracy on test set\n",
      "Got 428 / 1000 correct (42.80)\n",
      "7 epoch,   500 iteration, loss:1.598\n",
      "Checking accuracy on test set\n",
      "Got 490 / 1000 correct (49.00)\n",
      "7 epoch,  1000 iteration, loss:1.544\n",
      " num 6 epoch \n",
      "####### Training Loss #######\n",
      "[1.5492568]\n",
      "Checking accuracy on test set\n",
      "Got 472 / 1000 correct (47.20)\n",
      "8 epoch,   500 iteration, loss:1.517\n",
      "Checking accuracy on test set\n",
      "Got 519 / 1000 correct (51.90)\n",
      "8 epoch,  1000 iteration, loss:1.468\n",
      " num 7 epoch \n",
      "####### Training Loss #######\n",
      "[1.47228902]\n",
      "Checking accuracy on test set\n",
      "Got 503 / 1000 correct (50.30)\n",
      "9 epoch,   500 iteration, loss:1.436\n",
      "Checking accuracy on test set\n",
      "Got 556 / 1000 correct (55.60)\n",
      "9 epoch,  1000 iteration, loss:1.412\n",
      " num 8 epoch \n",
      "####### Training Loss #######\n",
      "[1.40218396]\n",
      "Checking accuracy on test set\n",
      "Got 526 / 1000 correct (52.60)\n",
      "10 epoch,   500 iteration, loss:1.367\n",
      "Checking accuracy on test set\n",
      "Got 564 / 1000 correct (56.40)\n",
      "10 epoch,  1000 iteration, loss:1.362\n",
      " num 9 epoch \n",
      "####### Training Loss #######\n",
      "[1.33696882]\n",
      "Checking accuracy on test set\n",
      "Got 545 / 1000 correct (54.50)\n",
      "11 epoch,   500 iteration, loss:1.309\n",
      "Checking accuracy on test set\n",
      "Got 559 / 1000 correct (55.90)\n",
      "11 epoch,  1000 iteration, loss:1.300\n",
      " num 10 epoch \n",
      "####### Training Loss #######\n",
      "[1.27898749]\n",
      "Checking accuracy on test set\n",
      "Got 575 / 1000 correct (57.50)\n",
      "12 epoch,   500 iteration, loss:1.245\n",
      "Checking accuracy on test set\n",
      "Got 589 / 1000 correct (58.90)\n",
      "12 epoch,  1000 iteration, loss:1.253\n",
      " num 11 epoch \n",
      "####### Training Loss #######\n",
      "[1.22810606]\n",
      "Checking accuracy on test set\n",
      "Got 595 / 1000 correct (59.50)\n",
      "13 epoch,   500 iteration, loss:1.198\n",
      "Checking accuracy on test set\n",
      "Got 591 / 1000 correct (59.10)\n",
      "13 epoch,  1000 iteration, loss:1.203\n",
      " num 12 epoch \n",
      "####### Training Loss #######\n",
      "[1.18422996]\n",
      "Checking accuracy on test set\n",
      "Got 598 / 1000 correct (59.80)\n",
      "14 epoch,   500 iteration, loss:1.163\n",
      "Checking accuracy on test set\n",
      "Got 602 / 1000 correct (60.20)\n",
      "14 epoch,  1000 iteration, loss:1.162\n",
      " num 13 epoch \n",
      "####### Training Loss #######\n",
      "[1.14312079]\n",
      "Checking accuracy on test set\n",
      "Got 598 / 1000 correct (59.80)\n",
      "15 epoch,   500 iteration, loss:1.113\n",
      "Checking accuracy on test set\n",
      "Got 591 / 1000 correct (59.10)\n",
      "15 epoch,  1000 iteration, loss:1.122\n",
      " num 14 epoch \n",
      "####### Training Loss #######\n",
      "[1.09509476]\n",
      "Checking accuracy on test set\n",
      "Got 613 / 1000 correct (61.30)\n",
      "16 epoch,   500 iteration, loss:1.071\n",
      "Checking accuracy on test set\n",
      "Got 593 / 1000 correct (59.30)\n",
      "16 epoch,  1000 iteration, loss:1.089\n",
      " num 15 epoch \n",
      "####### Training Loss #######\n",
      "[1.05904313]\n",
      "Checking accuracy on test set\n",
      "Got 621 / 1000 correct (62.10)\n",
      "17 epoch,   500 iteration, loss:1.049\n",
      "Checking accuracy on test set\n",
      "Got 617 / 1000 correct (61.70)\n",
      "17 epoch,  1000 iteration, loss:1.047\n",
      " num 16 epoch \n",
      "####### Training Loss #######\n",
      "[1.03238995]\n",
      "Checking accuracy on test set\n",
      "Got 646 / 1000 correct (64.60)\n",
      "18 epoch,   500 iteration, loss:1.000\n",
      "Checking accuracy on test set\n",
      "Got 626 / 1000 correct (62.60)\n",
      "18 epoch,  1000 iteration, loss:1.017\n",
      " num 17 epoch \n",
      "####### Training Loss #######\n",
      "[0.98832289]\n",
      "Checking accuracy on test set\n",
      "Got 642 / 1000 correct (64.20)\n",
      "19 epoch,   500 iteration, loss:0.973\n",
      "Checking accuracy on test set\n",
      "Got 642 / 1000 correct (64.20)\n",
      "19 epoch,  1000 iteration, loss:0.966\n",
      " num 18 epoch \n",
      "####### Training Loss #######\n",
      "[0.95228978]\n",
      "Checking accuracy on test set\n",
      "Got 642 / 1000 correct (64.20)\n",
      "20 epoch,   500 iteration, loss:0.959\n",
      "Checking accuracy on test set\n",
      "Got 627 / 1000 correct (62.70)\n",
      "20 epoch,  1000 iteration, loss:0.922\n",
      " num 19 epoch \n",
      "####### Training Loss #######\n",
      "[0.92546441]\n",
      "Checking accuracy on test set\n",
      "Got 619 / 1000 correct (61.90)\n",
      "21 epoch,   500 iteration, loss:0.926\n",
      "Checking accuracy on test set\n",
      "Got 599 / 1000 correct (59.90)\n",
      "21 epoch,  1000 iteration, loss:0.902\n",
      " num 20 epoch \n",
      "####### Training Loss #######\n",
      "[0.89035369]\n",
      "Checking accuracy on test set\n",
      "Got 634 / 1000 correct (63.40)\n",
      "22 epoch,   500 iteration, loss:0.878\n",
      "Checking accuracy on test set\n",
      "Got 633 / 1000 correct (63.30)\n",
      "22 epoch,  1000 iteration, loss:0.887\n",
      " num 21 epoch \n",
      "####### Training Loss #######\n",
      "[0.86312028]\n",
      "Checking accuracy on test set\n",
      "Got 632 / 1000 correct (63.20)\n",
      "23 epoch,   500 iteration, loss:0.847\n",
      "Checking accuracy on test set\n",
      "Got 621 / 1000 correct (62.10)\n",
      "23 epoch,  1000 iteration, loss:0.851\n",
      " num 22 epoch \n",
      "####### Training Loss #######\n",
      "[0.8317183]\n",
      "Checking accuracy on test set\n",
      "Got 671 / 1000 correct (67.10)\n",
      "24 epoch,   500 iteration, loss:0.819\n",
      "Checking accuracy on test set\n",
      "Got 616 / 1000 correct (61.60)\n",
      "24 epoch,  1000 iteration, loss:0.828\n",
      " num 23 epoch \n",
      "####### Training Loss #######\n",
      "[0.81220198]\n",
      "Checking accuracy on test set\n",
      "Got 663 / 1000 correct (66.30)\n",
      "25 epoch,   500 iteration, loss:0.798\n",
      "Checking accuracy on test set\n",
      "Got 620 / 1000 correct (62.00)\n",
      "25 epoch,  1000 iteration, loss:0.776\n",
      " num 24 epoch \n",
      "####### Training Loss #######\n",
      "[0.77215593]\n",
      "Checking accuracy on test set\n",
      "Got 658 / 1000 correct (65.80)\n",
      "26 epoch,   500 iteration, loss:0.760\n",
      "Checking accuracy on test set\n",
      "Got 659 / 1000 correct (65.90)\n",
      "26 epoch,  1000 iteration, loss:0.766\n",
      " num 25 epoch \n",
      "####### Training Loss #######\n",
      "[0.74429197]\n",
      "Checking accuracy on test set\n",
      "Got 652 / 1000 correct (65.20)\n",
      "27 epoch,   500 iteration, loss:0.710\n",
      "Checking accuracy on test set\n",
      "Got 654 / 1000 correct (65.40)\n",
      "27 epoch,  1000 iteration, loss:0.733\n",
      " num 26 epoch \n",
      "####### Training Loss #######\n",
      "[0.70690417]\n",
      "Checking accuracy on test set\n",
      "Got 662 / 1000 correct (66.20)\n",
      "28 epoch,   500 iteration, loss:0.721\n",
      "Checking accuracy on test set\n",
      "Got 648 / 1000 correct (64.80)\n",
      "28 epoch,  1000 iteration, loss:0.698\n",
      " num 27 epoch \n",
      "####### Training Loss #######\n",
      "[0.68618895]\n",
      "Checking accuracy on test set\n",
      "Got 655 / 1000 correct (65.50)\n",
      "29 epoch,   500 iteration, loss:0.669\n",
      "Checking accuracy on test set\n",
      "Got 644 / 1000 correct (64.40)\n",
      "29 epoch,  1000 iteration, loss:0.667\n",
      " num 28 epoch \n",
      "####### Training Loss #######\n",
      "[0.65281723]\n",
      "Checking accuracy on test set\n",
      "Got 685 / 1000 correct (68.50)\n",
      "30 epoch,   500 iteration, loss:0.667\n",
      "Checking accuracy on test set\n",
      "Got 663 / 1000 correct (66.30)\n",
      "30 epoch,  1000 iteration, loss:0.672\n",
      " num 29 epoch \n",
      "####### Training Loss #######\n",
      "[0.65398597]\n",
      "Checking accuracy on test set\n",
      "Got 642 / 1000 correct (64.20)\n",
      "31 epoch,   500 iteration, loss:0.624\n",
      "Checking accuracy on test set\n",
      "Got 634 / 1000 correct (63.40)\n",
      "31 epoch,  1000 iteration, loss:0.611\n",
      " num 30 epoch \n",
      "####### Training Loss #######\n",
      "[0.606066]\n",
      "Checking accuracy on test set\n",
      "Got 657 / 1000 correct (65.70)\n",
      "32 epoch,   500 iteration, loss:0.632\n",
      "Checking accuracy on test set\n",
      "Got 649 / 1000 correct (64.90)\n",
      "32 epoch,  1000 iteration, loss:0.583\n",
      " num 31 epoch \n",
      "####### Training Loss #######\n",
      "[0.59545037]\n",
      "Checking accuracy on test set\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 667 / 1000 correct (66.70)\n",
      "33 epoch,   500 iteration, loss:0.599\n",
      "Checking accuracy on test set\n",
      "Got 668 / 1000 correct (66.80)\n",
      "33 epoch,  1000 iteration, loss:0.575\n",
      " num 32 epoch \n",
      "####### Training Loss #######\n",
      "[0.56951155]\n",
      "Checking accuracy on test set\n",
      "Got 696 / 1000 correct (69.60)\n",
      "34 epoch,   500 iteration, loss:0.577\n",
      "Checking accuracy on test set\n",
      "Got 686 / 1000 correct (68.60)\n",
      "34 epoch,  1000 iteration, loss:0.578\n",
      " num 33 epoch \n",
      "####### Training Loss #######\n",
      "[0.5596024]\n",
      "Checking accuracy on test set\n",
      "Got 674 / 1000 correct (67.40)\n",
      "35 epoch,   500 iteration, loss:0.553\n",
      "Checking accuracy on test set\n",
      "Got 673 / 1000 correct (67.30)\n",
      "35 epoch,  1000 iteration, loss:0.535\n",
      " num 34 epoch \n",
      "####### Training Loss #######\n",
      "[0.53640391]\n",
      "Checking accuracy on test set\n",
      "Got 672 / 1000 correct (67.20)\n",
      "36 epoch,   500 iteration, loss:0.529\n",
      "Checking accuracy on test set\n",
      "Got 674 / 1000 correct (67.40)\n",
      "36 epoch,  1000 iteration, loss:0.511\n",
      " num 35 epoch \n",
      "####### Training Loss #######\n",
      "[0.5012605]\n",
      "Checking accuracy on test set\n",
      "Got 659 / 1000 correct (65.90)\n",
      "37 epoch,   500 iteration, loss:0.491\n",
      "Checking accuracy on test set\n",
      "Got 659 / 1000 correct (65.90)\n",
      "37 epoch,  1000 iteration, loss:0.509\n",
      " num 36 epoch \n",
      "####### Training Loss #######\n",
      "[0.48631855]\n",
      "Checking accuracy on test set\n",
      "Got 668 / 1000 correct (66.80)\n",
      "38 epoch,   500 iteration, loss:0.475\n",
      "Checking accuracy on test set\n",
      "Got 605 / 1000 correct (60.50)\n",
      "38 epoch,  1000 iteration, loss:0.497\n",
      " num 37 epoch \n",
      "####### Training Loss #######\n",
      "[0.48216878]\n",
      "Checking accuracy on test set\n",
      "Got 701 / 1000 correct (70.10)\n",
      "39 epoch,   500 iteration, loss:0.456\n",
      "Checking accuracy on test set\n",
      "Got 663 / 1000 correct (66.30)\n",
      "39 epoch,  1000 iteration, loss:0.446\n",
      " num 38 epoch \n",
      "####### Training Loss #######\n",
      "[0.45300309]\n",
      "Checking accuracy on test set\n",
      "Got 672 / 1000 correct (67.20)\n",
      "40 epoch,   500 iteration, loss:0.410\n",
      "Checking accuracy on test set\n",
      "Got 669 / 1000 correct (66.90)\n",
      "40 epoch,  1000 iteration, loss:0.445\n",
      " num 39 epoch \n",
      "####### Training Loss #######\n",
      "[0.41962002]\n",
      "Checking accuracy on test set\n",
      "Got 665 / 1000 correct (66.50)\n",
      "41 epoch,   500 iteration, loss:0.409\n",
      "Checking accuracy on test set\n",
      "Got 692 / 1000 correct (69.20)\n",
      "41 epoch,  1000 iteration, loss:0.449\n",
      " num 40 epoch \n",
      "####### Training Loss #######\n",
      "[0.43196119]\n",
      "Checking accuracy on test set\n",
      "Got 696 / 1000 correct (69.60)\n",
      "42 epoch,   500 iteration, loss:0.424\n",
      "Checking accuracy on test set\n",
      "Got 681 / 1000 correct (68.10)\n",
      "42 epoch,  1000 iteration, loss:0.426\n",
      " num 41 epoch \n",
      "####### Training Loss #######\n",
      "[0.42302577]\n",
      "Checking accuracy on test set\n",
      "Got 688 / 1000 correct (68.80)\n",
      "43 epoch,   500 iteration, loss:0.367\n",
      "Checking accuracy on test set\n",
      "Got 674 / 1000 correct (67.40)\n",
      "43 epoch,  1000 iteration, loss:0.397\n",
      " num 42 epoch \n",
      "####### Training Loss #######\n",
      "[0.38119271]\n",
      "Checking accuracy on test set\n",
      "Got 661 / 1000 correct (66.10)\n",
      "44 epoch,   500 iteration, loss:0.372\n",
      "Checking accuracy on test set\n",
      "Got 695 / 1000 correct (69.50)\n",
      "44 epoch,  1000 iteration, loss:0.392\n",
      " num 43 epoch \n",
      "####### Training Loss #######\n",
      "[0.37644187]\n",
      "Checking accuracy on test set\n",
      "Got 699 / 1000 correct (69.90)\n",
      "45 epoch,   500 iteration, loss:0.374\n",
      "Checking accuracy on test set\n",
      "Got 673 / 1000 correct (67.30)\n",
      "45 epoch,  1000 iteration, loss:0.376\n",
      " num 44 epoch \n",
      "####### Training Loss #######\n",
      "[0.36204615]\n",
      "Checking accuracy on test set\n",
      "Got 664 / 1000 correct (66.40)\n",
      "46 epoch,   500 iteration, loss:0.335\n",
      "Checking accuracy on test set\n",
      "Got 680 / 1000 correct (68.00)\n",
      "46 epoch,  1000 iteration, loss:0.373\n",
      " num 45 epoch \n",
      "####### Training Loss #######\n",
      "[0.35078613]\n",
      "Checking accuracy on test set\n",
      "Got 667 / 1000 correct (66.70)\n",
      "47 epoch,   500 iteration, loss:0.330\n",
      "Checking accuracy on test set\n",
      "Got 628 / 1000 correct (62.80)\n",
      "47 epoch,  1000 iteration, loss:0.362\n",
      " num 46 epoch \n",
      "####### Training Loss #######\n",
      "[0.34503776]\n",
      "Checking accuracy on test set\n",
      "Got 689 / 1000 correct (68.90)\n",
      "48 epoch,   500 iteration, loss:0.334\n",
      "Checking accuracy on test set\n",
      "Got 685 / 1000 correct (68.50)\n",
      "48 epoch,  1000 iteration, loss:0.342\n",
      " num 47 epoch \n",
      "####### Training Loss #######\n",
      "[0.32850675]\n",
      "Checking accuracy on test set\n",
      "Got 691 / 1000 correct (69.10)\n",
      "49 epoch,   500 iteration, loss:0.319\n",
      "Checking accuracy on test set\n",
      "Got 670 / 1000 correct (67.00)\n",
      "49 epoch,  1000 iteration, loss:0.310\n",
      " num 48 epoch \n",
      "####### Training Loss #######\n",
      "[0.3156192]\n",
      "Checking accuracy on test set\n",
      "Got 683 / 1000 correct (68.30)\n",
      "50 epoch,   500 iteration, loss:0.313\n",
      "Checking accuracy on test set\n",
      "Got 668 / 1000 correct (66.80)\n",
      "50 epoch,  1000 iteration, loss:0.315\n",
      " num 49 epoch \n",
      "####### Training Loss #######\n",
      "[0.31550896]\n",
      "Checking accuracy on test set\n",
      "Got 700 / 1000 correct (70.00)\n",
      "51 epoch,   500 iteration, loss:0.348\n",
      "Checking accuracy on test set\n",
      "Got 675 / 1000 correct (67.50)\n",
      "51 epoch,  1000 iteration, loss:0.286\n",
      " num 50 epoch \n",
      "####### Training Loss #######\n",
      "[0.31314365]\n",
      "Checking accuracy on test set\n",
      "Got 676 / 1000 correct (67.60)\n",
      "52 epoch,   500 iteration, loss:0.284\n",
      "Checking accuracy on test set\n",
      "Got 666 / 1000 correct (66.60)\n",
      "52 epoch,  1000 iteration, loss:0.281\n",
      " num 51 epoch \n",
      "####### Training Loss #######\n",
      "[0.2775606]\n",
      "Checking accuracy on test set\n",
      "Got 662 / 1000 correct (66.20)\n",
      "53 epoch,   500 iteration, loss:0.274\n",
      "Checking accuracy on test set\n",
      "Got 676 / 1000 correct (67.60)\n",
      "53 epoch,  1000 iteration, loss:0.319\n",
      " num 52 epoch \n",
      "####### Training Loss #######\n",
      "[0.29304823]\n",
      "Checking accuracy on test set\n",
      "Got 679 / 1000 correct (67.90)\n",
      "54 epoch,   500 iteration, loss:0.265\n",
      "Checking accuracy on test set\n",
      "Got 670 / 1000 correct (67.00)\n",
      "54 epoch,  1000 iteration, loss:0.301\n",
      " num 53 epoch \n",
      "####### Training Loss #######\n",
      "[0.272638]\n",
      "Checking accuracy on test set\n",
      "Got 687 / 1000 correct (68.70)\n",
      "55 epoch,   500 iteration, loss:0.289\n",
      "Checking accuracy on test set\n",
      "Got 677 / 1000 correct (67.70)\n",
      "55 epoch,  1000 iteration, loss:0.270\n",
      " num 54 epoch \n",
      "####### Training Loss #######\n",
      "[0.2709485]\n",
      "Checking accuracy on test set\n",
      "Got 673 / 1000 correct (67.30)\n",
      "56 epoch,   500 iteration, loss:0.252\n",
      "Checking accuracy on test set\n",
      "Got 664 / 1000 correct (66.40)\n",
      "56 epoch,  1000 iteration, loss:0.270\n",
      " num 55 epoch \n",
      "####### Training Loss #######\n",
      "[0.2570098]\n",
      "Checking accuracy on test set\n",
      "Got 698 / 1000 correct (69.80)\n",
      "57 epoch,   500 iteration, loss:0.229\n",
      "Checking accuracy on test set\n",
      "Got 674 / 1000 correct (67.40)\n",
      "57 epoch,  1000 iteration, loss:0.215\n",
      " num 56 epoch \n",
      "####### Training Loss #######\n",
      "[0.23817417]\n",
      "Checking accuracy on test set\n",
      "Got 676 / 1000 correct (67.60)\n",
      "58 epoch,   500 iteration, loss:0.249\n",
      "Checking accuracy on test set\n",
      "Got 694 / 1000 correct (69.40)\n",
      "58 epoch,  1000 iteration, loss:0.276\n",
      " num 57 epoch \n",
      "####### Training Loss #######\n",
      "[0.26331441]\n",
      "Checking accuracy on test set\n",
      "Got 692 / 1000 correct (69.20)\n",
      "59 epoch,   500 iteration, loss:0.215\n",
      "Checking accuracy on test set\n",
      "Got 692 / 1000 correct (69.20)\n",
      "59 epoch,  1000 iteration, loss:0.262\n",
      " num 58 epoch \n",
      "####### Training Loss #######\n",
      "[0.23775049]\n",
      "Checking accuracy on test set\n",
      "Got 672 / 1000 correct (67.20)\n",
      "60 epoch,   500 iteration, loss:0.243\n",
      "Checking accuracy on test set\n",
      "Got 680 / 1000 correct (68.00)\n",
      "60 epoch,  1000 iteration, loss:0.242\n",
      " num 59 epoch \n",
      "####### Training Loss #######\n",
      "[0.23841391]\n",
      "Checking accuracy on test set\n",
      "Got 684 / 1000 correct (68.40)\n",
      "61 epoch,   500 iteration, loss:0.211\n",
      "Checking accuracy on test set\n",
      "Got 665 / 1000 correct (66.50)\n",
      "61 epoch,  1000 iteration, loss:0.209\n",
      " num 60 epoch \n",
      "####### Training Loss #######\n",
      "[0.21287137]\n",
      "Checking accuracy on test set\n",
      "Got 672 / 1000 correct (67.20)\n",
      "62 epoch,   500 iteration, loss:0.202\n",
      "Checking accuracy on test set\n",
      "Got 675 / 1000 correct (67.50)\n",
      "62 epoch,  1000 iteration, loss:0.206\n",
      " num 61 epoch \n",
      "####### Training Loss #######\n",
      "[0.20676604]\n",
      "Checking accuracy on test set\n",
      "Got 655 / 1000 correct (65.50)\n",
      "63 epoch,   500 iteration, loss:0.225\n",
      "Checking accuracy on test set\n",
      "Got 671 / 1000 correct (67.10)\n",
      "63 epoch,  1000 iteration, loss:0.221\n",
      " num 62 epoch \n",
      "####### Training Loss #######\n",
      "[0.22097526]\n",
      "Checking accuracy on test set\n",
      "Got 681 / 1000 correct (68.10)\n",
      "64 epoch,   500 iteration, loss:0.206\n",
      "Checking accuracy on test set\n",
      "Got 660 / 1000 correct (66.00)\n",
      "64 epoch,  1000 iteration, loss:0.170\n",
      " num 63 epoch \n",
      "####### Training Loss #######\n",
      "[0.1898247]\n",
      "Checking accuracy on test set\n",
      "Got 688 / 1000 correct (68.80)\n",
      "65 epoch,   500 iteration, loss:0.192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking accuracy on test set\n",
      "Got 695 / 1000 correct (69.50)\n",
      "65 epoch,  1000 iteration, loss:0.202\n",
      " num 64 epoch \n",
      "####### Training Loss #######\n",
      "[0.19070092]\n",
      "Checking accuracy on test set\n",
      "Got 677 / 1000 correct (67.70)\n",
      "66 epoch,   500 iteration, loss:0.176\n",
      "Checking accuracy on test set\n",
      "Got 697 / 1000 correct (69.70)\n",
      "66 epoch,  1000 iteration, loss:0.199\n",
      " num 65 epoch \n",
      "####### Training Loss #######\n",
      "[0.19159108]\n",
      "Checking accuracy on test set\n",
      "Got 693 / 1000 correct (69.30)\n",
      "67 epoch,   500 iteration, loss:0.211\n",
      "Checking accuracy on test set\n",
      "Got 660 / 1000 correct (66.00)\n",
      "67 epoch,  1000 iteration, loss:0.189\n",
      " num 66 epoch \n",
      "####### Training Loss #######\n",
      "[0.2094313]\n",
      "Checking accuracy on test set\n",
      "Got 697 / 1000 correct (69.70)\n",
      "68 epoch,   500 iteration, loss:0.219\n",
      "Checking accuracy on test set\n",
      "Got 696 / 1000 correct (69.60)\n",
      "68 epoch,  1000 iteration, loss:0.186\n",
      " num 67 epoch \n",
      "####### Training Loss #######\n",
      "[0.19054151]\n",
      "Checking accuracy on test set\n",
      "Got 678 / 1000 correct (67.80)\n",
      "69 epoch,   500 iteration, loss:0.178\n",
      "Checking accuracy on test set\n",
      "Got 700 / 1000 correct (70.00)\n",
      "69 epoch,  1000 iteration, loss:0.199\n",
      " num 68 epoch \n",
      "####### Training Loss #######\n",
      "[0.19187802]\n",
      "Checking accuracy on test set\n",
      "Got 682 / 1000 correct (68.20)\n",
      "70 epoch,   500 iteration, loss:0.193\n",
      "Checking accuracy on test set\n",
      "Got 680 / 1000 correct (68.00)\n",
      "70 epoch,  1000 iteration, loss:0.142\n",
      " num 69 epoch \n",
      "####### Training Loss #######\n",
      "[0.1659308]\n",
      "Checking accuracy on test set\n",
      "Got 674 / 1000 correct (67.40)\n",
      "71 epoch,   500 iteration, loss:0.173\n",
      "Checking accuracy on test set\n",
      "Got 668 / 1000 correct (66.80)\n",
      "71 epoch,  1000 iteration, loss:0.175\n",
      " num 70 epoch \n",
      "####### Training Loss #######\n",
      "[0.16899179]\n",
      "Checking accuracy on test set\n",
      "Got 684 / 1000 correct (68.40)\n",
      "72 epoch,   500 iteration, loss:0.153\n",
      "Checking accuracy on test set\n",
      "Got 703 / 1000 correct (70.30)\n",
      "72 epoch,  1000 iteration, loss:0.172\n",
      " num 71 epoch \n",
      "####### Training Loss #######\n",
      "[0.17277639]\n",
      "Checking accuracy on test set\n",
      "Got 686 / 1000 correct (68.60)\n",
      "73 epoch,   500 iteration, loss:0.148\n",
      "Checking accuracy on test set\n",
      "Got 689 / 1000 correct (68.90)\n",
      "73 epoch,  1000 iteration, loss:0.178\n",
      " num 72 epoch \n",
      "####### Training Loss #######\n",
      "[0.17013295]\n",
      "Checking accuracy on test set\n",
      "Got 695 / 1000 correct (69.50)\n",
      "74 epoch,   500 iteration, loss:0.160\n",
      "Checking accuracy on test set\n",
      "Got 682 / 1000 correct (68.20)\n",
      "74 epoch,  1000 iteration, loss:0.170\n",
      " num 73 epoch \n",
      "####### Training Loss #######\n",
      "[0.17347909]\n",
      "Checking accuracy on test set\n",
      "Got 694 / 1000 correct (69.40)\n",
      "75 epoch,   500 iteration, loss:0.161\n",
      "Checking accuracy on test set\n",
      "Got 661 / 1000 correct (66.10)\n",
      "75 epoch,  1000 iteration, loss:0.161\n",
      " num 74 epoch \n",
      "####### Training Loss #######\n",
      "[0.16589601]\n",
      "Checking accuracy on test set\n",
      "Got 699 / 1000 correct (69.90)\n",
      "76 epoch,   500 iteration, loss:0.160\n",
      "Checking accuracy on test set\n",
      "Got 633 / 1000 correct (63.30)\n",
      "76 epoch,  1000 iteration, loss:0.160\n",
      " num 75 epoch \n",
      "####### Training Loss #######\n",
      "[0.16064738]\n",
      "Checking accuracy on test set\n",
      "Got 676 / 1000 correct (67.60)\n",
      "77 epoch,   500 iteration, loss:0.190\n",
      "Checking accuracy on test set\n",
      "Got 695 / 1000 correct (69.50)\n",
      "77 epoch,  1000 iteration, loss:0.143\n",
      " num 76 epoch \n",
      "####### Training Loss #######\n",
      "[0.17594436]\n",
      "Checking accuracy on test set\n",
      "Got 684 / 1000 correct (68.40)\n",
      "78 epoch,   500 iteration, loss:0.182\n",
      "Checking accuracy on test set\n",
      "Got 689 / 1000 correct (68.90)\n",
      "78 epoch,  1000 iteration, loss:0.153\n",
      " num 77 epoch \n",
      "####### Training Loss #######\n",
      "[0.16434809]\n",
      "Checking accuracy on test set\n",
      "Got 694 / 1000 correct (69.40)\n",
      "79 epoch,   500 iteration, loss:0.152\n",
      "Checking accuracy on test set\n",
      "Got 685 / 1000 correct (68.50)\n",
      "79 epoch,  1000 iteration, loss:0.143\n",
      " num 78 epoch \n",
      "####### Training Loss #######\n",
      "[0.14960464]\n",
      "Checking accuracy on test set\n",
      "Got 694 / 1000 correct (69.40)\n",
      "80 epoch,   500 iteration, loss:0.134\n",
      "Checking accuracy on test set\n",
      "Got 711 / 1000 correct (71.10)\n",
      "80 epoch,  1000 iteration, loss:0.131\n",
      " num 79 epoch \n",
      "####### Training Loss #######\n",
      "[0.13767614]\n",
      "Checking accuracy on test set\n",
      "Got 696 / 1000 correct (69.60)\n",
      "81 epoch,   500 iteration, loss:0.185\n",
      "Checking accuracy on test set\n",
      "Got 700 / 1000 correct (70.00)\n",
      "81 epoch,  1000 iteration, loss:0.169\n",
      " num 80 epoch \n",
      "####### Training Loss #######\n",
      "[0.18034132]\n",
      "Checking accuracy on test set\n",
      "Got 689 / 1000 correct (68.90)\n",
      "82 epoch,   500 iteration, loss:0.149\n",
      "Checking accuracy on test set\n",
      "Got 708 / 1000 correct (70.80)\n",
      "82 epoch,  1000 iteration, loss:0.120\n",
      " num 81 epoch \n",
      "####### Training Loss #######\n",
      "[0.13345986]\n",
      "Checking accuracy on test set\n",
      "Got 656 / 1000 correct (65.60)\n",
      "83 epoch,   500 iteration, loss:0.120\n",
      "Checking accuracy on test set\n",
      "Got 692 / 1000 correct (69.20)\n",
      "83 epoch,  1000 iteration, loss:0.147\n",
      " num 82 epoch \n",
      "####### Training Loss #######\n",
      "[0.14368442]\n",
      "Checking accuracy on test set\n",
      "Got 691 / 1000 correct (69.10)\n",
      "84 epoch,   500 iteration, loss:0.117\n",
      "Checking accuracy on test set\n",
      "Got 708 / 1000 correct (70.80)\n",
      "84 epoch,  1000 iteration, loss:0.126\n",
      " num 83 epoch \n",
      "####### Training Loss #######\n",
      "[0.12153244]\n",
      "Checking accuracy on test set\n",
      "Got 695 / 1000 correct (69.50)\n",
      "85 epoch,   500 iteration, loss:0.157\n",
      "Checking accuracy on test set\n",
      "Got 683 / 1000 correct (68.30)\n",
      "85 epoch,  1000 iteration, loss:0.149\n",
      " num 84 epoch \n",
      "####### Training Loss #######\n",
      "[0.15113907]\n",
      "Checking accuracy on test set\n",
      "Got 697 / 1000 correct (69.70)\n",
      "86 epoch,   500 iteration, loss:0.111\n",
      "Checking accuracy on test set\n",
      "Got 686 / 1000 correct (68.60)\n",
      "86 epoch,  1000 iteration, loss:0.147\n",
      " num 85 epoch \n",
      "####### Training Loss #######\n",
      "[0.13445862]\n",
      "Checking accuracy on test set\n",
      "Got 683 / 1000 correct (68.30)\n",
      "87 epoch,   500 iteration, loss:0.114\n",
      "Checking accuracy on test set\n",
      "Got 686 / 1000 correct (68.60)\n",
      "87 epoch,  1000 iteration, loss:0.126\n",
      " num 86 epoch \n",
      "####### Training Loss #######\n",
      "[0.11155656]\n",
      "Checking accuracy on test set\n",
      "Got 681 / 1000 correct (68.10)\n",
      "88 epoch,   500 iteration, loss:0.094\n",
      "Checking accuracy on test set\n",
      "Got 646 / 1000 correct (64.60)\n",
      "88 epoch,  1000 iteration, loss:0.105\n",
      " num 87 epoch \n",
      "####### Training Loss #######\n",
      "[0.10596888]\n",
      "Checking accuracy on test set\n",
      "Got 677 / 1000 correct (67.70)\n",
      "89 epoch,   500 iteration, loss:0.103\n",
      "Checking accuracy on test set\n",
      "Got 683 / 1000 correct (68.30)\n",
      "89 epoch,  1000 iteration, loss:0.132\n",
      " num 88 epoch \n",
      "####### Training Loss #######\n",
      "[0.11996387]\n",
      "Checking accuracy on test set\n",
      "Got 690 / 1000 correct (69.00)\n",
      "90 epoch,   500 iteration, loss:0.134\n",
      "Checking accuracy on test set\n",
      "Got 690 / 1000 correct (69.00)\n",
      "90 epoch,  1000 iteration, loss:0.111\n",
      " num 89 epoch \n",
      "####### Training Loss #######\n",
      "[0.11913794]\n",
      "Checking accuracy on test set\n",
      "Got 682 / 1000 correct (68.20)\n",
      "91 epoch,   500 iteration, loss:0.110\n",
      "Checking accuracy on test set\n",
      "Got 685 / 1000 correct (68.50)\n",
      "91 epoch,  1000 iteration, loss:0.137\n",
      " num 90 epoch \n",
      "####### Training Loss #######\n",
      "[0.12166978]\n",
      "Checking accuracy on test set\n",
      "Got 677 / 1000 correct (67.70)\n",
      "92 epoch,   500 iteration, loss:0.109\n",
      "Checking accuracy on test set\n",
      "Got 696 / 1000 correct (69.60)\n",
      "92 epoch,  1000 iteration, loss:0.106\n",
      " num 91 epoch \n",
      "####### Training Loss #######\n",
      "[0.10821854]\n",
      "Checking accuracy on test set\n",
      "Got 690 / 1000 correct (69.00)\n",
      "93 epoch,   500 iteration, loss:0.112\n",
      "Checking accuracy on test set\n",
      "Got 672 / 1000 correct (67.20)\n",
      "93 epoch,  1000 iteration, loss:0.112\n",
      " num 92 epoch \n",
      "####### Training Loss #######\n",
      "[0.12746964]\n",
      "Checking accuracy on test set\n",
      "Got 680 / 1000 correct (68.00)\n",
      "94 epoch,   500 iteration, loss:0.109\n",
      "Checking accuracy on test set\n",
      "Got 693 / 1000 correct (69.30)\n",
      "94 epoch,  1000 iteration, loss:0.139\n",
      " num 93 epoch \n",
      "####### Training Loss #######\n",
      "[0.12278081]\n",
      "Checking accuracy on test set\n",
      "Got 631 / 1000 correct (63.10)\n",
      "95 epoch,   500 iteration, loss:0.093\n",
      "Checking accuracy on test set\n",
      "Got 702 / 1000 correct (70.20)\n",
      "95 epoch,  1000 iteration, loss:0.148\n",
      " num 94 epoch \n",
      "####### Training Loss #######\n",
      "[0.11837251]\n",
      "Checking accuracy on test set\n",
      "Got 679 / 1000 correct (67.90)\n",
      "96 epoch,   500 iteration, loss:0.113\n",
      "Checking accuracy on test set\n",
      "Got 678 / 1000 correct (67.80)\n",
      "96 epoch,  1000 iteration, loss:0.145\n",
      " num 95 epoch \n",
      "####### Training Loss #######\n",
      "[0.1261602]\n",
      "Checking accuracy on test set\n",
      "Got 686 / 1000 correct (68.60)\n",
      "97 epoch,   500 iteration, loss:0.133\n",
      "Checking accuracy on test set\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 671 / 1000 correct (67.10)\n",
      "97 epoch,  1000 iteration, loss:0.097\n",
      " num 96 epoch \n",
      "####### Training Loss #######\n",
      "[0.10764187]\n",
      "Checking accuracy on test set\n",
      "Got 698 / 1000 correct (69.80)\n",
      "98 epoch,   500 iteration, loss:0.108\n",
      "Checking accuracy on test set\n",
      "Got 693 / 1000 correct (69.30)\n",
      "98 epoch,  1000 iteration, loss:0.115\n",
      " num 97 epoch \n",
      "####### Training Loss #######\n",
      "[0.11585148]\n",
      "Checking accuracy on test set\n",
      "Got 643 / 1000 correct (64.30)\n",
      "99 epoch,   500 iteration, loss:0.125\n",
      "Checking accuracy on test set\n",
      "Got 700 / 1000 correct (70.00)\n",
      "99 epoch,  1000 iteration, loss:0.097\n",
      " num 98 epoch \n",
      "####### Training Loss #######\n",
      "[0.11002209]\n",
      "Checking accuracy on test set\n",
      "Got 709 / 1000 correct (70.90)\n",
      "100 epoch,   500 iteration, loss:0.106\n",
      "Checking accuracy on test set\n",
      "Got 657 / 1000 correct (65.70)\n",
      "100 epoch,  1000 iteration, loss:0.111\n",
      " num 99 epoch \n",
      "####### Training Loss #######\n",
      "[0.11122951]\n",
      "Checking accuracy on test set\n",
      "Got 681 / 1000 correct (68.10)\n",
      "101 epoch,   500 iteration, loss:0.132\n",
      "Checking accuracy on test set\n",
      "Got 691 / 1000 correct (69.10)\n",
      "101 epoch,  1000 iteration, loss:0.112\n",
      " num 100 epoch \n",
      "####### Training Loss #######\n",
      "[0.11550189]\n",
      "Checking accuracy on test set\n",
      "Got 674 / 1000 correct (67.40)\n",
      "102 epoch,   500 iteration, loss:0.123\n",
      "Checking accuracy on test set\n",
      "Got 695 / 1000 correct (69.50)\n",
      "102 epoch,  1000 iteration, loss:0.141\n",
      " num 101 epoch \n",
      "####### Training Loss #######\n",
      "[0.13293981]\n",
      "Checking accuracy on test set\n",
      "Got 690 / 1000 correct (69.00)\n",
      "103 epoch,   500 iteration, loss:0.102\n",
      "Checking accuracy on test set\n",
      "Got 693 / 1000 correct (69.30)\n",
      "103 epoch,  1000 iteration, loss:0.081\n",
      " num 102 epoch \n",
      "####### Training Loss #######\n",
      "[0.1017595]\n",
      "Checking accuracy on test set\n",
      "Got 699 / 1000 correct (69.90)\n",
      "104 epoch,   500 iteration, loss:0.125\n",
      "Checking accuracy on test set\n",
      "Got 704 / 1000 correct (70.40)\n",
      "104 epoch,  1000 iteration, loss:0.133\n",
      " num 103 epoch \n",
      "####### Training Loss #######\n",
      "[0.12286126]\n",
      "Checking accuracy on test set\n",
      "Got 680 / 1000 correct (68.00)\n",
      "105 epoch,   500 iteration, loss:0.115\n",
      "Checking accuracy on test set\n",
      "Got 657 / 1000 correct (65.70)\n",
      "105 epoch,  1000 iteration, loss:0.092\n",
      " num 104 epoch \n",
      "####### Training Loss #######\n",
      "[0.10572993]\n",
      "Checking accuracy on test set\n",
      "Got 692 / 1000 correct (69.20)\n",
      "106 epoch,   500 iteration, loss:0.127\n",
      "Checking accuracy on test set\n",
      "Got 676 / 1000 correct (67.60)\n",
      "106 epoch,  1000 iteration, loss:0.103\n",
      " num 105 epoch \n",
      "####### Training Loss #######\n",
      "[0.11315551]\n",
      "Checking accuracy on test set\n",
      "Got 696 / 1000 correct (69.60)\n",
      "107 epoch,   500 iteration, loss:0.098\n",
      "Checking accuracy on test set\n",
      "Got 672 / 1000 correct (67.20)\n",
      "107 epoch,  1000 iteration, loss:0.075\n",
      " num 106 epoch \n",
      "####### Training Loss #######\n",
      "[0.09296922]\n",
      "Checking accuracy on test set\n",
      "Got 658 / 1000 correct (65.80)\n",
      "108 epoch,   500 iteration, loss:0.116\n",
      "Checking accuracy on test set\n",
      "Got 667 / 1000 correct (66.70)\n",
      "108 epoch,  1000 iteration, loss:0.120\n",
      " num 107 epoch \n",
      "####### Training Loss #######\n",
      "[0.11400003]\n",
      "Checking accuracy on test set\n",
      "Got 669 / 1000 correct (66.90)\n",
      "109 epoch,   500 iteration, loss:0.096\n",
      "Checking accuracy on test set\n",
      "Got 657 / 1000 correct (65.70)\n",
      "109 epoch,  1000 iteration, loss:0.107\n",
      " num 108 epoch \n",
      "####### Training Loss #######\n",
      "[0.10539239]\n",
      "Checking accuracy on test set\n",
      "Got 695 / 1000 correct (69.50)\n",
      "110 epoch,   500 iteration, loss:0.094\n",
      "Checking accuracy on test set\n",
      "Got 699 / 1000 correct (69.90)\n",
      "110 epoch,  1000 iteration, loss:0.135\n",
      " num 109 epoch \n",
      "####### Training Loss #######\n",
      "[0.10929233]\n",
      "Checking accuracy on test set\n",
      "Got 693 / 1000 correct (69.30)\n",
      "111 epoch,   500 iteration, loss:0.132\n",
      "Checking accuracy on test set\n",
      "Got 706 / 1000 correct (70.60)\n",
      "111 epoch,  1000 iteration, loss:0.101\n",
      " num 110 epoch \n",
      "####### Training Loss #######\n",
      "[0.11231321]\n",
      "Checking accuracy on test set\n",
      "Got 706 / 1000 correct (70.60)\n",
      "112 epoch,   500 iteration, loss:0.089\n",
      "Checking accuracy on test set\n",
      "Got 691 / 1000 correct (69.10)\n",
      "112 epoch,  1000 iteration, loss:0.133\n",
      " num 111 epoch \n",
      "####### Training Loss #######\n",
      "[0.10856071]\n",
      "Checking accuracy on test set\n",
      "Got 701 / 1000 correct (70.10)\n",
      "113 epoch,   500 iteration, loss:0.076\n",
      "Checking accuracy on test set\n",
      "Got 691 / 1000 correct (69.10)\n",
      "113 epoch,  1000 iteration, loss:0.058\n",
      " num 112 epoch \n",
      "####### Training Loss #######\n",
      "[0.09298719]\n",
      "Checking accuracy on test set\n",
      "Got 701 / 1000 correct (70.10)\n",
      "114 epoch,   500 iteration, loss:0.067\n",
      "Checking accuracy on test set\n",
      "Got 685 / 1000 correct (68.50)\n",
      "114 epoch,  1000 iteration, loss:0.078\n",
      " num 113 epoch \n",
      "####### Training Loss #######\n",
      "[0.07320058]\n",
      "Checking accuracy on test set\n",
      "Got 684 / 1000 correct (68.40)\n",
      "115 epoch,   500 iteration, loss:0.104\n",
      "Checking accuracy on test set\n",
      "Got 708 / 1000 correct (70.80)\n",
      "115 epoch,  1000 iteration, loss:0.086\n",
      " num 114 epoch \n",
      "####### Training Loss #######\n",
      "[0.09297595]\n",
      "Checking accuracy on test set\n",
      "Got 699 / 1000 correct (69.90)\n",
      "116 epoch,   500 iteration, loss:0.094\n",
      "Checking accuracy on test set\n",
      "Got 691 / 1000 correct (69.10)\n",
      "116 epoch,  1000 iteration, loss:0.110\n",
      " num 115 epoch \n",
      "####### Training Loss #######\n",
      "[0.09553844]\n",
      "Checking accuracy on test set\n",
      "Got 670 / 1000 correct (67.00)\n",
      "117 epoch,   500 iteration, loss:0.110\n",
      "Checking accuracy on test set\n",
      "Got 716 / 1000 correct (71.60)\n",
      "117 epoch,  1000 iteration, loss:0.115\n",
      " num 116 epoch \n",
      "####### Training Loss #######\n",
      "[0.11024274]\n",
      "Checking accuracy on test set\n",
      "Got 702 / 1000 correct (70.20)\n",
      "118 epoch,   500 iteration, loss:0.083\n",
      "Checking accuracy on test set\n",
      "Got 703 / 1000 correct (70.30)\n",
      "118 epoch,  1000 iteration, loss:0.077\n",
      " num 117 epoch \n",
      "####### Training Loss #######\n",
      "[0.08289478]\n",
      "Checking accuracy on test set\n",
      "Got 711 / 1000 correct (71.10)\n",
      "119 epoch,   500 iteration, loss:0.119\n",
      "Checking accuracy on test set\n",
      "Got 697 / 1000 correct (69.70)\n",
      "119 epoch,  1000 iteration, loss:0.107\n",
      " num 118 epoch \n",
      "####### Training Loss #######\n",
      "[0.10669935]\n",
      "Checking accuracy on test set\n",
      "Got 702 / 1000 correct (70.20)\n",
      "120 epoch,   500 iteration, loss:0.113\n",
      "Checking accuracy on test set\n",
      "Got 684 / 1000 correct (68.40)\n",
      "120 epoch,  1000 iteration, loss:0.104\n",
      " num 119 epoch \n",
      "####### Training Loss #######\n",
      "[0.10698526]\n",
      "Checking accuracy on test set\n",
      "Got 696 / 1000 correct (69.60)\n",
      "121 epoch,   500 iteration, loss:0.086\n",
      "Checking accuracy on test set\n",
      "Got 675 / 1000 correct (67.50)\n",
      "121 epoch,  1000 iteration, loss:0.119\n",
      " num 120 epoch \n",
      "####### Training Loss #######\n",
      "[0.10345817]\n",
      "Checking accuracy on test set\n",
      "Got 699 / 1000 correct (69.90)\n",
      "122 epoch,   500 iteration, loss:0.129\n",
      "Checking accuracy on test set\n",
      "Got 679 / 1000 correct (67.90)\n",
      "122 epoch,  1000 iteration, loss:0.108\n",
      " num 121 epoch \n",
      "####### Training Loss #######\n",
      "[0.11145197]\n",
      "Checking accuracy on test set\n",
      "Got 667 / 1000 correct (66.70)\n",
      "123 epoch,   500 iteration, loss:0.097\n",
      "Checking accuracy on test set\n",
      "Got 711 / 1000 correct (71.10)\n",
      "123 epoch,  1000 iteration, loss:0.108\n",
      " num 122 epoch \n",
      "####### Training Loss #######\n",
      "[0.09739277]\n",
      "Checking accuracy on test set\n",
      "Got 691 / 1000 correct (69.10)\n",
      "124 epoch,   500 iteration, loss:0.100\n",
      "Checking accuracy on test set\n",
      "Got 708 / 1000 correct (70.80)\n",
      "124 epoch,  1000 iteration, loss:0.101\n",
      " num 123 epoch \n",
      "####### Training Loss #######\n",
      "[0.09403348]\n",
      "Checking accuracy on test set\n",
      "Got 667 / 1000 correct (66.70)\n",
      "125 epoch,   500 iteration, loss:0.063\n",
      "Checking accuracy on test set\n",
      "Got 700 / 1000 correct (70.00)\n",
      "125 epoch,  1000 iteration, loss:0.085\n",
      " num 124 epoch \n",
      "####### Training Loss #######\n",
      "[0.07792341]\n",
      "Checking accuracy on test set\n",
      "Got 711 / 1000 correct (71.10)\n",
      "126 epoch,   500 iteration, loss:0.067\n",
      "Checking accuracy on test set\n",
      "Got 701 / 1000 correct (70.10)\n",
      "126 epoch,  1000 iteration, loss:0.094\n",
      " num 125 epoch \n",
      "####### Training Loss #######\n",
      "[0.08700409]\n",
      "Checking accuracy on test set\n",
      "Got 705 / 1000 correct (70.50)\n",
      "127 epoch,   500 iteration, loss:0.080\n",
      "Checking accuracy on test set\n",
      "Got 708 / 1000 correct (70.80)\n",
      "127 epoch,  1000 iteration, loss:0.102\n",
      " num 126 epoch \n",
      "####### Training Loss #######\n",
      "[0.10349559]\n",
      "Checking accuracy on test set\n",
      "Got 694 / 1000 correct (69.40)\n",
      "128 epoch,   500 iteration, loss:0.072\n",
      "Checking accuracy on test set\n",
      "Got 658 / 1000 correct (65.80)\n",
      "128 epoch,  1000 iteration, loss:0.095\n",
      " num 127 epoch \n",
      "####### Training Loss #######\n",
      "[0.08428698]\n",
      "Checking accuracy on test set\n",
      "Got 684 / 1000 correct (68.40)\n",
      "129 epoch,   500 iteration, loss:0.116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking accuracy on test set\n",
      "Got 684 / 1000 correct (68.40)\n",
      "129 epoch,  1000 iteration, loss:0.093\n",
      " num 128 epoch \n",
      "####### Training Loss #######\n",
      "[0.10662753]\n",
      "Checking accuracy on test set\n",
      "Got 702 / 1000 correct (70.20)\n",
      "130 epoch,   500 iteration, loss:0.103\n",
      "Checking accuracy on test set\n",
      "Got 717 / 1000 correct (71.70)\n",
      "130 epoch,  1000 iteration, loss:0.087\n",
      " num 129 epoch \n",
      "####### Training Loss #######\n",
      "[0.09071733]\n",
      "Checking accuracy on test set\n",
      "Got 672 / 1000 correct (67.20)\n",
      "131 epoch,   500 iteration, loss:0.105\n",
      "Checking accuracy on test set\n",
      "Got 683 / 1000 correct (68.30)\n",
      "131 epoch,  1000 iteration, loss:0.101\n",
      " num 130 epoch \n",
      "####### Training Loss #######\n",
      "[0.10176901]\n",
      "Checking accuracy on test set\n",
      "Got 680 / 1000 correct (68.00)\n",
      "132 epoch,   500 iteration, loss:0.107\n",
      "Checking accuracy on test set\n",
      "Got 698 / 1000 correct (69.80)\n",
      "132 epoch,  1000 iteration, loss:0.113\n",
      " num 131 epoch \n",
      "####### Training Loss #######\n",
      "[0.10667607]\n",
      "Checking accuracy on test set\n",
      "Got 683 / 1000 correct (68.30)\n",
      "133 epoch,   500 iteration, loss:0.085\n",
      "Checking accuracy on test set\n",
      "Got 707 / 1000 correct (70.70)\n",
      "133 epoch,  1000 iteration, loss:0.062\n",
      " num 132 epoch \n",
      "####### Training Loss #######\n",
      "[0.07826302]\n",
      "Checking accuracy on test set\n",
      "Got 716 / 1000 correct (71.60)\n",
      "134 epoch,   500 iteration, loss:0.111\n",
      "Checking accuracy on test set\n",
      "Got 711 / 1000 correct (71.10)\n",
      "134 epoch,  1000 iteration, loss:0.083\n",
      " num 133 epoch \n",
      "####### Training Loss #######\n",
      "[0.08852503]\n",
      "Checking accuracy on test set\n",
      "Got 702 / 1000 correct (70.20)\n",
      "135 epoch,   500 iteration, loss:0.065\n",
      "Checking accuracy on test set\n",
      "Got 715 / 1000 correct (71.50)\n",
      "135 epoch,  1000 iteration, loss:0.083\n",
      " num 134 epoch \n",
      "####### Training Loss #######\n",
      "[0.08493792]\n",
      "Checking accuracy on test set\n",
      "Got 687 / 1000 correct (68.70)\n",
      "136 epoch,   500 iteration, loss:0.071\n",
      "Checking accuracy on test set\n",
      "Got 704 / 1000 correct (70.40)\n",
      "136 epoch,  1000 iteration, loss:0.109\n",
      " num 135 epoch \n",
      "####### Training Loss #######\n",
      "[0.08658037]\n",
      "Checking accuracy on test set\n",
      "Got 704 / 1000 correct (70.40)\n",
      "137 epoch,   500 iteration, loss:0.088\n",
      "Checking accuracy on test set\n",
      "Got 673 / 1000 correct (67.30)\n",
      "137 epoch,  1000 iteration, loss:0.073\n",
      " num 136 epoch \n",
      "####### Training Loss #######\n",
      "[0.08152053]\n",
      "Checking accuracy on test set\n",
      "Got 694 / 1000 correct (69.40)\n",
      "138 epoch,   500 iteration, loss:0.107\n",
      "Checking accuracy on test set\n",
      "Got 689 / 1000 correct (68.90)\n",
      "138 epoch,  1000 iteration, loss:0.079\n",
      " num 137 epoch \n",
      "####### Training Loss #######\n",
      "[0.0910151]\n",
      "Checking accuracy on test set\n",
      "Got 682 / 1000 correct (68.20)\n",
      "139 epoch,   500 iteration, loss:0.114\n",
      "Checking accuracy on test set\n",
      "Got 686 / 1000 correct (68.60)\n",
      "139 epoch,  1000 iteration, loss:0.080\n",
      " num 138 epoch \n",
      "####### Training Loss #######\n",
      "[0.09400247]\n",
      "Checking accuracy on test set\n",
      "Got 666 / 1000 correct (66.60)\n",
      "140 epoch,   500 iteration, loss:0.079\n",
      "Checking accuracy on test set\n",
      "Got 702 / 1000 correct (70.20)\n",
      "140 epoch,  1000 iteration, loss:0.085\n",
      " num 139 epoch \n",
      "####### Training Loss #######\n",
      "[0.08152837]\n",
      "Checking accuracy on test set\n",
      "Got 703 / 1000 correct (70.30)\n",
      "141 epoch,   500 iteration, loss:0.070\n",
      "Checking accuracy on test set\n",
      "Got 661 / 1000 correct (66.10)\n",
      "141 epoch,  1000 iteration, loss:0.114\n",
      " num 140 epoch \n",
      "####### Training Loss #######\n",
      "[0.09535035]\n",
      "Checking accuracy on test set\n",
      "Got 708 / 1000 correct (70.80)\n",
      "142 epoch,   500 iteration, loss:0.090\n",
      "Checking accuracy on test set\n",
      "Got 702 / 1000 correct (70.20)\n",
      "142 epoch,  1000 iteration, loss:0.078\n",
      " num 141 epoch \n",
      "####### Training Loss #######\n",
      "[0.07779972]\n",
      "Checking accuracy on test set\n",
      "Got 676 / 1000 correct (67.60)\n",
      "143 epoch,   500 iteration, loss:0.091\n",
      "Checking accuracy on test set\n",
      "Got 710 / 1000 correct (71.00)\n",
      "143 epoch,  1000 iteration, loss:0.067\n",
      " num 142 epoch \n",
      "####### Training Loss #######\n",
      "[0.07839405]\n",
      "Checking accuracy on test set\n",
      "Got 690 / 1000 correct (69.00)\n",
      "144 epoch,   500 iteration, loss:0.105\n",
      "Checking accuracy on test set\n",
      "Got 713 / 1000 correct (71.30)\n",
      "144 epoch,  1000 iteration, loss:0.121\n",
      " num 143 epoch \n",
      "####### Training Loss #######\n",
      "[0.10528259]\n",
      "Checking accuracy on test set\n",
      "Got 672 / 1000 correct (67.20)\n",
      "145 epoch,   500 iteration, loss:0.085\n",
      "Checking accuracy on test set\n",
      "Got 694 / 1000 correct (69.40)\n",
      "145 epoch,  1000 iteration, loss:0.090\n",
      " num 144 epoch \n",
      "####### Training Loss #######\n",
      "[0.08507034]\n",
      "Checking accuracy on test set\n",
      "Got 690 / 1000 correct (69.00)\n",
      "146 epoch,   500 iteration, loss:0.074\n",
      "Checking accuracy on test set\n",
      "Got 691 / 1000 correct (69.10)\n",
      "146 epoch,  1000 iteration, loss:0.077\n",
      " num 145 epoch \n",
      "####### Training Loss #######\n",
      "[0.07639966]\n",
      "Checking accuracy on test set\n",
      "Got 701 / 1000 correct (70.10)\n",
      "147 epoch,   500 iteration, loss:0.067\n",
      "Checking accuracy on test set\n",
      "Got 658 / 1000 correct (65.80)\n",
      "147 epoch,  1000 iteration, loss:0.085\n",
      " num 146 epoch \n",
      "####### Training Loss #######\n",
      "[0.07991328]\n",
      "Checking accuracy on test set\n",
      "Got 689 / 1000 correct (68.90)\n",
      "148 epoch,   500 iteration, loss:0.068\n",
      "Checking accuracy on test set\n",
      "Got 702 / 1000 correct (70.20)\n",
      "148 epoch,  1000 iteration, loss:0.078\n",
      " num 147 epoch \n",
      "####### Training Loss #######\n",
      "[0.07302996]\n",
      "Checking accuracy on test set\n",
      "Got 698 / 1000 correct (69.80)\n",
      "149 epoch,   500 iteration, loss:0.087\n",
      "Checking accuracy on test set\n",
      "Got 695 / 1000 correct (69.50)\n",
      "149 epoch,  1000 iteration, loss:0.088\n",
      " num 148 epoch \n",
      "####### Training Loss #######\n",
      "[0.08834876]\n",
      "Checking accuracy on test set\n",
      "Got 671 / 1000 correct (67.10)\n",
      "150 epoch,   500 iteration, loss:0.098\n",
      "Checking accuracy on test set\n",
      "Got 703 / 1000 correct (70.30)\n",
      "150 epoch,  1000 iteration, loss:0.094\n",
      " num 149 epoch \n",
      "####### Training Loss #######\n",
      "[0.09520889]\n",
      "Checking accuracy on test set\n",
      "Got 690 / 1000 correct (69.00)\n",
      "151 epoch,   500 iteration, loss:0.107\n",
      "Checking accuracy on test set\n",
      "Got 696 / 1000 correct (69.60)\n",
      "151 epoch,  1000 iteration, loss:0.094\n",
      " num 150 epoch \n",
      "####### Training Loss #######\n",
      "[0.09249746]\n",
      "Checking accuracy on test set\n",
      "Got 693 / 1000 correct (69.30)\n",
      "152 epoch,   500 iteration, loss:0.057\n",
      "Checking accuracy on test set\n",
      "Got 715 / 1000 correct (71.50)\n",
      "152 epoch,  1000 iteration, loss:0.075\n",
      " num 151 epoch \n",
      "####### Training Loss #######\n",
      "[0.07157296]\n",
      "Checking accuracy on test set\n",
      "Got 673 / 1000 correct (67.30)\n",
      "153 epoch,   500 iteration, loss:0.109\n",
      "Checking accuracy on test set\n",
      "Got 698 / 1000 correct (69.80)\n",
      "153 epoch,  1000 iteration, loss:0.075\n",
      " num 152 epoch \n",
      "####### Training Loss #######\n",
      "[0.08838481]\n",
      "Checking accuracy on test set\n",
      "Got 689 / 1000 correct (68.90)\n",
      "154 epoch,   500 iteration, loss:0.078\n",
      "Checking accuracy on test set\n",
      "Got 692 / 1000 correct (69.20)\n",
      "154 epoch,  1000 iteration, loss:0.088\n",
      " num 153 epoch \n",
      "####### Training Loss #######\n",
      "[0.08685476]\n",
      "Checking accuracy on test set\n",
      "Got 689 / 1000 correct (68.90)\n",
      "155 epoch,   500 iteration, loss:0.054\n",
      "Checking accuracy on test set\n",
      "Got 669 / 1000 correct (66.90)\n",
      "155 epoch,  1000 iteration, loss:0.081\n",
      " num 154 epoch \n",
      "####### Training Loss #######\n",
      "[0.08418963]\n",
      "Checking accuracy on test set\n",
      "Got 696 / 1000 correct (69.60)\n",
      "156 epoch,   500 iteration, loss:0.088\n",
      "Checking accuracy on test set\n",
      "Got 685 / 1000 correct (68.50)\n",
      "156 epoch,  1000 iteration, loss:0.050\n",
      " num 155 epoch \n",
      "####### Training Loss #######\n",
      "[0.06406795]\n",
      "Checking accuracy on test set\n",
      "Got 709 / 1000 correct (70.90)\n",
      "157 epoch,   500 iteration, loss:0.061\n",
      "Checking accuracy on test set\n",
      "Got 712 / 1000 correct (71.20)\n",
      "157 epoch,  1000 iteration, loss:0.064\n",
      " num 156 epoch \n",
      "####### Training Loss #######\n",
      "[0.0637603]\n",
      "Checking accuracy on test set\n",
      "Got 691 / 1000 correct (69.10)\n",
      "158 epoch,   500 iteration, loss:0.078\n",
      "Checking accuracy on test set\n",
      "Got 682 / 1000 correct (68.20)\n",
      "158 epoch,  1000 iteration, loss:0.068\n",
      " num 157 epoch \n",
      "####### Training Loss #######\n",
      "[0.07910613]\n",
      "Checking accuracy on test set\n",
      "Got 698 / 1000 correct (69.80)\n",
      "159 epoch,   500 iteration, loss:0.083\n",
      "Checking accuracy on test set\n",
      "Got 696 / 1000 correct (69.60)\n",
      "159 epoch,  1000 iteration, loss:0.064\n",
      " num 158 epoch \n",
      "####### Training Loss #######\n",
      "[0.07335329]\n",
      "Checking accuracy on test set\n",
      "Got 698 / 1000 correct (69.80)\n",
      "160 epoch,   500 iteration, loss:0.081\n",
      "Checking accuracy on test set\n",
      "Got 668 / 1000 correct (66.80)\n",
      "160 epoch,  1000 iteration, loss:0.092\n",
      " num 159 epoch \n",
      "####### Training Loss #######\n",
      "[0.08281189]\n",
      "Checking accuracy on test set\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 695 / 1000 correct (69.50)\n",
      "161 epoch,   500 iteration, loss:0.082\n",
      "Checking accuracy on test set\n",
      "Got 693 / 1000 correct (69.30)\n",
      "161 epoch,  1000 iteration, loss:0.073\n",
      " num 160 epoch \n",
      "####### Training Loss #######\n",
      "[0.07867793]\n",
      "Checking accuracy on test set\n",
      "Got 706 / 1000 correct (70.60)\n",
      "162 epoch,   500 iteration, loss:0.102\n",
      "Checking accuracy on test set\n",
      "Got 676 / 1000 correct (67.60)\n",
      "162 epoch,  1000 iteration, loss:0.106\n",
      " num 161 epoch \n",
      "####### Training Loss #######\n",
      "[0.10544602]\n",
      "Checking accuracy on test set\n",
      "Got 676 / 1000 correct (67.60)\n",
      "163 epoch,   500 iteration, loss:0.091\n",
      "Checking accuracy on test set\n",
      "Got 675 / 1000 correct (67.50)\n",
      "163 epoch,  1000 iteration, loss:0.098\n",
      " num 162 epoch \n",
      "####### Training Loss #######\n",
      "[0.10775353]\n",
      "Checking accuracy on test set\n",
      "Got 691 / 1000 correct (69.10)\n",
      "164 epoch,   500 iteration, loss:0.070\n",
      "Checking accuracy on test set\n",
      "Got 709 / 1000 correct (70.90)\n",
      "164 epoch,  1000 iteration, loss:0.076\n",
      " num 163 epoch \n",
      "####### Training Loss #######\n",
      "[0.07211521]\n",
      "Checking accuracy on test set\n",
      "Got 691 / 1000 correct (69.10)\n",
      "165 epoch,   500 iteration, loss:0.078\n",
      "Checking accuracy on test set\n",
      "Got 678 / 1000 correct (67.80)\n",
      "165 epoch,  1000 iteration, loss:0.088\n",
      " num 164 epoch \n",
      "####### Training Loss #######\n",
      "[0.0868299]\n",
      "Checking accuracy on test set\n",
      "Got 671 / 1000 correct (67.10)\n",
      "166 epoch,   500 iteration, loss:0.086\n",
      "Checking accuracy on test set\n",
      "Got 676 / 1000 correct (67.60)\n",
      "166 epoch,  1000 iteration, loss:0.137\n",
      " num 165 epoch \n",
      "####### Training Loss #######\n",
      "[0.10270249]\n",
      "Checking accuracy on test set\n",
      "Got 674 / 1000 correct (67.40)\n",
      "167 epoch,   500 iteration, loss:0.065\n",
      "Checking accuracy on test set\n",
      "Got 658 / 1000 correct (65.80)\n",
      "167 epoch,  1000 iteration, loss:0.098\n",
      " num 166 epoch \n",
      "####### Training Loss #######\n",
      "[0.07575878]\n",
      "Checking accuracy on test set\n",
      "Got 673 / 1000 correct (67.30)\n",
      "168 epoch,   500 iteration, loss:0.076\n",
      "Checking accuracy on test set\n",
      "Got 705 / 1000 correct (70.50)\n",
      "168 epoch,  1000 iteration, loss:0.080\n",
      " num 167 epoch \n",
      "####### Training Loss #######\n",
      "[0.09662163]\n",
      "Checking accuracy on test set\n",
      "Got 653 / 1000 correct (65.30)\n",
      "169 epoch,   500 iteration, loss:0.066\n",
      "Checking accuracy on test set\n",
      "Got 670 / 1000 correct (67.00)\n",
      "169 epoch,  1000 iteration, loss:0.062\n",
      " num 168 epoch \n",
      "####### Training Loss #######\n",
      "[0.07003406]\n",
      "Checking accuracy on test set\n",
      "Got 709 / 1000 correct (70.90)\n",
      "170 epoch,   500 iteration, loss:0.048\n",
      "Checking accuracy on test set\n",
      "Got 645 / 1000 correct (64.50)\n",
      "170 epoch,  1000 iteration, loss:0.084\n",
      " num 169 epoch \n",
      "####### Training Loss #######\n",
      "[0.07320837]\n",
      "Checking accuracy on test set\n",
      "Got 667 / 1000 correct (66.70)\n",
      "171 epoch,   500 iteration, loss:0.081\n",
      "Checking accuracy on test set\n",
      "Got 696 / 1000 correct (69.60)\n",
      "171 epoch,  1000 iteration, loss:0.081\n",
      " num 170 epoch \n",
      "####### Training Loss #######\n",
      "[0.0788839]\n",
      "Checking accuracy on test set\n",
      "Got 675 / 1000 correct (67.50)\n",
      "172 epoch,   500 iteration, loss:0.098\n",
      "Checking accuracy on test set\n",
      "Got 671 / 1000 correct (67.10)\n",
      "172 epoch,  1000 iteration, loss:0.087\n",
      " num 171 epoch \n",
      "####### Training Loss #######\n",
      "[0.0863592]\n",
      "Checking accuracy on test set\n",
      "Got 655 / 1000 correct (65.50)\n",
      "173 epoch,   500 iteration, loss:0.096\n",
      "Checking accuracy on test set\n",
      "Got 697 / 1000 correct (69.70)\n",
      "173 epoch,  1000 iteration, loss:0.069\n",
      " num 172 epoch \n",
      "####### Training Loss #######\n",
      "[0.08353741]\n",
      "Checking accuracy on test set\n",
      "Got 696 / 1000 correct (69.60)\n",
      "174 epoch,   500 iteration, loss:0.082\n",
      "Checking accuracy on test set\n",
      "Got 699 / 1000 correct (69.90)\n",
      "174 epoch,  1000 iteration, loss:0.091\n",
      " num 173 epoch \n",
      "####### Training Loss #######\n",
      "[0.0918221]\n",
      "Checking accuracy on test set\n",
      "Got 641 / 1000 correct (64.10)\n",
      "175 epoch,   500 iteration, loss:0.077\n",
      "Checking accuracy on test set\n",
      "Got 690 / 1000 correct (69.00)\n",
      "175 epoch,  1000 iteration, loss:0.069\n",
      " num 174 epoch \n",
      "####### Training Loss #######\n",
      "[0.07075097]\n",
      "Checking accuracy on test set\n",
      "Got 676 / 1000 correct (67.60)\n",
      "176 epoch,   500 iteration, loss:0.085\n",
      "Checking accuracy on test set\n",
      "Got 682 / 1000 correct (68.20)\n",
      "176 epoch,  1000 iteration, loss:0.081\n",
      " num 175 epoch \n",
      "####### Training Loss #######\n",
      "[0.08838338]\n",
      "Checking accuracy on test set\n",
      "Got 681 / 1000 correct (68.10)\n",
      "177 epoch,   500 iteration, loss:0.065\n",
      "Checking accuracy on test set\n",
      "Got 694 / 1000 correct (69.40)\n",
      "177 epoch,  1000 iteration, loss:0.079\n",
      " num 176 epoch \n",
      "####### Training Loss #######\n",
      "[0.0711301]\n",
      "Checking accuracy on test set\n",
      "Got 681 / 1000 correct (68.10)\n",
      "178 epoch,   500 iteration, loss:0.061\n",
      "Checking accuracy on test set\n",
      "Got 674 / 1000 correct (67.40)\n",
      "178 epoch,  1000 iteration, loss:0.082\n",
      " num 177 epoch \n",
      "####### Training Loss #######\n",
      "[0.07598282]\n",
      "Checking accuracy on test set\n",
      "Got 678 / 1000 correct (67.80)\n",
      "179 epoch,   500 iteration, loss:0.073\n",
      "Checking accuracy on test set\n",
      "Got 671 / 1000 correct (67.10)\n",
      "179 epoch,  1000 iteration, loss:0.084\n",
      " num 178 epoch \n",
      "####### Training Loss #######\n",
      "[0.08361145]\n",
      "Checking accuracy on test set\n",
      "Got 653 / 1000 correct (65.30)\n",
      "180 epoch,   500 iteration, loss:0.080\n",
      "Checking accuracy on test set\n",
      "Got 695 / 1000 correct (69.50)\n",
      "180 epoch,  1000 iteration, loss:0.063\n",
      " num 179 epoch \n",
      "####### Training Loss #######\n",
      "[0.06645719]\n",
      "Checking accuracy on test set\n",
      "Got 649 / 1000 correct (64.90)\n",
      "181 epoch,   500 iteration, loss:0.092\n",
      "Checking accuracy on test set\n",
      "Got 660 / 1000 correct (66.00)\n",
      "181 epoch,  1000 iteration, loss:0.091\n",
      " num 180 epoch \n",
      "####### Training Loss #######\n",
      "[0.09159336]\n",
      "Checking accuracy on test set\n",
      "Got 685 / 1000 correct (68.50)\n",
      "182 epoch,   500 iteration, loss:0.078\n",
      "Checking accuracy on test set\n",
      "Got 705 / 1000 correct (70.50)\n",
      "182 epoch,  1000 iteration, loss:0.069\n",
      " num 181 epoch \n",
      "####### Training Loss #######\n",
      "[0.0827343]\n",
      "Checking accuracy on test set\n",
      "Got 700 / 1000 correct (70.00)\n",
      "183 epoch,   500 iteration, loss:0.065\n",
      "Checking accuracy on test set\n",
      "Got 695 / 1000 correct (69.50)\n",
      "183 epoch,  1000 iteration, loss:0.080\n",
      " num 182 epoch \n",
      "####### Training Loss #######\n",
      "[0.07544617]\n",
      "Checking accuracy on test set\n",
      "Got 681 / 1000 correct (68.10)\n",
      "184 epoch,   500 iteration, loss:0.056\n",
      "Checking accuracy on test set\n",
      "Got 697 / 1000 correct (69.70)\n",
      "184 epoch,  1000 iteration, loss:0.066\n",
      " num 183 epoch \n",
      "####### Training Loss #######\n",
      "[0.06686065]\n",
      "Checking accuracy on test set\n",
      "Got 680 / 1000 correct (68.00)\n",
      "185 epoch,   500 iteration, loss:0.066\n",
      "Checking accuracy on test set\n",
      "Got 679 / 1000 correct (67.90)\n",
      "185 epoch,  1000 iteration, loss:0.070\n",
      " num 184 epoch \n",
      "####### Training Loss #######\n",
      "[0.07056066]\n",
      "Checking accuracy on test set\n",
      "Got 675 / 1000 correct (67.50)\n",
      "186 epoch,   500 iteration, loss:0.114\n",
      "Checking accuracy on test set\n",
      "Got 694 / 1000 correct (69.40)\n",
      "186 epoch,  1000 iteration, loss:0.055\n",
      " num 185 epoch \n",
      "####### Training Loss #######\n",
      "[0.07389309]\n",
      "Checking accuracy on test set\n",
      "Got 695 / 1000 correct (69.50)\n",
      "187 epoch,   500 iteration, loss:0.044\n",
      "Checking accuracy on test set\n",
      "Got 703 / 1000 correct (70.30)\n",
      "187 epoch,  1000 iteration, loss:0.057\n",
      " num 186 epoch \n",
      "####### Training Loss #######\n",
      "[0.05136909]\n",
      "Checking accuracy on test set\n",
      "Got 635 / 1000 correct (63.50)\n",
      "188 epoch,   500 iteration, loss:0.092\n",
      "Checking accuracy on test set\n",
      "Got 692 / 1000 correct (69.20)\n",
      "188 epoch,  1000 iteration, loss:0.089\n",
      " num 187 epoch \n",
      "####### Training Loss #######\n",
      "[0.08387001]\n",
      "Checking accuracy on test set\n",
      "Got 693 / 1000 correct (69.30)\n",
      "189 epoch,   500 iteration, loss:0.080\n",
      "Checking accuracy on test set\n",
      "Got 700 / 1000 correct (70.00)\n",
      "189 epoch,  1000 iteration, loss:0.065\n",
      " num 188 epoch \n",
      "####### Training Loss #######\n",
      "[0.067331]\n",
      "Checking accuracy on test set\n",
      "Got 685 / 1000 correct (68.50)\n",
      "190 epoch,   500 iteration, loss:0.053\n",
      "Checking accuracy on test set\n",
      "Got 711 / 1000 correct (71.10)\n",
      "190 epoch,  1000 iteration, loss:0.078\n",
      " num 189 epoch \n",
      "####### Training Loss #######\n",
      "[0.07284246]\n",
      "Checking accuracy on test set\n",
      "Got 677 / 1000 correct (67.70)\n",
      "191 epoch,   500 iteration, loss:0.074\n",
      "Checking accuracy on test set\n",
      "Got 714 / 1000 correct (71.40)\n",
      "191 epoch,  1000 iteration, loss:0.077\n",
      " num 190 epoch \n",
      "####### Training Loss #######\n",
      "[0.07228356]\n",
      "Checking accuracy on test set\n",
      "Got 690 / 1000 correct (69.00)\n",
      "192 epoch,   500 iteration, loss:0.059\n",
      "Checking accuracy on test set\n",
      "Got 713 / 1000 correct (71.30)\n",
      "192 epoch,  1000 iteration, loss:0.075\n",
      " num 191 epoch \n",
      "####### Training Loss #######\n",
      "[0.06986435]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking accuracy on test set\n",
      "Got 686 / 1000 correct (68.60)\n",
      "193 epoch,   500 iteration, loss:0.122\n",
      "Checking accuracy on test set\n",
      "Got 695 / 1000 correct (69.50)\n",
      "193 epoch,  1000 iteration, loss:0.059\n",
      " num 192 epoch \n",
      "####### Training Loss #######\n",
      "[0.08844236]\n",
      "Checking accuracy on test set\n",
      "Got 662 / 1000 correct (66.20)\n",
      "194 epoch,   500 iteration, loss:0.077\n",
      "Checking accuracy on test set\n",
      "Got 705 / 1000 correct (70.50)\n",
      "194 epoch,  1000 iteration, loss:0.072\n",
      " num 193 epoch \n",
      "####### Training Loss #######\n",
      "[0.07543994]\n",
      "Checking accuracy on test set\n",
      "Got 704 / 1000 correct (70.40)\n",
      "195 epoch,   500 iteration, loss:0.083\n",
      "Checking accuracy on test set\n",
      "Got 669 / 1000 correct (66.90)\n",
      "195 epoch,  1000 iteration, loss:0.097\n",
      " num 194 epoch \n",
      "####### Training Loss #######\n",
      "[0.09330854]\n",
      "Checking accuracy on test set\n",
      "Got 716 / 1000 correct (71.60)\n",
      "196 epoch,   500 iteration, loss:0.056\n",
      "Checking accuracy on test set\n",
      "Got 702 / 1000 correct (70.20)\n",
      "196 epoch,  1000 iteration, loss:0.070\n",
      " num 195 epoch \n",
      "####### Training Loss #######\n",
      "[0.05875321]\n",
      "Checking accuracy on test set\n",
      "Got 688 / 1000 correct (68.80)\n",
      "197 epoch,   500 iteration, loss:0.081\n",
      "Checking accuracy on test set\n",
      "Got 698 / 1000 correct (69.80)\n",
      "197 epoch,  1000 iteration, loss:0.106\n",
      " num 196 epoch \n",
      "####### Training Loss #######\n",
      "[0.08671473]\n",
      "Checking accuracy on test set\n",
      "Got 710 / 1000 correct (71.00)\n",
      "198 epoch,   500 iteration, loss:0.067\n",
      "Checking accuracy on test set\n",
      "Got 698 / 1000 correct (69.80)\n",
      "198 epoch,  1000 iteration, loss:0.078\n",
      " num 197 epoch \n",
      "####### Training Loss #######\n",
      "[0.07620307]\n",
      "Checking accuracy on test set\n",
      "Got 708 / 1000 correct (70.80)\n",
      "199 epoch,   500 iteration, loss:0.079\n",
      "Checking accuracy on test set\n",
      "Got 675 / 1000 correct (67.50)\n",
      "199 epoch,  1000 iteration, loss:0.076\n",
      " num 198 epoch \n",
      "####### Training Loss #######\n",
      "[0.07871202]\n",
      "Checking accuracy on test set\n",
      "Got 668 / 1000 correct (66.80)\n",
      "200 epoch,   500 iteration, loss:0.077\n",
      "Checking accuracy on test set\n",
      "Got 703 / 1000 correct (70.30)\n",
      "200 epoch,  1000 iteration, loss:0.081\n",
      " num 199 epoch \n",
      "####### Training Loss #######\n",
      "[0.0810425]\n",
      "Checking accuracy on test set\n",
      "Got 688 / 1000 correct (68.80)\n",
      "201 epoch,   500 iteration, loss:0.088\n",
      "Checking accuracy on test set\n",
      "Got 691 / 1000 correct (69.10)\n",
      "201 epoch,  1000 iteration, loss:0.062\n",
      " num 200 epoch \n",
      "####### Training Loss #######\n",
      "[0.08003685]\n",
      "Checking accuracy on test set\n",
      "Got 696 / 1000 correct (69.60)\n",
      "202 epoch,   500 iteration, loss:0.096\n",
      "Checking accuracy on test set\n",
      "Got 700 / 1000 correct (70.00)\n",
      "202 epoch,  1000 iteration, loss:0.095\n",
      " num 201 epoch \n",
      "####### Training Loss #######\n",
      "[0.08397634]\n",
      "Checking accuracy on test set\n",
      "Got 672 / 1000 correct (67.20)\n",
      "203 epoch,   500 iteration, loss:0.055\n",
      "Checking accuracy on test set\n",
      "Got 686 / 1000 correct (68.60)\n",
      "203 epoch,  1000 iteration, loss:0.077\n",
      " num 202 epoch \n",
      "####### Training Loss #######\n",
      "[0.06784556]\n",
      "Checking accuracy on test set\n",
      "Got 708 / 1000 correct (70.80)\n",
      "204 epoch,   500 iteration, loss:0.044\n",
      "Checking accuracy on test set\n",
      "Got 695 / 1000 correct (69.50)\n",
      "204 epoch,  1000 iteration, loss:0.039\n",
      " num 203 epoch \n",
      "####### Training Loss #######\n",
      "[0.04863197]\n",
      "Checking accuracy on test set\n",
      "Got 707 / 1000 correct (70.70)\n",
      "205 epoch,   500 iteration, loss:0.066\n",
      "Checking accuracy on test set\n",
      "Got 696 / 1000 correct (69.60)\n",
      "205 epoch,  1000 iteration, loss:0.068\n",
      " num 204 epoch \n",
      "####### Training Loss #######\n",
      "[0.06771015]\n",
      "Checking accuracy on test set\n",
      "Got 713 / 1000 correct (71.30)\n",
      "206 epoch,   500 iteration, loss:0.081\n",
      "Checking accuracy on test set\n",
      "Got 708 / 1000 correct (70.80)\n",
      "206 epoch,  1000 iteration, loss:0.043\n",
      " num 205 epoch \n",
      "####### Training Loss #######\n",
      "[0.06316085]\n",
      "Checking accuracy on test set\n",
      "Got 686 / 1000 correct (68.60)\n",
      "207 epoch,   500 iteration, loss:0.071\n",
      "Checking accuracy on test set\n",
      "Got 684 / 1000 correct (68.40)\n",
      "207 epoch,  1000 iteration, loss:0.064\n",
      " num 206 epoch \n",
      "####### Training Loss #######\n",
      "[0.06140575]\n",
      "Checking accuracy on test set\n",
      "Got 679 / 1000 correct (67.90)\n",
      "208 epoch,   500 iteration, loss:0.081\n",
      "Checking accuracy on test set\n",
      "Got 680 / 1000 correct (68.00)\n",
      "208 epoch,  1000 iteration, loss:0.083\n",
      " num 207 epoch \n",
      "####### Training Loss #######\n",
      "[0.08288357]\n",
      "Checking accuracy on test set\n",
      "Got 699 / 1000 correct (69.90)\n",
      "209 epoch,   500 iteration, loss:0.082\n",
      "Checking accuracy on test set\n",
      "Got 697 / 1000 correct (69.70)\n",
      "209 epoch,  1000 iteration, loss:0.060\n",
      " num 208 epoch \n",
      "####### Training Loss #######\n",
      "[0.0702424]\n",
      "Checking accuracy on test set\n",
      "Got 699 / 1000 correct (69.90)\n",
      "210 epoch,   500 iteration, loss:0.067\n",
      "Checking accuracy on test set\n",
      "Got 692 / 1000 correct (69.20)\n",
      "210 epoch,  1000 iteration, loss:0.067\n",
      " num 209 epoch \n",
      "####### Training Loss #######\n",
      "[0.06505121]\n",
      "Checking accuracy on test set\n",
      "Got 699 / 1000 correct (69.90)\n",
      "211 epoch,   500 iteration, loss:0.050\n",
      "Checking accuracy on test set\n",
      "Got 702 / 1000 correct (70.20)\n",
      "211 epoch,  1000 iteration, loss:0.067\n",
      " num 210 epoch \n",
      "####### Training Loss #######\n",
      "[0.06006139]\n",
      "Checking accuracy on test set\n",
      "Got 678 / 1000 correct (67.80)\n",
      "212 epoch,   500 iteration, loss:0.063\n",
      "Checking accuracy on test set\n",
      "Got 687 / 1000 correct (68.70)\n",
      "212 epoch,  1000 iteration, loss:0.043\n",
      " num 211 epoch \n",
      "####### Training Loss #######\n",
      "[0.06031751]\n",
      "Checking accuracy on test set\n",
      "Got 719 / 1000 correct (71.90)\n",
      "213 epoch,   500 iteration, loss:0.054\n",
      "Checking accuracy on test set\n",
      "Got 699 / 1000 correct (69.90)\n",
      "213 epoch,  1000 iteration, loss:0.074\n",
      " num 212 epoch \n",
      "####### Training Loss #######\n",
      "[0.06806582]\n",
      "Checking accuracy on test set\n",
      "Got 687 / 1000 correct (68.70)\n",
      "214 epoch,   500 iteration, loss:0.089\n",
      "Checking accuracy on test set\n",
      "Got 702 / 1000 correct (70.20)\n",
      "214 epoch,  1000 iteration, loss:0.097\n",
      " num 213 epoch \n",
      "####### Training Loss #######\n",
      "[0.08924589]\n",
      "Checking accuracy on test set\n",
      "Got 678 / 1000 correct (67.80)\n",
      "215 epoch,   500 iteration, loss:0.085\n",
      "Checking accuracy on test set\n",
      "Got 703 / 1000 correct (70.30)\n",
      "215 epoch,  1000 iteration, loss:0.056\n",
      " num 214 epoch \n",
      "####### Training Loss #######\n",
      "[0.0647368]\n",
      "Checking accuracy on test set\n",
      "Got 657 / 1000 correct (65.70)\n",
      "216 epoch,   500 iteration, loss:0.050\n",
      "Checking accuracy on test set\n",
      "Got 718 / 1000 correct (71.80)\n",
      "216 epoch,  1000 iteration, loss:0.057\n",
      " num 215 epoch \n",
      "####### Training Loss #######\n",
      "[0.05624096]\n",
      "Checking accuracy on test set\n",
      "Got 691 / 1000 correct (69.10)\n",
      "217 epoch,   500 iteration, loss:0.060\n",
      "Checking accuracy on test set\n",
      "Got 670 / 1000 correct (67.00)\n",
      "217 epoch,  1000 iteration, loss:0.069\n",
      " num 216 epoch \n",
      "####### Training Loss #######\n",
      "[0.06516132]\n",
      "Checking accuracy on test set\n",
      "Got 678 / 1000 correct (67.80)\n",
      "218 epoch,   500 iteration, loss:0.062\n",
      "Checking accuracy on test set\n",
      "Got 701 / 1000 correct (70.10)\n",
      "218 epoch,  1000 iteration, loss:0.063\n",
      " num 217 epoch \n",
      "####### Training Loss #######\n",
      "[0.0624993]\n",
      "Checking accuracy on test set\n",
      "Got 673 / 1000 correct (67.30)\n",
      "219 epoch,   500 iteration, loss:0.086\n",
      "Checking accuracy on test set\n",
      "Got 681 / 1000 correct (68.10)\n",
      "219 epoch,  1000 iteration, loss:0.061\n",
      " num 218 epoch \n",
      "####### Training Loss #######\n",
      "[0.07117928]\n",
      "Checking accuracy on test set\n",
      "Got 647 / 1000 correct (64.70)\n",
      "220 epoch,   500 iteration, loss:0.038\n",
      "Checking accuracy on test set\n",
      "Got 668 / 1000 correct (66.80)\n",
      "220 epoch,  1000 iteration, loss:0.091\n",
      " num 219 epoch \n",
      "####### Training Loss #######\n",
      "[0.06851496]\n",
      "Checking accuracy on test set\n",
      "Got 694 / 1000 correct (69.40)\n",
      "221 epoch,   500 iteration, loss:0.063\n",
      "Checking accuracy on test set\n",
      "Got 690 / 1000 correct (69.00)\n",
      "221 epoch,  1000 iteration, loss:0.086\n",
      " num 220 epoch \n",
      "####### Training Loss #######\n",
      "[0.07386258]\n",
      "Checking accuracy on test set\n",
      "Got 712 / 1000 correct (71.20)\n",
      "222 epoch,   500 iteration, loss:0.060\n",
      "Checking accuracy on test set\n",
      "Got 701 / 1000 correct (70.10)\n",
      "222 epoch,  1000 iteration, loss:0.062\n",
      " num 221 epoch \n",
      "####### Training Loss #######\n",
      "[0.06311171]\n",
      "Checking accuracy on test set\n",
      "Got 710 / 1000 correct (71.00)\n",
      "223 epoch,   500 iteration, loss:0.060\n",
      "Checking accuracy on test set\n",
      "Got 700 / 1000 correct (70.00)\n",
      "223 epoch,  1000 iteration, loss:0.096\n",
      " num 222 epoch \n",
      "####### Training Loss #######\n",
      "[0.0859881]\n",
      "Checking accuracy on test set\n",
      "Got 639 / 1000 correct (63.90)\n",
      "224 epoch,   500 iteration, loss:0.061\n",
      "Checking accuracy on test set\n",
      "Got 680 / 1000 correct (68.00)\n",
      "224 epoch,  1000 iteration, loss:0.085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " num 223 epoch \n",
      "####### Training Loss #######\n",
      "[0.0726074]\n",
      "Checking accuracy on test set\n",
      "Got 698 / 1000 correct (69.80)\n",
      "225 epoch,   500 iteration, loss:0.087\n",
      "Checking accuracy on test set\n",
      "Got 695 / 1000 correct (69.50)\n",
      "225 epoch,  1000 iteration, loss:0.065\n",
      " num 224 epoch \n",
      "####### Training Loss #######\n",
      "[0.07646271]\n",
      "Checking accuracy on test set\n",
      "Got 692 / 1000 correct (69.20)\n",
      "226 epoch,   500 iteration, loss:0.055\n",
      "Checking accuracy on test set\n",
      "Got 701 / 1000 correct (70.10)\n",
      "226 epoch,  1000 iteration, loss:0.065\n",
      " num 225 epoch \n",
      "####### Training Loss #######\n",
      "[0.05412331]\n",
      "Checking accuracy on test set\n",
      "Got 681 / 1000 correct (68.10)\n",
      "227 epoch,   500 iteration, loss:0.037\n",
      "Checking accuracy on test set\n",
      "Got 683 / 1000 correct (68.30)\n",
      "227 epoch,  1000 iteration, loss:0.047\n",
      " num 226 epoch \n",
      "####### Training Loss #######\n",
      "[0.04998977]\n",
      "Checking accuracy on test set\n",
      "Got 669 / 1000 correct (66.90)\n",
      "228 epoch,   500 iteration, loss:0.090\n",
      "Checking accuracy on test set\n",
      "Got 708 / 1000 correct (70.80)\n",
      "228 epoch,  1000 iteration, loss:0.064\n",
      " num 227 epoch \n",
      "####### Training Loss #######\n",
      "[0.07009445]\n",
      "Checking accuracy on test set\n",
      "Got 698 / 1000 correct (69.80)\n",
      "229 epoch,   500 iteration, loss:0.082\n",
      "Checking accuracy on test set\n",
      "Got 712 / 1000 correct (71.20)\n",
      "229 epoch,  1000 iteration, loss:0.058\n",
      " num 228 epoch \n",
      "####### Training Loss #######\n",
      "[0.06627252]\n",
      "Checking accuracy on test set\n",
      "Got 688 / 1000 correct (68.80)\n",
      "230 epoch,   500 iteration, loss:0.060\n",
      "Checking accuracy on test set\n",
      "Got 682 / 1000 correct (68.20)\n",
      "230 epoch,  1000 iteration, loss:0.055\n",
      " num 229 epoch \n",
      "####### Training Loss #######\n",
      "[0.05878919]\n",
      "Checking accuracy on test set\n",
      "Got 666 / 1000 correct (66.60)\n",
      "231 epoch,   500 iteration, loss:0.055\n",
      "Checking accuracy on test set\n",
      "Got 697 / 1000 correct (69.70)\n",
      "231 epoch,  1000 iteration, loss:0.074\n",
      " num 230 epoch \n",
      "####### Training Loss #######\n",
      "[0.06876526]\n",
      "Checking accuracy on test set\n",
      "Got 679 / 1000 correct (67.90)\n",
      "232 epoch,   500 iteration, loss:0.074\n",
      "Checking accuracy on test set\n",
      "Got 648 / 1000 correct (64.80)\n",
      "232 epoch,  1000 iteration, loss:0.069\n",
      " num 231 epoch \n",
      "####### Training Loss #######\n",
      "[0.07054259]\n",
      "Checking accuracy on test set\n",
      "Got 711 / 1000 correct (71.10)\n",
      "233 epoch,   500 iteration, loss:0.056\n",
      "Checking accuracy on test set\n",
      "Got 701 / 1000 correct (70.10)\n",
      "233 epoch,  1000 iteration, loss:0.064\n",
      " num 232 epoch \n",
      "####### Training Loss #######\n",
      "[0.06148416]\n",
      "Checking accuracy on test set\n",
      "Got 665 / 1000 correct (66.50)\n",
      "234 epoch,   500 iteration, loss:0.086\n",
      "Checking accuracy on test set\n",
      "Got 680 / 1000 correct (68.00)\n",
      "234 epoch,  1000 iteration, loss:0.068\n",
      " num 233 epoch \n",
      "####### Training Loss #######\n",
      "[0.07371565]\n",
      "Checking accuracy on test set\n",
      "Got 681 / 1000 correct (68.10)\n",
      "235 epoch,   500 iteration, loss:0.050\n",
      "Checking accuracy on test set\n",
      "Got 709 / 1000 correct (70.90)\n",
      "235 epoch,  1000 iteration, loss:0.071\n",
      " num 234 epoch \n",
      "####### Training Loss #######\n",
      "[0.05909122]\n",
      "Checking accuracy on test set\n",
      "Got 703 / 1000 correct (70.30)\n",
      "236 epoch,   500 iteration, loss:0.087\n",
      "Checking accuracy on test set\n",
      "Got 700 / 1000 correct (70.00)\n",
      "236 epoch,  1000 iteration, loss:0.076\n",
      " num 235 epoch \n",
      "####### Training Loss #######\n",
      "[0.07233541]\n",
      "Checking accuracy on test set\n",
      "Got 701 / 1000 correct (70.10)\n",
      "237 epoch,   500 iteration, loss:0.088\n",
      "Checking accuracy on test set\n",
      "Got 693 / 1000 correct (69.30)\n",
      "237 epoch,  1000 iteration, loss:0.048\n",
      " num 236 epoch \n",
      "####### Training Loss #######\n",
      "[0.0674829]\n",
      "Checking accuracy on test set\n",
      "Got 699 / 1000 correct (69.90)\n",
      "238 epoch,   500 iteration, loss:0.044\n",
      "Checking accuracy on test set\n",
      "Got 698 / 1000 correct (69.80)\n",
      "238 epoch,  1000 iteration, loss:0.059\n",
      " num 237 epoch \n",
      "####### Training Loss #######\n",
      "[0.04867376]\n",
      "Checking accuracy on test set\n",
      "Got 670 / 1000 correct (67.00)\n",
      "239 epoch,   500 iteration, loss:0.055\n",
      "Checking accuracy on test set\n",
      "Got 658 / 1000 correct (65.80)\n",
      "239 epoch,  1000 iteration, loss:0.085\n",
      " num 238 epoch \n",
      "####### Training Loss #######\n",
      "[0.06858811]\n",
      "Checking accuracy on test set\n",
      "Got 681 / 1000 correct (68.10)\n",
      "240 epoch,   500 iteration, loss:0.083\n",
      "Checking accuracy on test set\n",
      "Got 689 / 1000 correct (68.90)\n",
      "240 epoch,  1000 iteration, loss:0.049\n",
      " num 239 epoch \n",
      "####### Training Loss #######\n",
      "[0.06060428]\n",
      "Checking accuracy on test set\n",
      "Got 695 / 1000 correct (69.50)\n",
      "241 epoch,   500 iteration, loss:0.120\n",
      "Checking accuracy on test set\n",
      "Got 690 / 1000 correct (69.00)\n",
      "241 epoch,  1000 iteration, loss:0.063\n",
      " num 240 epoch \n",
      "####### Training Loss #######\n",
      "[0.08919198]\n",
      "Checking accuracy on test set\n",
      "Got 685 / 1000 correct (68.50)\n",
      "242 epoch,   500 iteration, loss:0.087\n",
      "Checking accuracy on test set\n",
      "Got 683 / 1000 correct (68.30)\n",
      "242 epoch,  1000 iteration, loss:0.061\n",
      " num 241 epoch \n",
      "####### Training Loss #######\n",
      "[0.07384511]\n",
      "Checking accuracy on test set\n",
      "Got 688 / 1000 correct (68.80)\n",
      "243 epoch,   500 iteration, loss:0.091\n",
      "Checking accuracy on test set\n",
      "Got 680 / 1000 correct (68.00)\n",
      "243 epoch,  1000 iteration, loss:0.054\n",
      " num 242 epoch \n",
      "####### Training Loss #######\n",
      "[0.07523385]\n",
      "Checking accuracy on test set\n",
      "Got 674 / 1000 correct (67.40)\n",
      "244 epoch,   500 iteration, loss:0.062\n",
      "Checking accuracy on test set\n",
      "Got 674 / 1000 correct (67.40)\n",
      "244 epoch,  1000 iteration, loss:0.056\n",
      " num 243 epoch \n",
      "####### Training Loss #######\n",
      "[0.06481446]\n",
      "Checking accuracy on test set\n",
      "Got 695 / 1000 correct (69.50)\n",
      "245 epoch,   500 iteration, loss:0.046\n",
      "Checking accuracy on test set\n",
      "Got 680 / 1000 correct (68.00)\n",
      "245 epoch,  1000 iteration, loss:0.089\n",
      " num 244 epoch \n",
      "####### Training Loss #######\n",
      "[0.07243161]\n",
      "Checking accuracy on test set\n",
      "Got 708 / 1000 correct (70.80)\n",
      "246 epoch,   500 iteration, loss:0.074\n",
      "Checking accuracy on test set\n",
      "Got 718 / 1000 correct (71.80)\n",
      "246 epoch,  1000 iteration, loss:0.083\n",
      " num 245 epoch \n",
      "####### Training Loss #######\n",
      "[0.07170658]\n",
      "Checking accuracy on test set\n",
      "Got 700 / 1000 correct (70.00)\n",
      "247 epoch,   500 iteration, loss:0.041\n",
      "Checking accuracy on test set\n",
      "Got 710 / 1000 correct (71.00)\n",
      "247 epoch,  1000 iteration, loss:0.043\n",
      " num 246 epoch \n",
      "####### Training Loss #######\n",
      "[0.04434038]\n",
      "Checking accuracy on test set\n",
      "Got 710 / 1000 correct (71.00)\n",
      "248 epoch,   500 iteration, loss:0.070\n",
      "Checking accuracy on test set\n",
      "Got 704 / 1000 correct (70.40)\n",
      "248 epoch,  1000 iteration, loss:0.043\n",
      " num 247 epoch \n",
      "####### Training Loss #######\n",
      "[0.06291587]\n",
      "Checking accuracy on test set\n",
      "Got 649 / 1000 correct (64.90)\n",
      "249 epoch,   500 iteration, loss:0.061\n",
      "Checking accuracy on test set\n",
      "Got 696 / 1000 correct (69.60)\n",
      "249 epoch,  1000 iteration, loss:0.079\n",
      " num 248 epoch \n",
      "####### Training Loss #######\n",
      "[0.06683798]\n",
      "Checking accuracy on test set\n",
      "Got 681 / 1000 correct (68.10)\n",
      "250 epoch,   500 iteration, loss:0.029\n",
      "Checking accuracy on test set\n",
      "Got 692 / 1000 correct (69.20)\n",
      "250 epoch,  1000 iteration, loss:0.080\n",
      " num 249 epoch \n",
      "####### Training Loss #######\n",
      "[0.05449872]\n",
      "Checking accuracy on test set\n",
      "Got 676 / 1000 correct (67.60)\n",
      "251 epoch,   500 iteration, loss:0.049\n",
      "Checking accuracy on test set\n",
      "Got 699 / 1000 correct (69.90)\n",
      "251 epoch,  1000 iteration, loss:0.097\n",
      " num 250 epoch \n",
      "####### Training Loss #######\n",
      "[0.07125605]\n",
      "Checking accuracy on test set\n",
      "Got 680 / 1000 correct (68.00)\n",
      "252 epoch,   500 iteration, loss:0.074\n",
      "Checking accuracy on test set\n",
      "Got 682 / 1000 correct (68.20)\n",
      "252 epoch,  1000 iteration, loss:0.038\n",
      " num 251 epoch \n",
      "####### Training Loss #######\n",
      "[0.05786465]\n",
      "Checking accuracy on test set\n",
      "Got 641 / 1000 correct (64.10)\n",
      "253 epoch,   500 iteration, loss:0.074\n",
      "Checking accuracy on test set\n",
      "Got 671 / 1000 correct (67.10)\n",
      "253 epoch,  1000 iteration, loss:0.057\n",
      " num 252 epoch \n",
      "####### Training Loss #######\n",
      "[0.06974513]\n",
      "Checking accuracy on test set\n",
      "Got 691 / 1000 correct (69.10)\n",
      "254 epoch,   500 iteration, loss:0.045\n",
      "Checking accuracy on test set\n",
      "Got 693 / 1000 correct (69.30)\n",
      "254 epoch,  1000 iteration, loss:0.056\n",
      " num 253 epoch \n",
      "####### Training Loss #######\n",
      "[0.04906527]\n",
      "Checking accuracy on test set\n",
      "Got 694 / 1000 correct (69.40)\n",
      "255 epoch,   500 iteration, loss:0.052\n",
      "Checking accuracy on test set\n",
      "Got 636 / 1000 correct (63.60)\n",
      "255 epoch,  1000 iteration, loss:0.055\n",
      " num 254 epoch \n",
      "####### Training Loss #######\n",
      "[0.05215955]\n",
      "Checking accuracy on test set\n",
      "Got 686 / 1000 correct (68.60)\n",
      "256 epoch,   500 iteration, loss:0.065\n",
      "Checking accuracy on test set\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 681 / 1000 correct (68.10)\n",
      "256 epoch,  1000 iteration, loss:0.094\n",
      " num 255 epoch \n",
      "####### Training Loss #######\n",
      "[0.08336278]\n",
      "Checking accuracy on test set\n",
      "Got 666 / 1000 correct (66.60)\n",
      "257 epoch,   500 iteration, loss:0.062\n",
      "Checking accuracy on test set\n",
      "Got 700 / 1000 correct (70.00)\n",
      "257 epoch,  1000 iteration, loss:0.032\n",
      " num 256 epoch \n",
      "####### Training Loss #######\n",
      "[0.05145449]\n",
      "Checking accuracy on test set\n",
      "Got 696 / 1000 correct (69.60)\n",
      "258 epoch,   500 iteration, loss:0.031\n",
      "Checking accuracy on test set\n",
      "Got 686 / 1000 correct (68.60)\n",
      "258 epoch,  1000 iteration, loss:0.042\n",
      " num 257 epoch \n",
      "####### Training Loss #######\n",
      "[0.03512485]\n",
      "Checking accuracy on test set\n",
      "Got 692 / 1000 correct (69.20)\n",
      "259 epoch,   500 iteration, loss:0.042\n",
      "Checking accuracy on test set\n",
      "Got 668 / 1000 correct (66.80)\n",
      "259 epoch,  1000 iteration, loss:0.059\n",
      " num 258 epoch \n",
      "####### Training Loss #######\n",
      "[0.05527621]\n",
      "Checking accuracy on test set\n",
      "Got 699 / 1000 correct (69.90)\n",
      "260 epoch,   500 iteration, loss:0.043\n",
      "Checking accuracy on test set\n",
      "Got 685 / 1000 correct (68.50)\n",
      "260 epoch,  1000 iteration, loss:0.075\n",
      " num 259 epoch \n",
      "####### Training Loss #######\n",
      "[0.05437589]\n",
      "Checking accuracy on test set\n",
      "Got 700 / 1000 correct (70.00)\n",
      "261 epoch,   500 iteration, loss:0.032\n",
      "Checking accuracy on test set\n",
      "Got 702 / 1000 correct (70.20)\n",
      "261 epoch,  1000 iteration, loss:0.040\n",
      " num 260 epoch \n",
      "####### Training Loss #######\n",
      "[0.04377706]\n",
      "Checking accuracy on test set\n",
      "Got 693 / 1000 correct (69.30)\n",
      "262 epoch,   500 iteration, loss:0.040\n",
      "Checking accuracy on test set\n",
      "Got 672 / 1000 correct (67.20)\n",
      "262 epoch,  1000 iteration, loss:0.071\n",
      " num 261 epoch \n",
      "####### Training Loss #######\n",
      "[0.05634113]\n",
      "Checking accuracy on test set\n",
      "Got 688 / 1000 correct (68.80)\n",
      "263 epoch,   500 iteration, loss:0.074\n",
      "Checking accuracy on test set\n",
      "Got 681 / 1000 correct (68.10)\n",
      "263 epoch,  1000 iteration, loss:0.064\n",
      " num 262 epoch \n",
      "####### Training Loss #######\n",
      "[0.06647845]\n",
      "Checking accuracy on test set\n",
      "Got 681 / 1000 correct (68.10)\n",
      "264 epoch,   500 iteration, loss:0.060\n",
      "Checking accuracy on test set\n",
      "Got 677 / 1000 correct (67.70)\n",
      "264 epoch,  1000 iteration, loss:0.084\n",
      " num 263 epoch \n",
      "####### Training Loss #######\n",
      "[0.06901987]\n",
      "Checking accuracy on test set\n",
      "Got 678 / 1000 correct (67.80)\n",
      "265 epoch,   500 iteration, loss:0.072\n",
      "Checking accuracy on test set\n",
      "Got 668 / 1000 correct (66.80)\n",
      "265 epoch,  1000 iteration, loss:0.052\n",
      " num 264 epoch \n",
      "####### Training Loss #######\n",
      "[0.06966275]\n",
      "Checking accuracy on test set\n",
      "Got 687 / 1000 correct (68.70)\n",
      "266 epoch,   500 iteration, loss:0.113\n",
      "Checking accuracy on test set\n",
      "Got 676 / 1000 correct (67.60)\n",
      "266 epoch,  1000 iteration, loss:0.046\n",
      " num 265 epoch \n",
      "####### Training Loss #######\n",
      "[0.07671031]\n",
      "Checking accuracy on test set\n",
      "Got 701 / 1000 correct (70.10)\n",
      "267 epoch,   500 iteration, loss:0.059\n",
      "Checking accuracy on test set\n",
      "Got 653 / 1000 correct (65.30)\n",
      "267 epoch,  1000 iteration, loss:0.058\n",
      " num 266 epoch \n",
      "####### Training Loss #######\n",
      "[0.06979645]\n",
      "Checking accuracy on test set\n",
      "Got 667 / 1000 correct (66.70)\n",
      "268 epoch,   500 iteration, loss:0.058\n",
      "Checking accuracy on test set\n",
      "Got 666 / 1000 correct (66.60)\n",
      "268 epoch,  1000 iteration, loss:0.093\n",
      " num 267 epoch \n",
      "####### Training Loss #######\n",
      "[0.07617105]\n",
      "Checking accuracy on test set\n",
      "Got 707 / 1000 correct (70.70)\n",
      "269 epoch,   500 iteration, loss:0.047\n",
      "Checking accuracy on test set\n",
      "Got 666 / 1000 correct (66.60)\n",
      "269 epoch,  1000 iteration, loss:0.065\n",
      " num 268 epoch \n",
      "####### Training Loss #######\n",
      "[0.05611849]\n",
      "Checking accuracy on test set\n",
      "Got 687 / 1000 correct (68.70)\n",
      "270 epoch,   500 iteration, loss:0.073\n",
      "Checking accuracy on test set\n",
      "Got 666 / 1000 correct (66.60)\n",
      "270 epoch,  1000 iteration, loss:0.070\n",
      " num 269 epoch \n",
      "####### Training Loss #######\n",
      "[0.07041424]\n",
      "Checking accuracy on test set\n",
      "Got 668 / 1000 correct (66.80)\n",
      "271 epoch,   500 iteration, loss:0.054\n",
      "Checking accuracy on test set\n",
      "Got 691 / 1000 correct (69.10)\n",
      "271 epoch,  1000 iteration, loss:0.079\n",
      " num 270 epoch \n",
      "####### Training Loss #######\n",
      "[0.07256963]\n",
      "Checking accuracy on test set\n",
      "Got 687 / 1000 correct (68.70)\n",
      "272 epoch,   500 iteration, loss:0.074\n",
      "Checking accuracy on test set\n",
      "Got 678 / 1000 correct (67.80)\n",
      "272 epoch,  1000 iteration, loss:0.063\n",
      " num 271 epoch \n",
      "####### Training Loss #######\n",
      "[0.08205267]\n",
      "Checking accuracy on test set\n",
      "Got 687 / 1000 correct (68.70)\n",
      "273 epoch,   500 iteration, loss:0.066\n",
      "Checking accuracy on test set\n",
      "Got 701 / 1000 correct (70.10)\n",
      "273 epoch,  1000 iteration, loss:0.048\n",
      " num 272 epoch \n",
      "####### Training Loss #######\n",
      "[0.05553611]\n",
      "Checking accuracy on test set\n",
      "Got 683 / 1000 correct (68.30)\n",
      "274 epoch,   500 iteration, loss:0.063\n",
      "Checking accuracy on test set\n",
      "Got 689 / 1000 correct (68.90)\n",
      "274 epoch,  1000 iteration, loss:0.062\n",
      " num 273 epoch \n",
      "####### Training Loss #######\n",
      "[0.06188137]\n",
      "Checking accuracy on test set\n",
      "Got 693 / 1000 correct (69.30)\n",
      "275 epoch,   500 iteration, loss:0.073\n",
      "Checking accuracy on test set\n",
      "Got 695 / 1000 correct (69.50)\n",
      "275 epoch,  1000 iteration, loss:0.024\n",
      " num 274 epoch \n",
      "####### Training Loss #######\n",
      "[0.0426708]\n",
      "Checking accuracy on test set\n",
      "Got 699 / 1000 correct (69.90)\n",
      "276 epoch,   500 iteration, loss:0.037\n",
      "Checking accuracy on test set\n",
      "Got 705 / 1000 correct (70.50)\n",
      "276 epoch,  1000 iteration, loss:0.083\n",
      " num 275 epoch \n",
      "####### Training Loss #######\n",
      "[0.05785047]\n",
      "Checking accuracy on test set\n",
      "Got 690 / 1000 correct (69.00)\n",
      "277 epoch,   500 iteration, loss:0.044\n",
      "Checking accuracy on test set\n",
      "Got 705 / 1000 correct (70.50)\n",
      "277 epoch,  1000 iteration, loss:0.057\n",
      " num 276 epoch \n",
      "####### Training Loss #######\n",
      "[0.05509037]\n",
      "Checking accuracy on test set\n",
      "Got 693 / 1000 correct (69.30)\n",
      "278 epoch,   500 iteration, loss:0.048\n",
      "Checking accuracy on test set\n",
      "Got 670 / 1000 correct (67.00)\n",
      "278 epoch,  1000 iteration, loss:0.055\n",
      " num 277 epoch \n",
      "####### Training Loss #######\n",
      "[0.04945652]\n",
      "Checking accuracy on test set\n",
      "Got 654 / 1000 correct (65.40)\n",
      "279 epoch,   500 iteration, loss:0.070\n",
      "Checking accuracy on test set\n",
      "Got 668 / 1000 correct (66.80)\n",
      "279 epoch,  1000 iteration, loss:0.058\n",
      " num 278 epoch \n",
      "####### Training Loss #######\n",
      "[0.05578081]\n",
      "Checking accuracy on test set\n",
      "Got 699 / 1000 correct (69.90)\n",
      "280 epoch,   500 iteration, loss:0.066\n",
      "Checking accuracy on test set\n",
      "Got 697 / 1000 correct (69.70)\n",
      "280 epoch,  1000 iteration, loss:0.025\n",
      " num 279 epoch \n",
      "####### Training Loss #######\n",
      "[0.05188296]\n",
      "Checking accuracy on test set\n",
      "Got 680 / 1000 correct (68.00)\n",
      "281 epoch,   500 iteration, loss:0.042\n",
      "Checking accuracy on test set\n",
      "Got 707 / 1000 correct (70.70)\n",
      "281 epoch,  1000 iteration, loss:0.051\n",
      " num 280 epoch \n",
      "####### Training Loss #######\n",
      "[0.0476148]\n",
      "Checking accuracy on test set\n",
      "Got 675 / 1000 correct (67.50)\n",
      "282 epoch,   500 iteration, loss:0.057\n",
      "Checking accuracy on test set\n",
      "Got 697 / 1000 correct (69.70)\n",
      "282 epoch,  1000 iteration, loss:0.079\n",
      " num 281 epoch \n",
      "####### Training Loss #######\n",
      "[0.06539903]\n",
      "Checking accuracy on test set\n",
      "Got 674 / 1000 correct (67.40)\n",
      "283 epoch,   500 iteration, loss:0.061\n",
      "Checking accuracy on test set\n",
      "Got 704 / 1000 correct (70.40)\n",
      "283 epoch,  1000 iteration, loss:0.062\n",
      " num 282 epoch \n",
      "####### Training Loss #######\n",
      "[0.05552996]\n",
      "Checking accuracy on test set\n",
      "Got 693 / 1000 correct (69.30)\n",
      "284 epoch,   500 iteration, loss:0.029\n",
      "Checking accuracy on test set\n",
      "Got 674 / 1000 correct (67.40)\n",
      "284 epoch,  1000 iteration, loss:0.057\n",
      " num 283 epoch \n",
      "####### Training Loss #######\n",
      "[0.04457712]\n",
      "Checking accuracy on test set\n",
      "Got 700 / 1000 correct (70.00)\n",
      "285 epoch,   500 iteration, loss:0.073\n",
      "Checking accuracy on test set\n",
      "Got 635 / 1000 correct (63.50)\n",
      "285 epoch,  1000 iteration, loss:0.086\n",
      " num 284 epoch \n",
      "####### Training Loss #######\n",
      "[0.07977603]\n",
      "Checking accuracy on test set\n",
      "Got 692 / 1000 correct (69.20)\n",
      "286 epoch,   500 iteration, loss:0.051\n",
      "Checking accuracy on test set\n",
      "Got 636 / 1000 correct (63.60)\n",
      "286 epoch,  1000 iteration, loss:0.067\n",
      " num 285 epoch \n",
      "####### Training Loss #######\n",
      "[0.05870862]\n",
      "Checking accuracy on test set\n",
      "Got 688 / 1000 correct (68.80)\n",
      "287 epoch,   500 iteration, loss:0.032\n",
      "Checking accuracy on test set\n",
      "Got 695 / 1000 correct (69.50)\n",
      "287 epoch,  1000 iteration, loss:0.038\n",
      " num 286 epoch \n",
      "####### Training Loss #######\n",
      "[0.04203875]\n",
      "Checking accuracy on test set\n",
      "Got 712 / 1000 correct (71.20)\n",
      "288 epoch,   500 iteration, loss:0.035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking accuracy on test set\n",
      "Got 652 / 1000 correct (65.20)\n",
      "288 epoch,  1000 iteration, loss:0.043\n",
      " num 287 epoch \n",
      "####### Training Loss #######\n",
      "[0.04160685]\n",
      "Checking accuracy on test set\n",
      "Got 687 / 1000 correct (68.70)\n",
      "289 epoch,   500 iteration, loss:0.066\n",
      "Checking accuracy on test set\n",
      "Got 705 / 1000 correct (70.50)\n",
      "289 epoch,  1000 iteration, loss:0.064\n",
      " num 288 epoch \n",
      "####### Training Loss #######\n",
      "[0.06653314]\n",
      "Checking accuracy on test set\n",
      "Got 697 / 1000 correct (69.70)\n",
      "290 epoch,   500 iteration, loss:0.078\n",
      "Checking accuracy on test set\n",
      "Got 709 / 1000 correct (70.90)\n",
      "290 epoch,  1000 iteration, loss:0.068\n",
      " num 289 epoch \n",
      "####### Training Loss #######\n",
      "[0.06318462]\n",
      "Checking accuracy on test set\n",
      "Got 683 / 1000 correct (68.30)\n",
      "291 epoch,   500 iteration, loss:0.101\n",
      "Checking accuracy on test set\n",
      "Got 676 / 1000 correct (67.60)\n",
      "291 epoch,  1000 iteration, loss:0.045\n",
      " num 290 epoch \n",
      "####### Training Loss #######\n",
      "[0.06652789]\n",
      "Checking accuracy on test set\n",
      "Got 701 / 1000 correct (70.10)\n",
      "292 epoch,   500 iteration, loss:0.066\n",
      "Checking accuracy on test set\n",
      "Got 696 / 1000 correct (69.60)\n",
      "292 epoch,  1000 iteration, loss:0.106\n",
      " num 291 epoch \n",
      "####### Training Loss #######\n",
      "[0.08430241]\n",
      "Checking accuracy on test set\n",
      "Got 686 / 1000 correct (68.60)\n",
      "293 epoch,   500 iteration, loss:0.070\n",
      "Checking accuracy on test set\n",
      "Got 695 / 1000 correct (69.50)\n",
      "293 epoch,  1000 iteration, loss:0.042\n",
      " num 292 epoch \n",
      "####### Training Loss #######\n",
      "[0.05364202]\n",
      "Checking accuracy on test set\n",
      "Got 673 / 1000 correct (67.30)\n",
      "294 epoch,   500 iteration, loss:0.097\n",
      "Checking accuracy on test set\n",
      "Got 695 / 1000 correct (69.50)\n",
      "294 epoch,  1000 iteration, loss:0.054\n",
      " num 293 epoch \n",
      "####### Training Loss #######\n",
      "[0.06923945]\n",
      "Checking accuracy on test set\n",
      "Got 696 / 1000 correct (69.60)\n",
      "295 epoch,   500 iteration, loss:0.073\n",
      "Checking accuracy on test set\n",
      "Got 684 / 1000 correct (68.40)\n",
      "295 epoch,  1000 iteration, loss:0.077\n",
      " num 294 epoch \n",
      "####### Training Loss #######\n",
      "[0.07527648]\n",
      "Checking accuracy on test set\n",
      "Got 697 / 1000 correct (69.70)\n",
      "296 epoch,   500 iteration, loss:0.048\n",
      "Checking accuracy on test set\n",
      "Got 707 / 1000 correct (70.70)\n",
      "296 epoch,  1000 iteration, loss:0.056\n",
      " num 295 epoch \n",
      "####### Training Loss #######\n",
      "[0.05568021]\n",
      "Checking accuracy on test set\n",
      "Got 713 / 1000 correct (71.30)\n",
      "297 epoch,   500 iteration, loss:0.043\n",
      "Checking accuracy on test set\n",
      "Got 695 / 1000 correct (69.50)\n",
      "297 epoch,  1000 iteration, loss:0.074\n",
      " num 296 epoch \n",
      "####### Training Loss #######\n",
      "[0.05384186]\n",
      "Checking accuracy on test set\n",
      "Got 671 / 1000 correct (67.10)\n",
      "298 epoch,   500 iteration, loss:0.032\n",
      "Checking accuracy on test set\n",
      "Got 686 / 1000 correct (68.60)\n",
      "298 epoch,  1000 iteration, loss:0.057\n",
      " num 297 epoch \n",
      "####### Training Loss #######\n",
      "[0.04320786]\n",
      "Checking accuracy on test set\n",
      "Got 706 / 1000 correct (70.60)\n",
      "299 epoch,   500 iteration, loss:0.066\n",
      "Checking accuracy on test set\n",
      "Got 663 / 1000 correct (66.30)\n",
      "299 epoch,  1000 iteration, loss:0.077\n",
      " num 298 epoch \n",
      "####### Training Loss #######\n",
      "[0.06678783]\n",
      "Checking accuracy on test set\n",
      "Got 675 / 1000 correct (67.50)\n",
      "300 epoch,   500 iteration, loss:0.057\n",
      "Checking accuracy on test set\n",
      "Got 688 / 1000 correct (68.80)\n",
      "300 epoch,  1000 iteration, loss:0.042\n",
      " num 299 epoch \n",
      "####### Training Loss #######\n",
      "[0.04823654]\n",
      "Checking accuracy on test set\n",
      "Got 692 / 1000 correct (69.20)\n",
      "301 epoch,   500 iteration, loss:0.053\n",
      "Checking accuracy on test set\n",
      "Got 699 / 1000 correct (69.90)\n",
      "301 epoch,  1000 iteration, loss:0.055\n",
      " num 300 epoch \n",
      "####### Training Loss #######\n",
      "[0.04952919]\n",
      "Checking accuracy on test set\n",
      "Got 704 / 1000 correct (70.40)\n",
      "302 epoch,   500 iteration, loss:0.048\n",
      "Checking accuracy on test set\n",
      "Got 690 / 1000 correct (69.00)\n",
      "302 epoch,  1000 iteration, loss:0.065\n",
      " num 301 epoch \n",
      "####### Training Loss #######\n",
      "[0.05744507]\n",
      "Checking accuracy on test set\n",
      "Got 692 / 1000 correct (69.20)\n",
      "303 epoch,   500 iteration, loss:0.062\n",
      "Checking accuracy on test set\n",
      "Got 676 / 1000 correct (67.60)\n",
      "303 epoch,  1000 iteration, loss:0.030\n",
      " num 302 epoch \n",
      "####### Training Loss #######\n",
      "[0.04748955]\n",
      "Checking accuracy on test set\n",
      "Got 687 / 1000 correct (68.70)\n",
      "304 epoch,   500 iteration, loss:0.065\n",
      "Checking accuracy on test set\n",
      "Got 684 / 1000 correct (68.40)\n",
      "304 epoch,  1000 iteration, loss:0.049\n",
      " num 303 epoch \n",
      "####### Training Loss #######\n",
      "[0.05390037]\n",
      "Checking accuracy on test set\n",
      "Got 687 / 1000 correct (68.70)\n",
      "305 epoch,   500 iteration, loss:0.056\n",
      "Checking accuracy on test set\n",
      "Got 674 / 1000 correct (67.40)\n",
      "305 epoch,  1000 iteration, loss:0.085\n",
      " num 304 epoch \n",
      "####### Training Loss #######\n",
      "[0.07148076]\n",
      "Checking accuracy on test set\n",
      "Got 702 / 1000 correct (70.20)\n",
      "306 epoch,   500 iteration, loss:0.056\n",
      "Checking accuracy on test set\n",
      "Got 612 / 1000 correct (61.20)\n",
      "306 epoch,  1000 iteration, loss:0.070\n",
      " num 305 epoch \n",
      "####### Training Loss #######\n",
      "[0.06816363]\n",
      "Checking accuracy on test set\n",
      "Got 673 / 1000 correct (67.30)\n",
      "307 epoch,   500 iteration, loss:0.086\n",
      "Checking accuracy on test set\n",
      "Got 686 / 1000 correct (68.60)\n",
      "307 epoch,  1000 iteration, loss:0.074\n",
      " num 306 epoch \n",
      "####### Training Loss #######\n",
      "[0.07203795]\n",
      "Checking accuracy on test set\n",
      "Got 722 / 1000 correct (72.20)\n",
      "308 epoch,   500 iteration, loss:0.036\n",
      "Checking accuracy on test set\n",
      "Got 679 / 1000 correct (67.90)\n",
      "308 epoch,  1000 iteration, loss:0.050\n",
      " num 307 epoch \n",
      "####### Training Loss #######\n",
      "[0.05116077]\n",
      "Checking accuracy on test set\n",
      "Got 694 / 1000 correct (69.40)\n",
      "309 epoch,   500 iteration, loss:0.043\n",
      "Checking accuracy on test set\n",
      "Got 695 / 1000 correct (69.50)\n",
      "309 epoch,  1000 iteration, loss:0.035\n",
      " num 308 epoch \n",
      "####### Training Loss #######\n",
      "[0.04532458]\n",
      "Checking accuracy on test set\n",
      "Got 680 / 1000 correct (68.00)\n",
      "310 epoch,   500 iteration, loss:0.061\n",
      "Checking accuracy on test set\n",
      "Got 680 / 1000 correct (68.00)\n",
      "310 epoch,  1000 iteration, loss:0.080\n",
      " num 309 epoch \n",
      "####### Training Loss #######\n",
      "[0.06527502]\n",
      "Checking accuracy on test set\n",
      "Got 665 / 1000 correct (66.50)\n",
      "311 epoch,   500 iteration, loss:0.070\n",
      "Checking accuracy on test set\n",
      "Got 674 / 1000 correct (67.40)\n",
      "311 epoch,  1000 iteration, loss:0.046\n",
      " num 310 epoch \n",
      "####### Training Loss #######\n",
      "[0.06116866]\n",
      "Checking accuracy on test set\n",
      "Got 667 / 1000 correct (66.70)\n",
      "312 epoch,   500 iteration, loss:0.050\n",
      "Checking accuracy on test set\n",
      "Got 687 / 1000 correct (68.70)\n",
      "312 epoch,  1000 iteration, loss:0.077\n",
      " num 311 epoch \n",
      "####### Training Loss #######\n",
      "[0.06976692]\n",
      "Checking accuracy on test set\n",
      "Got 710 / 1000 correct (71.00)\n",
      "313 epoch,   500 iteration, loss:0.050\n",
      "Checking accuracy on test set\n",
      "Got 692 / 1000 correct (69.20)\n",
      "313 epoch,  1000 iteration, loss:0.035\n",
      " num 312 epoch \n",
      "####### Training Loss #######\n",
      "[0.0449911]\n",
      "Checking accuracy on test set\n",
      "Got 692 / 1000 correct (69.20)\n",
      "314 epoch,   500 iteration, loss:0.052\n",
      "Checking accuracy on test set\n",
      "Got 665 / 1000 correct (66.50)\n",
      "314 epoch,  1000 iteration, loss:0.075\n",
      " num 313 epoch \n",
      "####### Training Loss #######\n",
      "[0.06054716]\n",
      "Checking accuracy on test set\n",
      "Got 700 / 1000 correct (70.00)\n",
      "315 epoch,   500 iteration, loss:0.050\n",
      "Checking accuracy on test set\n",
      "Got 672 / 1000 correct (67.20)\n",
      "315 epoch,  1000 iteration, loss:0.108\n",
      " num 314 epoch \n",
      "####### Training Loss #######\n",
      "[0.0735799]\n",
      "Checking accuracy on test set\n",
      "Got 632 / 1000 correct (63.20)\n",
      "316 epoch,   500 iteration, loss:0.045\n",
      "Checking accuracy on test set\n",
      "Got 684 / 1000 correct (68.40)\n",
      "316 epoch,  1000 iteration, loss:0.046\n",
      " num 315 epoch \n",
      "####### Training Loss #######\n",
      "[0.03913818]\n",
      "Checking accuracy on test set\n",
      "Got 657 / 1000 correct (65.70)\n",
      "317 epoch,   500 iteration, loss:0.058\n",
      "Checking accuracy on test set\n",
      "Got 678 / 1000 correct (67.80)\n",
      "317 epoch,  1000 iteration, loss:0.081\n",
      " num 316 epoch \n",
      "####### Training Loss #######\n",
      "[0.06727639]\n",
      "Checking accuracy on test set\n",
      "Got 681 / 1000 correct (68.10)\n",
      "318 epoch,   500 iteration, loss:0.084\n",
      "Checking accuracy on test set\n",
      "Got 677 / 1000 correct (67.70)\n",
      "318 epoch,  1000 iteration, loss:0.049\n",
      " num 317 epoch \n",
      "####### Training Loss #######\n",
      "[0.0727145]\n",
      "Checking accuracy on test set\n",
      "Got 683 / 1000 correct (68.30)\n",
      "319 epoch,   500 iteration, loss:0.069\n",
      "Checking accuracy on test set\n",
      "Got 681 / 1000 correct (68.10)\n",
      "319 epoch,  1000 iteration, loss:0.066\n",
      " num 318 epoch \n",
      "####### Training Loss #######\n",
      "[0.06503042]\n",
      "Checking accuracy on test set\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 669 / 1000 correct (66.90)\n",
      "320 epoch,   500 iteration, loss:0.061\n",
      "Checking accuracy on test set\n",
      "Got 684 / 1000 correct (68.40)\n",
      "320 epoch,  1000 iteration, loss:0.057\n",
      " num 319 epoch \n",
      "####### Training Loss #######\n",
      "[0.06510315]\n",
      "Checking accuracy on test set\n",
      "Got 642 / 1000 correct (64.20)\n",
      "321 epoch,   500 iteration, loss:0.053\n",
      "Checking accuracy on test set\n",
      "Got 686 / 1000 correct (68.60)\n",
      "321 epoch,  1000 iteration, loss:0.063\n",
      " num 320 epoch \n",
      "####### Training Loss #######\n",
      "[0.0539141]\n",
      "Checking accuracy on test set\n",
      "Got 668 / 1000 correct (66.80)\n",
      "322 epoch,   500 iteration, loss:0.082\n",
      "Checking accuracy on test set\n",
      "Got 674 / 1000 correct (67.40)\n",
      "322 epoch,  1000 iteration, loss:0.042\n",
      " num 321 epoch \n",
      "####### Training Loss #######\n",
      "[0.05894009]\n",
      "Checking accuracy on test set\n",
      "Got 672 / 1000 correct (67.20)\n",
      "323 epoch,   500 iteration, loss:0.062\n",
      "Checking accuracy on test set\n",
      "Got 684 / 1000 correct (68.40)\n",
      "323 epoch,  1000 iteration, loss:0.044\n",
      " num 322 epoch \n",
      "####### Training Loss #######\n",
      "[0.05592081]\n",
      "Checking accuracy on test set\n",
      "Got 602 / 1000 correct (60.20)\n",
      "324 epoch,   500 iteration, loss:0.055\n",
      "Checking accuracy on test set\n",
      "Got 699 / 1000 correct (69.90)\n",
      "324 epoch,  1000 iteration, loss:0.057\n",
      " num 323 epoch \n",
      "####### Training Loss #######\n",
      "[0.05128161]\n",
      "Checking accuracy on test set\n",
      "Got 649 / 1000 correct (64.90)\n",
      "325 epoch,   500 iteration, loss:0.072\n",
      "Checking accuracy on test set\n",
      "Got 653 / 1000 correct (65.30)\n",
      "325 epoch,  1000 iteration, loss:0.088\n",
      " num 324 epoch \n",
      "####### Training Loss #######\n",
      "[0.07677639]\n",
      "Checking accuracy on test set\n",
      "Got 674 / 1000 correct (67.40)\n",
      "326 epoch,   500 iteration, loss:0.063\n",
      "Checking accuracy on test set\n",
      "Got 677 / 1000 correct (67.70)\n",
      "326 epoch,  1000 iteration, loss:0.082\n",
      " num 325 epoch \n",
      "####### Training Loss #######\n",
      "[0.06401049]\n",
      "Checking accuracy on test set\n",
      "Got 689 / 1000 correct (68.90)\n",
      "327 epoch,   500 iteration, loss:0.054\n",
      "Checking accuracy on test set\n",
      "Got 695 / 1000 correct (69.50)\n",
      "327 epoch,  1000 iteration, loss:0.053\n",
      " num 326 epoch \n",
      "####### Training Loss #######\n",
      "[0.05107784]\n",
      "Checking accuracy on test set\n",
      "Got 655 / 1000 correct (65.50)\n",
      "328 epoch,   500 iteration, loss:0.057\n",
      "Checking accuracy on test set\n",
      "Got 677 / 1000 correct (67.70)\n",
      "328 epoch,  1000 iteration, loss:0.044\n",
      " num 327 epoch \n",
      "####### Training Loss #######\n",
      "[0.0495872]\n",
      "Checking accuracy on test set\n",
      "Got 625 / 1000 correct (62.50)\n",
      "329 epoch,   500 iteration, loss:0.052\n",
      "Checking accuracy on test set\n",
      "Got 693 / 1000 correct (69.30)\n",
      "329 epoch,  1000 iteration, loss:0.063\n",
      " num 328 epoch \n",
      "####### Training Loss #######\n",
      "[0.05038258]\n",
      "Checking accuracy on test set\n",
      "Got 659 / 1000 correct (65.90)\n",
      "330 epoch,   500 iteration, loss:0.068\n",
      "Checking accuracy on test set\n",
      "Got 677 / 1000 correct (67.70)\n",
      "330 epoch,  1000 iteration, loss:0.040\n",
      " num 329 epoch \n",
      "####### Training Loss #######\n",
      "[0.06117251]\n",
      "Checking accuracy on test set\n",
      "Got 691 / 1000 correct (69.10)\n",
      "331 epoch,   500 iteration, loss:0.032\n",
      "Checking accuracy on test set\n",
      "Got 665 / 1000 correct (66.50)\n",
      "331 epoch,  1000 iteration, loss:0.055\n",
      " num 330 epoch \n",
      "####### Training Loss #######\n",
      "[0.04376927]\n",
      "Checking accuracy on test set\n",
      "Got 682 / 1000 correct (68.20)\n",
      "332 epoch,   500 iteration, loss:0.054\n",
      "Checking accuracy on test set\n",
      "Got 697 / 1000 correct (69.70)\n",
      "332 epoch,  1000 iteration, loss:0.033\n",
      " num 331 epoch \n",
      "####### Training Loss #######\n",
      "[0.03815235]\n",
      "Checking accuracy on test set\n",
      "Got 694 / 1000 correct (69.40)\n",
      "333 epoch,   500 iteration, loss:0.039\n",
      "Checking accuracy on test set\n",
      "Got 687 / 1000 correct (68.70)\n",
      "333 epoch,  1000 iteration, loss:0.056\n",
      " num 332 epoch \n",
      "####### Training Loss #######\n",
      "[0.04855397]\n",
      "Checking accuracy on test set\n",
      "Got 685 / 1000 correct (68.50)\n",
      "334 epoch,   500 iteration, loss:0.059\n",
      "Checking accuracy on test set\n",
      "Got 692 / 1000 correct (69.20)\n",
      "334 epoch,  1000 iteration, loss:0.048\n",
      " num 333 epoch \n",
      "####### Training Loss #######\n",
      "[0.05713505]\n",
      "Checking accuracy on test set\n",
      "Got 684 / 1000 correct (68.40)\n",
      "335 epoch,   500 iteration, loss:0.077\n",
      "Checking accuracy on test set\n",
      "Got 687 / 1000 correct (68.70)\n",
      "335 epoch,  1000 iteration, loss:0.043\n",
      " num 334 epoch \n",
      "####### Training Loss #######\n",
      "[0.05305389]\n",
      "Checking accuracy on test set\n",
      "Got 662 / 1000 correct (66.20)\n",
      "336 epoch,   500 iteration, loss:0.045\n",
      "Checking accuracy on test set\n",
      "Got 655 / 1000 correct (65.50)\n",
      "336 epoch,  1000 iteration, loss:0.043\n",
      " num 335 epoch \n",
      "####### Training Loss #######\n",
      "[0.04432339]\n",
      "Checking accuracy on test set\n",
      "Got 666 / 1000 correct (66.60)\n",
      "337 epoch,   500 iteration, loss:0.081\n",
      "Checking accuracy on test set\n",
      "Got 671 / 1000 correct (67.10)\n",
      "337 epoch,  1000 iteration, loss:0.080\n",
      " num 336 epoch \n",
      "####### Training Loss #######\n",
      "[0.07458659]\n",
      "Checking accuracy on test set\n",
      "Got 690 / 1000 correct (69.00)\n",
      "338 epoch,   500 iteration, loss:0.059\n",
      "Checking accuracy on test set\n",
      "Got 680 / 1000 correct (68.00)\n",
      "338 epoch,  1000 iteration, loss:0.053\n",
      " num 337 epoch \n",
      "####### Training Loss #######\n",
      "[0.05978445]\n",
      "Checking accuracy on test set\n",
      "Got 699 / 1000 correct (69.90)\n",
      "339 epoch,   500 iteration, loss:0.089\n",
      "Checking accuracy on test set\n",
      "Got 680 / 1000 correct (68.00)\n",
      "339 epoch,  1000 iteration, loss:0.065\n",
      " num 338 epoch \n",
      "####### Training Loss #######\n",
      "[0.07725933]\n",
      "Checking accuracy on test set\n",
      "Got 645 / 1000 correct (64.50)\n",
      "340 epoch,   500 iteration, loss:0.092\n",
      "Checking accuracy on test set\n",
      "Got 672 / 1000 correct (67.20)\n",
      "340 epoch,  1000 iteration, loss:0.074\n",
      " num 339 epoch \n",
      "####### Training Loss #######\n",
      "[0.08573417]\n",
      "Checking accuracy on test set\n",
      "Got 678 / 1000 correct (67.80)\n",
      "341 epoch,   500 iteration, loss:0.070\n",
      "Checking accuracy on test set\n",
      "Got 698 / 1000 correct (69.80)\n",
      "341 epoch,  1000 iteration, loss:0.041\n",
      " num 340 epoch \n",
      "####### Training Loss #######\n",
      "[0.05900103]\n",
      "Checking accuracy on test set\n",
      "Got 676 / 1000 correct (67.60)\n",
      "342 epoch,   500 iteration, loss:0.081\n",
      "Checking accuracy on test set\n",
      "Got 673 / 1000 correct (67.30)\n",
      "342 epoch,  1000 iteration, loss:0.051\n",
      " num 341 epoch \n",
      "####### Training Loss #######\n",
      "[0.05989832]\n",
      "Checking accuracy on test set\n",
      "Got 681 / 1000 correct (68.10)\n",
      "343 epoch,   500 iteration, loss:0.079\n",
      "Checking accuracy on test set\n",
      "Got 692 / 1000 correct (69.20)\n",
      "343 epoch,  1000 iteration, loss:0.069\n",
      " num 342 epoch \n",
      "####### Training Loss #######\n",
      "[0.06583497]\n",
      "Checking accuracy on test set\n",
      "Got 665 / 1000 correct (66.50)\n",
      "344 epoch,   500 iteration, loss:0.044\n",
      "Checking accuracy on test set\n",
      "Got 666 / 1000 correct (66.60)\n",
      "344 epoch,  1000 iteration, loss:0.095\n",
      " num 343 epoch \n",
      "####### Training Loss #######\n",
      "[0.07125065]\n",
      "Checking accuracy on test set\n",
      "Got 681 / 1000 correct (68.10)\n",
      "345 epoch,   500 iteration, loss:0.037\n",
      "Checking accuracy on test set\n",
      "Got 693 / 1000 correct (69.30)\n",
      "345 epoch,  1000 iteration, loss:0.036\n",
      " num 344 epoch \n",
      "####### Training Loss #######\n",
      "[0.04050673]\n",
      "Checking accuracy on test set\n",
      "Got 683 / 1000 correct (68.30)\n",
      "346 epoch,   500 iteration, loss:0.094\n",
      "Checking accuracy on test set\n",
      "Got 679 / 1000 correct (67.90)\n",
      "346 epoch,  1000 iteration, loss:0.037\n",
      " num 345 epoch \n",
      "####### Training Loss #######\n",
      "[0.06254196]\n",
      "Checking accuracy on test set\n",
      "Got 683 / 1000 correct (68.30)\n",
      "347 epoch,   500 iteration, loss:0.034\n",
      "Checking accuracy on test set\n",
      "Got 695 / 1000 correct (69.50)\n",
      "347 epoch,  1000 iteration, loss:0.058\n",
      " num 346 epoch \n",
      "####### Training Loss #######\n",
      "[0.03978561]\n",
      "Checking accuracy on test set\n",
      "Got 672 / 1000 correct (67.20)\n",
      "348 epoch,   500 iteration, loss:0.029\n",
      "Checking accuracy on test set\n",
      "Got 691 / 1000 correct (69.10)\n",
      "348 epoch,  1000 iteration, loss:0.039\n",
      " num 347 epoch \n",
      "####### Training Loss #######\n",
      "[0.03498151]\n",
      "Checking accuracy on test set\n",
      "Got 702 / 1000 correct (70.20)\n",
      "349 epoch,   500 iteration, loss:0.043\n",
      "Checking accuracy on test set\n",
      "Got 662 / 1000 correct (66.20)\n",
      "349 epoch,  1000 iteration, loss:0.052\n",
      " num 348 epoch \n",
      "####### Training Loss #######\n",
      "[0.04307617]\n",
      "Checking accuracy on test set\n",
      "Got 662 / 1000 correct (66.20)\n",
      "350 epoch,   500 iteration, loss:0.031\n",
      "Checking accuracy on test set\n",
      "Got 686 / 1000 correct (68.60)\n",
      "350 epoch,  1000 iteration, loss:0.032\n",
      " num 349 epoch \n",
      "####### Training Loss #######\n",
      "[0.03176484]\n",
      "finish training \n",
      "\n",
      "now begin saving datum for next step plotting\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'cPickle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-1b680b7c6730>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m all_cnn_c = running_model_B(run_num, all_cnn_c, net_name, lr, epoch, \n\u001b[0;32m---> 10\u001b[0;31m                         loaderA_train, loaderA_test)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-6775a6e4bcd0>\u001b[0m in \u001b[0;36mrunning_model_B\u001b[0;34m(run_num, net, net_name, lr_list, epoch_list, loader_train, loader_test)\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/train_loss.save'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m     \u001b[0mcPickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcPickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHIGHEST_PROTOCOL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cPickle' is not defined"
     ]
    }
   ],
   "source": [
    "lr = [0.01, 0.005, 0.001, 0.0005]\n",
    "epoch = [200, 250, 300] # first20\n",
    "\n",
    "run_num = 10\n",
    "net_name = 'ALL_CNN_C_Class1'\n",
    "\n",
    "torch.cuda.set_device(0)\n",
    "\n",
    "all_cnn_c_class1 = running_model_B(run_num, all_cnn_c_class1, net_name, lr, epoch, \n",
    "                        loaderA_train, loaderA_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n",
      "begin training\n",
      "Checking accuracy on test set\n",
      "Got 285 / 1000 correct (28.50)\n",
      "1 epoch,   500 iteration, loss:2.227\n",
      "Checking accuracy on test set\n",
      "Got 326 / 1000 correct (32.60)\n",
      "1 epoch,  1000 iteration, loss:2.040\n",
      " num 0 epoch \n",
      "####### Training Loss #######\n",
      "[2.08894361]\n",
      "Checking accuracy on test set\n",
      "Got 424 / 1000 correct (42.40)\n",
      "2 epoch,   500 iteration, loss:1.857\n",
      "Checking accuracy on test set\n",
      "Got 410 / 1000 correct (41.00)\n",
      "2 epoch,  1000 iteration, loss:1.769\n",
      " num 1 epoch \n",
      "####### Training Loss #######\n",
      "[1.78663604]\n",
      "Checking accuracy on test set\n",
      "Got 477 / 1000 correct (47.70)\n",
      "3 epoch,   500 iteration, loss:1.686\n",
      "Checking accuracy on test set\n",
      "Got 469 / 1000 correct (46.90)\n",
      "3 epoch,  1000 iteration, loss:1.623\n",
      " num 2 epoch \n",
      "####### Training Loss #######\n",
      "[1.63211447]\n",
      "Checking accuracy on test set\n",
      "Got 495 / 1000 correct (49.50)\n",
      "4 epoch,   500 iteration, loss:1.570\n",
      "Checking accuracy on test set\n",
      "Got 501 / 1000 correct (50.10)\n",
      "4 epoch,  1000 iteration, loss:1.519\n",
      " num 3 epoch \n",
      "####### Training Loss #######\n",
      "[1.53254108]\n",
      "Checking accuracy on test set\n",
      "Got 520 / 1000 correct (52.00)\n",
      "5 epoch,   500 iteration, loss:1.486\n",
      "Checking accuracy on test set\n",
      "Got 499 / 1000 correct (49.90)\n",
      "5 epoch,  1000 iteration, loss:1.463\n",
      " num 4 epoch \n",
      "####### Training Loss #######\n",
      "[1.45843031]\n",
      "Checking accuracy on test set\n",
      "Got 521 / 1000 correct (52.10)\n",
      "6 epoch,   500 iteration, loss:1.448\n",
      "Checking accuracy on test set\n",
      "Got 507 / 1000 correct (50.70)\n",
      "6 epoch,  1000 iteration, loss:1.391\n",
      " num 5 epoch \n",
      "####### Training Loss #######\n",
      "[1.40468234]\n",
      "Checking accuracy on test set\n",
      "Got 532 / 1000 correct (53.20)\n",
      "7 epoch,   500 iteration, loss:1.388\n",
      "Checking accuracy on test set\n",
      "Got 529 / 1000 correct (52.90)\n",
      "7 epoch,  1000 iteration, loss:1.352\n",
      " num 6 epoch \n",
      "####### Training Loss #######\n",
      "[1.35964266]\n",
      "Checking accuracy on test set\n",
      "Got 532 / 1000 correct (53.20)\n",
      "8 epoch,   500 iteration, loss:1.366\n",
      "Checking accuracy on test set\n",
      "Got 538 / 1000 correct (53.80)\n",
      "8 epoch,  1000 iteration, loss:1.305\n",
      " num 7 epoch \n",
      "####### Training Loss #######\n",
      "[1.32421304]\n",
      "Checking accuracy on test set\n",
      "Got 546 / 1000 correct (54.60)\n",
      "9 epoch,   500 iteration, loss:1.326\n",
      "Checking accuracy on test set\n",
      "Got 547 / 1000 correct (54.70)\n",
      "9 epoch,  1000 iteration, loss:1.277\n",
      " num 8 epoch \n",
      "####### Training Loss #######\n",
      "[1.28965343]\n",
      "Checking accuracy on test set\n",
      "Got 556 / 1000 correct (55.60)\n",
      "10 epoch,   500 iteration, loss:1.296\n",
      "Checking accuracy on test set\n",
      "Got 557 / 1000 correct (55.70)\n",
      "10 epoch,  1000 iteration, loss:1.242\n",
      " num 9 epoch \n",
      "####### Training Loss #######\n",
      "[1.25918596]\n",
      "Checking accuracy on test set\n",
      "Got 565 / 1000 correct (56.50)\n",
      "11 epoch,   500 iteration, loss:1.252\n",
      "Checking accuracy on test set\n",
      "Got 571 / 1000 correct (57.10)\n",
      "11 epoch,  1000 iteration, loss:1.195\n",
      " num 10 epoch \n",
      "####### Training Loss #######\n",
      "[1.20959962]\n",
      "Checking accuracy on test set\n",
      "Got 547 / 1000 correct (54.70)\n",
      "12 epoch,   500 iteration, loss:1.236\n",
      "Checking accuracy on test set\n",
      "Got 569 / 1000 correct (56.90)\n",
      "12 epoch,  1000 iteration, loss:1.194\n",
      " num 11 epoch \n",
      "####### Training Loss #######\n",
      "[1.20275816]\n",
      "Checking accuracy on test set\n",
      "Got 581 / 1000 correct (58.10)\n",
      "13 epoch,   500 iteration, loss:1.205\n",
      "Checking accuracy on test set\n",
      "Got 571 / 1000 correct (57.10)\n",
      "13 epoch,  1000 iteration, loss:1.134\n",
      " num 12 epoch \n",
      "####### Training Loss #######\n",
      "[1.16333562]\n",
      "Checking accuracy on test set\n",
      "Got 569 / 1000 correct (56.90)\n",
      "14 epoch,   500 iteration, loss:1.163\n",
      "Checking accuracy on test set\n",
      "Got 581 / 1000 correct (58.10)\n",
      "14 epoch,  1000 iteration, loss:1.114\n",
      " num 13 epoch \n",
      "####### Training Loss #######\n",
      "[1.12574696]\n",
      "Checking accuracy on test set\n",
      "Got 584 / 1000 correct (58.40)\n",
      "15 epoch,   500 iteration, loss:1.140\n",
      "Checking accuracy on test set\n",
      "Got 589 / 1000 correct (58.90)\n",
      "15 epoch,  1000 iteration, loss:1.081\n",
      " num 14 epoch \n",
      "####### Training Loss #######\n",
      "[1.09918462]\n",
      "Checking accuracy on test set\n",
      "Got 597 / 1000 correct (59.70)\n",
      "16 epoch,   500 iteration, loss:1.120\n",
      "Checking accuracy on test set\n",
      "Got 596 / 1000 correct (59.60)\n",
      "16 epoch,  1000 iteration, loss:1.052\n",
      " num 15 epoch \n",
      "####### Training Loss #######\n",
      "[1.07689876]\n",
      "Checking accuracy on test set\n",
      "Got 602 / 1000 correct (60.20)\n",
      "17 epoch,   500 iteration, loss:1.088\n",
      "Checking accuracy on test set\n",
      "Got 596 / 1000 correct (59.60)\n",
      "17 epoch,  1000 iteration, loss:1.023\n",
      " num 16 epoch \n",
      "####### Training Loss #######\n",
      "[1.04841468]\n",
      "Checking accuracy on test set\n",
      "Got 615 / 1000 correct (61.50)\n",
      "18 epoch,   500 iteration, loss:1.076\n",
      "Checking accuracy on test set\n",
      "Got 600 / 1000 correct (60.00)\n",
      "18 epoch,  1000 iteration, loss:1.005\n",
      " num 17 epoch \n",
      "####### Training Loss #######\n",
      "[1.03348413]\n",
      "Checking accuracy on test set\n",
      "Got 610 / 1000 correct (61.00)\n",
      "19 epoch,   500 iteration, loss:1.033\n",
      "Checking accuracy on test set\n",
      "Got 616 / 1000 correct (61.60)\n",
      "19 epoch,  1000 iteration, loss:0.980\n",
      " num 18 epoch \n",
      "####### Training Loss #######\n",
      "[1.00103198]\n",
      "Checking accuracy on test set\n",
      "Got 611 / 1000 correct (61.10)\n",
      "20 epoch,   500 iteration, loss:1.032\n",
      "Checking accuracy on test set\n",
      "Got 622 / 1000 correct (62.20)\n",
      "20 epoch,  1000 iteration, loss:0.955\n",
      " num 19 epoch \n",
      "####### Training Loss #######\n",
      "[0.98504615]\n",
      "Checking accuracy on test set\n",
      "Got 614 / 1000 correct (61.40)\n",
      "21 epoch,   500 iteration, loss:0.997\n",
      "Checking accuracy on test set\n",
      "Got 634 / 1000 correct (63.40)\n",
      "21 epoch,  1000 iteration, loss:0.918\n",
      " num 20 epoch \n",
      "####### Training Loss #######\n",
      "[0.95448962]\n",
      "Checking accuracy on test set\n",
      "Got 639 / 1000 correct (63.90)\n",
      "22 epoch,   500 iteration, loss:0.989\n",
      "Checking accuracy on test set\n",
      "Got 626 / 1000 correct (62.60)\n",
      "22 epoch,  1000 iteration, loss:0.894\n",
      " num 21 epoch \n",
      "####### Training Loss #######\n",
      "[0.93573654]\n",
      "Checking accuracy on test set\n",
      "Got 607 / 1000 correct (60.70)\n",
      "23 epoch,   500 iteration, loss:0.938\n",
      "Checking accuracy on test set\n",
      "Got 637 / 1000 correct (63.70)\n",
      "23 epoch,  1000 iteration, loss:0.881\n",
      " num 22 epoch \n",
      "####### Training Loss #######\n",
      "[0.90631909]\n",
      "Checking accuracy on test set\n",
      "Got 628 / 1000 correct (62.80)\n",
      "24 epoch,   500 iteration, loss:0.923\n",
      "Checking accuracy on test set\n",
      "Got 628 / 1000 correct (62.80)\n",
      "24 epoch,  1000 iteration, loss:0.870\n",
      " num 23 epoch \n",
      "####### Training Loss #######\n",
      "[0.8896887]\n",
      "Checking accuracy on test set\n",
      "Got 644 / 1000 correct (64.40)\n",
      "25 epoch,   500 iteration, loss:0.910\n",
      "Checking accuracy on test set\n",
      "Got 639 / 1000 correct (63.90)\n",
      "25 epoch,  1000 iteration, loss:0.835\n",
      " num 24 epoch \n",
      "####### Training Loss #######\n",
      "[0.86910957]\n",
      "Checking accuracy on test set\n",
      "Got 637 / 1000 correct (63.70)\n",
      "26 epoch,   500 iteration, loss:0.890\n",
      "Checking accuracy on test set\n",
      "Got 638 / 1000 correct (63.80)\n",
      "26 epoch,  1000 iteration, loss:0.819\n",
      " num 25 epoch \n",
      "####### Training Loss #######\n",
      "[0.84600569]\n",
      "Checking accuracy on test set\n",
      "Got 633 / 1000 correct (63.30)\n",
      "27 epoch,   500 iteration, loss:0.877\n",
      "Checking accuracy on test set\n",
      "Got 650 / 1000 correct (65.00)\n",
      "27 epoch,  1000 iteration, loss:0.819\n",
      " num 26 epoch \n",
      "####### Training Loss #######\n",
      "[0.83493497]\n",
      "Checking accuracy on test set\n",
      "Got 657 / 1000 correct (65.70)\n",
      "28 epoch,   500 iteration, loss:0.848\n",
      "Checking accuracy on test set\n",
      "Got 657 / 1000 correct (65.70)\n",
      "28 epoch,  1000 iteration, loss:0.777\n",
      " num 27 epoch \n",
      "####### Training Loss #######\n",
      "[0.81240097]\n",
      "Checking accuracy on test set\n",
      "Got 635 / 1000 correct (63.50)\n",
      "29 epoch,   500 iteration, loss:0.822\n",
      "Checking accuracy on test set\n",
      "Got 647 / 1000 correct (64.70)\n",
      "29 epoch,  1000 iteration, loss:0.774\n",
      " num 28 epoch \n",
      "####### Training Loss #######\n",
      "[0.79331294]\n",
      "Checking accuracy on test set\n",
      "Got 659 / 1000 correct (65.90)\n",
      "30 epoch,   500 iteration, loss:0.829\n",
      "Checking accuracy on test set\n",
      "Got 647 / 1000 correct (64.70)\n",
      "30 epoch,  1000 iteration, loss:0.752\n",
      " num 29 epoch \n",
      "####### Training Loss #######\n",
      "[0.7907406]\n",
      "Checking accuracy on test set\n",
      "Got 651 / 1000 correct (65.10)\n",
      "31 epoch,   500 iteration, loss:0.805\n",
      "Checking accuracy on test set\n",
      "Got 656 / 1000 correct (65.60)\n",
      "31 epoch,  1000 iteration, loss:0.730\n",
      " num 30 epoch \n",
      "####### Training Loss #######\n",
      "[0.7656703]\n",
      "Checking accuracy on test set\n",
      "Got 662 / 1000 correct (66.20)\n",
      "32 epoch,   500 iteration, loss:0.777\n",
      "Checking accuracy on test set\n",
      "Got 658 / 1000 correct (65.80)\n",
      "32 epoch,  1000 iteration, loss:0.726\n",
      " num 31 epoch \n",
      "####### Training Loss #######\n",
      "[0.75109656]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking accuracy on test set\n",
      "Got 655 / 1000 correct (65.50)\n",
      "33 epoch,   500 iteration, loss:0.766\n",
      "Checking accuracy on test set\n",
      "Got 670 / 1000 correct (67.00)\n",
      "33 epoch,  1000 iteration, loss:0.710\n",
      " num 32 epoch \n",
      "####### Training Loss #######\n",
      "[0.73669778]\n",
      "Checking accuracy on test set\n",
      "Got 650 / 1000 correct (65.00)\n",
      "34 epoch,   500 iteration, loss:0.746\n",
      "Checking accuracy on test set\n",
      "Got 649 / 1000 correct (64.90)\n",
      "34 epoch,  1000 iteration, loss:0.689\n",
      " num 33 epoch \n",
      "####### Training Loss #######\n",
      "[0.71571388]\n",
      "Checking accuracy on test set\n",
      "Got 667 / 1000 correct (66.70)\n",
      "35 epoch,   500 iteration, loss:0.717\n",
      "Checking accuracy on test set\n",
      "Got 659 / 1000 correct (65.90)\n",
      "35 epoch,  1000 iteration, loss:0.699\n",
      " num 34 epoch \n",
      "####### Training Loss #######\n",
      "[0.70672636]\n",
      "Checking accuracy on test set\n",
      "Got 665 / 1000 correct (66.50)\n",
      "36 epoch,   500 iteration, loss:0.709\n",
      "Checking accuracy on test set\n",
      "Got 660 / 1000 correct (66.00)\n",
      "36 epoch,  1000 iteration, loss:0.649\n",
      " num 35 epoch \n",
      "####### Training Loss #######\n",
      "[0.67565041]\n",
      "Checking accuracy on test set\n",
      "Got 662 / 1000 correct (66.20)\n",
      "37 epoch,   500 iteration, loss:0.689\n",
      "Checking accuracy on test set\n",
      "Got 673 / 1000 correct (67.30)\n",
      "37 epoch,  1000 iteration, loss:0.640\n",
      " num 36 epoch \n",
      "####### Training Loss #######\n",
      "[0.6580141]\n",
      "Checking accuracy on test set\n",
      "Got 658 / 1000 correct (65.80)\n",
      "38 epoch,   500 iteration, loss:0.676\n",
      "Checking accuracy on test set\n",
      "Got 658 / 1000 correct (65.80)\n",
      "38 epoch,  1000 iteration, loss:0.633\n",
      " num 37 epoch \n",
      "####### Training Loss #######\n",
      "[0.64730939]\n",
      "Checking accuracy on test set\n",
      "Got 664 / 1000 correct (66.40)\n",
      "39 epoch,   500 iteration, loss:0.654\n",
      "Checking accuracy on test set\n",
      "Got 648 / 1000 correct (64.80)\n",
      "39 epoch,  1000 iteration, loss:0.603\n",
      " num 38 epoch \n",
      "####### Training Loss #######\n",
      "[0.62743826]\n",
      "Checking accuracy on test set\n",
      "Got 672 / 1000 correct (67.20)\n",
      "40 epoch,   500 iteration, loss:0.630\n",
      "Checking accuracy on test set\n",
      "Got 675 / 1000 correct (67.50)\n",
      "40 epoch,  1000 iteration, loss:0.592\n",
      " num 39 epoch \n",
      "####### Training Loss #######\n",
      "[0.6146801]\n",
      "Checking accuracy on test set\n",
      "Got 664 / 1000 correct (66.40)\n",
      "41 epoch,   500 iteration, loss:0.642\n",
      "Checking accuracy on test set\n",
      "Got 666 / 1000 correct (66.60)\n",
      "41 epoch,  1000 iteration, loss:0.579\n",
      " num 40 epoch \n",
      "####### Training Loss #######\n",
      "[0.60414451]\n",
      "Checking accuracy on test set\n",
      "Got 660 / 1000 correct (66.00)\n",
      "42 epoch,   500 iteration, loss:0.605\n",
      "Checking accuracy on test set\n",
      "Got 663 / 1000 correct (66.30)\n",
      "42 epoch,  1000 iteration, loss:0.563\n",
      " num 41 epoch \n",
      "####### Training Loss #######\n",
      "[0.59260494]\n",
      "Checking accuracy on test set\n",
      "Got 669 / 1000 correct (66.90)\n",
      "43 epoch,   500 iteration, loss:0.608\n",
      "Checking accuracy on test set\n",
      "Got 674 / 1000 correct (67.40)\n",
      "43 epoch,  1000 iteration, loss:0.544\n",
      " num 42 epoch \n",
      "####### Training Loss #######\n",
      "[0.5808668]\n",
      "Checking accuracy on test set\n",
      "Got 658 / 1000 correct (65.80)\n",
      "44 epoch,   500 iteration, loss:0.580\n",
      "Checking accuracy on test set\n",
      "Got 690 / 1000 correct (69.00)\n",
      "44 epoch,  1000 iteration, loss:0.516\n",
      " num 43 epoch \n",
      "####### Training Loss #######\n",
      "[0.54892655]\n",
      "Checking accuracy on test set\n",
      "Got 676 / 1000 correct (67.60)\n",
      "45 epoch,   500 iteration, loss:0.554\n",
      "Checking accuracy on test set\n",
      "Got 672 / 1000 correct (67.20)\n",
      "45 epoch,  1000 iteration, loss:0.541\n",
      " num 44 epoch \n",
      "####### Training Loss #######\n",
      "[0.549386]\n",
      "Checking accuracy on test set\n",
      "Got 664 / 1000 correct (66.40)\n",
      "46 epoch,   500 iteration, loss:0.543\n",
      "Checking accuracy on test set\n",
      "Got 678 / 1000 correct (67.80)\n",
      "46 epoch,  1000 iteration, loss:0.513\n",
      " num 45 epoch \n",
      "####### Training Loss #######\n",
      "[0.51996633]\n",
      "Checking accuracy on test set\n",
      "Got 691 / 1000 correct (69.10)\n",
      "47 epoch,   500 iteration, loss:0.513\n",
      "Checking accuracy on test set\n",
      "Got 675 / 1000 correct (67.50)\n",
      "47 epoch,  1000 iteration, loss:0.491\n",
      " num 46 epoch \n",
      "####### Training Loss #######\n",
      "[0.5063246]\n",
      "Checking accuracy on test set\n",
      "Got 662 / 1000 correct (66.20)\n",
      "48 epoch,   500 iteration, loss:0.543\n",
      "Checking accuracy on test set\n",
      "Got 688 / 1000 correct (68.80)\n",
      "48 epoch,  1000 iteration, loss:0.478\n",
      " num 47 epoch \n",
      "####### Training Loss #######\n",
      "[0.51219383]\n",
      "Checking accuracy on test set\n",
      "Got 642 / 1000 correct (64.20)\n",
      "49 epoch,   500 iteration, loss:0.497\n",
      "Checking accuracy on test set\n",
      "Got 698 / 1000 correct (69.80)\n",
      "49 epoch,  1000 iteration, loss:0.485\n",
      " num 48 epoch \n",
      "####### Training Loss #######\n",
      "[0.4875199]\n",
      "Checking accuracy on test set\n",
      "Got 681 / 1000 correct (68.10)\n",
      "50 epoch,   500 iteration, loss:0.506\n",
      "Checking accuracy on test set\n",
      "Got 674 / 1000 correct (67.40)\n",
      "50 epoch,  1000 iteration, loss:0.491\n",
      " num 49 epoch \n",
      "####### Training Loss #######\n",
      "[0.49077079]\n",
      "Checking accuracy on test set\n",
      "Got 667 / 1000 correct (66.70)\n",
      "51 epoch,   500 iteration, loss:0.472\n",
      "Checking accuracy on test set\n",
      "Got 693 / 1000 correct (69.30)\n",
      "51 epoch,  1000 iteration, loss:0.443\n",
      " num 50 epoch \n",
      "####### Training Loss #######\n",
      "[0.46053665]\n",
      "Checking accuracy on test set\n",
      "Got 644 / 1000 correct (64.40)\n",
      "52 epoch,   500 iteration, loss:0.489\n",
      "Checking accuracy on test set\n",
      "Got 678 / 1000 correct (67.80)\n",
      "52 epoch,  1000 iteration, loss:0.438\n",
      " num 51 epoch \n",
      "####### Training Loss #######\n",
      "[0.46161989]\n",
      "Checking accuracy on test set\n",
      "Got 677 / 1000 correct (67.70)\n",
      "53 epoch,   500 iteration, loss:0.427\n",
      "Checking accuracy on test set\n",
      "Got 679 / 1000 correct (67.90)\n",
      "53 epoch,  1000 iteration, loss:0.450\n",
      " num 52 epoch \n",
      "####### Training Loss #######\n",
      "[0.44075573]\n",
      "Checking accuracy on test set\n",
      "Got 677 / 1000 correct (67.70)\n",
      "54 epoch,   500 iteration, loss:0.470\n",
      "Checking accuracy on test set\n",
      "Got 654 / 1000 correct (65.40)\n",
      "54 epoch,  1000 iteration, loss:0.401\n",
      " num 53 epoch \n",
      "####### Training Loss #######\n",
      "[0.43461372]\n",
      "Checking accuracy on test set\n",
      "Got 659 / 1000 correct (65.90)\n",
      "55 epoch,   500 iteration, loss:0.437\n",
      "Checking accuracy on test set\n",
      "Got 671 / 1000 correct (67.10)\n",
      "55 epoch,  1000 iteration, loss:0.405\n",
      " num 54 epoch \n",
      "####### Training Loss #######\n",
      "[0.41373347]\n",
      "Checking accuracy on test set\n",
      "Got 686 / 1000 correct (68.60)\n",
      "56 epoch,   500 iteration, loss:0.433\n",
      "Checking accuracy on test set\n",
      "Got 670 / 1000 correct (67.00)\n",
      "56 epoch,  1000 iteration, loss:0.403\n",
      " num 55 epoch \n",
      "####### Training Loss #######\n",
      "[0.41990762]\n",
      "Checking accuracy on test set\n",
      "Got 661 / 1000 correct (66.10)\n",
      "57 epoch,   500 iteration, loss:0.419\n",
      "Checking accuracy on test set\n",
      "Got 688 / 1000 correct (68.80)\n",
      "57 epoch,  1000 iteration, loss:0.368\n",
      " num 56 epoch \n",
      "####### Training Loss #######\n",
      "[0.39146643]\n",
      "Checking accuracy on test set\n",
      "Got 660 / 1000 correct (66.00)\n",
      "58 epoch,   500 iteration, loss:0.385\n",
      "Checking accuracy on test set\n",
      "Got 679 / 1000 correct (67.90)\n",
      "58 epoch,  1000 iteration, loss:0.399\n",
      " num 57 epoch \n",
      "####### Training Loss #######\n",
      "[0.3906934]\n",
      "Checking accuracy on test set\n",
      "Got 666 / 1000 correct (66.60)\n",
      "59 epoch,   500 iteration, loss:0.376\n",
      "Checking accuracy on test set\n",
      "Got 693 / 1000 correct (69.30)\n",
      "59 epoch,  1000 iteration, loss:0.370\n",
      " num 58 epoch \n",
      "####### Training Loss #######\n",
      "[0.37205546]\n",
      "Checking accuracy on test set\n",
      "Got 685 / 1000 correct (68.50)\n",
      "60 epoch,   500 iteration, loss:0.382\n",
      "Checking accuracy on test set\n",
      "Got 657 / 1000 correct (65.70)\n",
      "60 epoch,  1000 iteration, loss:0.355\n",
      " num 59 epoch \n",
      "####### Training Loss #######\n",
      "[0.37237221]\n",
      "Checking accuracy on test set\n",
      "Got 660 / 1000 correct (66.00)\n",
      "61 epoch,   500 iteration, loss:0.359\n",
      "Checking accuracy on test set\n",
      "Got 684 / 1000 correct (68.40)\n",
      "61 epoch,  1000 iteration, loss:0.359\n",
      " num 60 epoch \n",
      "####### Training Loss #######\n",
      "[0.35841271]\n",
      "Checking accuracy on test set\n",
      "Got 664 / 1000 correct (66.40)\n",
      "62 epoch,   500 iteration, loss:0.367\n",
      "Checking accuracy on test set\n",
      "Got 683 / 1000 correct (68.30)\n",
      "62 epoch,  1000 iteration, loss:0.371\n",
      " num 61 epoch \n",
      "####### Training Loss #######\n",
      "[0.36431005]\n",
      "Checking accuracy on test set\n",
      "Got 691 / 1000 correct (69.10)\n",
      "63 epoch,   500 iteration, loss:0.349\n",
      "Checking accuracy on test set\n",
      "Got 685 / 1000 correct (68.50)\n",
      "63 epoch,  1000 iteration, loss:0.332\n",
      " num 62 epoch \n",
      "####### Training Loss #######\n",
      "[0.34003603]\n",
      "Checking accuracy on test set\n",
      "Got 673 / 1000 correct (67.30)\n",
      "64 epoch,   500 iteration, loss:0.317\n",
      "Checking accuracy on test set\n",
      "Got 665 / 1000 correct (66.50)\n",
      "64 epoch,  1000 iteration, loss:0.341\n",
      " num 63 epoch \n",
      "####### Training Loss #######\n",
      "[0.32883458]\n",
      "Checking accuracy on test set\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 679 / 1000 correct (67.90)\n",
      "65 epoch,   500 iteration, loss:0.345\n",
      "Checking accuracy on test set\n",
      "Got 670 / 1000 correct (67.00)\n",
      "65 epoch,  1000 iteration, loss:0.306\n",
      " num 64 epoch \n",
      "####### Training Loss #######\n",
      "[0.32198758]\n",
      "Checking accuracy on test set\n",
      "Got 690 / 1000 correct (69.00)\n",
      "66 epoch,   500 iteration, loss:0.339\n",
      "Checking accuracy on test set\n",
      "Got 688 / 1000 correct (68.80)\n",
      "66 epoch,  1000 iteration, loss:0.303\n",
      " num 65 epoch \n",
      "####### Training Loss #######\n",
      "[0.31515415]\n",
      "Checking accuracy on test set\n",
      "Got 662 / 1000 correct (66.20)\n",
      "67 epoch,   500 iteration, loss:0.300\n",
      "Checking accuracy on test set\n",
      "Got 677 / 1000 correct (67.70)\n",
      "67 epoch,  1000 iteration, loss:0.310\n",
      " num 66 epoch \n",
      "####### Training Loss #######\n",
      "[0.30774676]\n",
      "Checking accuracy on test set\n",
      "Got 676 / 1000 correct (67.60)\n",
      "68 epoch,   500 iteration, loss:0.301\n",
      "Checking accuracy on test set\n",
      "Got 688 / 1000 correct (68.80)\n",
      "68 epoch,  1000 iteration, loss:0.309\n",
      " num 67 epoch \n",
      "####### Training Loss #######\n",
      "[0.30643499]\n",
      "Checking accuracy on test set\n",
      "Got 697 / 1000 correct (69.70)\n",
      "69 epoch,   500 iteration, loss:0.302\n",
      "Checking accuracy on test set\n",
      "Got 659 / 1000 correct (65.90)\n",
      "69 epoch,  1000 iteration, loss:0.288\n",
      " num 68 epoch \n",
      "####### Training Loss #######\n",
      "[0.29317511]\n",
      "Checking accuracy on test set\n",
      "Got 666 / 1000 correct (66.60)\n",
      "70 epoch,   500 iteration, loss:0.279\n",
      "Checking accuracy on test set\n",
      "Got 656 / 1000 correct (65.60)\n",
      "70 epoch,  1000 iteration, loss:0.293\n",
      " num 69 epoch \n",
      "####### Training Loss #######\n",
      "[0.29219471]\n",
      "Checking accuracy on test set\n",
      "Got 671 / 1000 correct (67.10)\n",
      "71 epoch,   500 iteration, loss:0.308\n",
      "Checking accuracy on test set\n",
      "Got 695 / 1000 correct (69.50)\n",
      "71 epoch,  1000 iteration, loss:0.262\n",
      " num 70 epoch \n",
      "####### Training Loss #######\n",
      "[0.28083964]\n",
      "Checking accuracy on test set\n",
      "Got 674 / 1000 correct (67.40)\n",
      "72 epoch,   500 iteration, loss:0.275\n",
      "Checking accuracy on test set\n",
      "Got 675 / 1000 correct (67.50)\n",
      "72 epoch,  1000 iteration, loss:0.275\n",
      " num 71 epoch \n",
      "####### Training Loss #######\n",
      "[0.27708367]\n",
      "Checking accuracy on test set\n",
      "Got 696 / 1000 correct (69.60)\n",
      "73 epoch,   500 iteration, loss:0.262\n",
      "Checking accuracy on test set\n",
      "Got 691 / 1000 correct (69.10)\n",
      "73 epoch,  1000 iteration, loss:0.286\n",
      " num 72 epoch \n",
      "####### Training Loss #######\n",
      "[0.27380546]\n",
      "Checking accuracy on test set\n",
      "Got 704 / 1000 correct (70.40)\n",
      "74 epoch,   500 iteration, loss:0.260\n",
      "Checking accuracy on test set\n",
      "Got 683 / 1000 correct (68.30)\n",
      "74 epoch,  1000 iteration, loss:0.255\n",
      " num 73 epoch \n",
      "####### Training Loss #######\n",
      "[0.25387304]\n",
      "Checking accuracy on test set\n",
      "Got 692 / 1000 correct (69.20)\n",
      "75 epoch,   500 iteration, loss:0.270\n",
      "Checking accuracy on test set\n",
      "Got 682 / 1000 correct (68.20)\n",
      "75 epoch,  1000 iteration, loss:0.284\n",
      " num 74 epoch \n",
      "####### Training Loss #######\n",
      "[0.27363364]\n",
      "Checking accuracy on test set\n",
      "Got 681 / 1000 correct (68.10)\n",
      "76 epoch,   500 iteration, loss:0.282\n",
      "Checking accuracy on test set\n",
      "Got 654 / 1000 correct (65.40)\n",
      "76 epoch,  1000 iteration, loss:0.232\n",
      " num 75 epoch \n",
      "####### Training Loss #######\n",
      "[0.25561033]\n",
      "Checking accuracy on test set\n",
      "Got 683 / 1000 correct (68.30)\n",
      "77 epoch,   500 iteration, loss:0.272\n",
      "Checking accuracy on test set\n",
      "Got 685 / 1000 correct (68.50)\n",
      "77 epoch,  1000 iteration, loss:0.229\n",
      " num 76 epoch \n",
      "####### Training Loss #######\n",
      "[0.2501521]\n",
      "Checking accuracy on test set\n",
      "Got 673 / 1000 correct (67.30)\n",
      "78 epoch,   500 iteration, loss:0.281\n",
      "Checking accuracy on test set\n",
      "Got 688 / 1000 correct (68.80)\n",
      "78 epoch,  1000 iteration, loss:0.259\n",
      " num 77 epoch \n",
      "####### Training Loss #######\n",
      "[0.26439851]\n",
      "Checking accuracy on test set\n",
      "Got 657 / 1000 correct (65.70)\n",
      "79 epoch,   500 iteration, loss:0.226\n",
      "Checking accuracy on test set\n",
      "Got 688 / 1000 correct (68.80)\n",
      "79 epoch,  1000 iteration, loss:0.221\n",
      " num 78 epoch \n",
      "####### Training Loss #######\n",
      "[0.2275728]\n",
      "Checking accuracy on test set\n",
      "Got 669 / 1000 correct (66.90)\n",
      "80 epoch,   500 iteration, loss:0.232\n",
      "Checking accuracy on test set\n",
      "Got 679 / 1000 correct (67.90)\n",
      "80 epoch,  1000 iteration, loss:0.209\n",
      " num 79 epoch \n",
      "####### Training Loss #######\n",
      "[0.23062688]\n",
      "Checking accuracy on test set\n",
      "Got 671 / 1000 correct (67.10)\n",
      "81 epoch,   500 iteration, loss:0.230\n",
      "Checking accuracy on test set\n",
      "Got 665 / 1000 correct (66.50)\n",
      "81 epoch,  1000 iteration, loss:0.258\n",
      " num 80 epoch \n",
      "####### Training Loss #######\n",
      "[0.23926759]\n",
      "Checking accuracy on test set\n",
      "Got 670 / 1000 correct (67.00)\n",
      "82 epoch,   500 iteration, loss:0.231\n",
      "Checking accuracy on test set\n",
      "Got 688 / 1000 correct (68.80)\n",
      "82 epoch,  1000 iteration, loss:0.249\n",
      " num 81 epoch \n",
      "####### Training Loss #######\n",
      "[0.23633908]\n",
      "Checking accuracy on test set\n",
      "Got 676 / 1000 correct (67.60)\n",
      "83 epoch,   500 iteration, loss:0.237\n",
      "Checking accuracy on test set\n",
      "Got 699 / 1000 correct (69.90)\n",
      "83 epoch,  1000 iteration, loss:0.187\n",
      " num 82 epoch \n",
      "####### Training Loss #######\n",
      "[0.20898008]\n",
      "Checking accuracy on test set\n",
      "Got 667 / 1000 correct (66.70)\n",
      "84 epoch,   500 iteration, loss:0.217\n",
      "Checking accuracy on test set\n",
      "Got 684 / 1000 correct (68.40)\n",
      "84 epoch,  1000 iteration, loss:0.225\n",
      " num 83 epoch \n",
      "####### Training Loss #######\n",
      "[0.21901162]\n",
      "Checking accuracy on test set\n",
      "Got 682 / 1000 correct (68.20)\n",
      "85 epoch,   500 iteration, loss:0.216\n",
      "Checking accuracy on test set\n",
      "Got 680 / 1000 correct (68.00)\n",
      "85 epoch,  1000 iteration, loss:0.201\n",
      " num 84 epoch \n",
      "####### Training Loss #######\n",
      "[0.20760942]\n",
      "Checking accuracy on test set\n",
      "Got 683 / 1000 correct (68.30)\n",
      "86 epoch,   500 iteration, loss:0.246\n",
      "Checking accuracy on test set\n",
      "Got 672 / 1000 correct (67.20)\n",
      "86 epoch,  1000 iteration, loss:0.206\n",
      " num 85 epoch \n",
      "####### Training Loss #######\n",
      "[0.21986778]\n",
      "Checking accuracy on test set\n",
      "Got 686 / 1000 correct (68.60)\n",
      "87 epoch,   500 iteration, loss:0.227\n",
      "Checking accuracy on test set\n",
      "Got 672 / 1000 correct (67.20)\n",
      "87 epoch,  1000 iteration, loss:0.197\n",
      " num 86 epoch \n",
      "####### Training Loss #######\n",
      "[0.21618131]\n",
      "Checking accuracy on test set\n",
      "Got 687 / 1000 correct (68.70)\n",
      "88 epoch,   500 iteration, loss:0.221\n",
      "Checking accuracy on test set\n",
      "Got 679 / 1000 correct (67.90)\n",
      "88 epoch,  1000 iteration, loss:0.188\n",
      " num 87 epoch \n",
      "####### Training Loss #######\n",
      "[0.20466612]\n",
      "Checking accuracy on test set\n",
      "Got 659 / 1000 correct (65.90)\n",
      "89 epoch,   500 iteration, loss:0.197\n",
      "Checking accuracy on test set\n",
      "Got 684 / 1000 correct (68.40)\n",
      "89 epoch,  1000 iteration, loss:0.227\n",
      " num 88 epoch \n",
      "####### Training Loss #######\n",
      "[0.2110905]\n",
      "Checking accuracy on test set\n",
      "Got 674 / 1000 correct (67.40)\n",
      "90 epoch,   500 iteration, loss:0.207\n",
      "Checking accuracy on test set\n",
      "Got 672 / 1000 correct (67.20)\n",
      "90 epoch,  1000 iteration, loss:0.217\n",
      " num 89 epoch \n",
      "####### Training Loss #######\n",
      "[0.20597336]\n",
      "Checking accuracy on test set\n",
      "Got 671 / 1000 correct (67.10)\n",
      "91 epoch,   500 iteration, loss:0.185\n",
      "Checking accuracy on test set\n",
      "Got 689 / 1000 correct (68.90)\n",
      "91 epoch,  1000 iteration, loss:0.208\n",
      " num 90 epoch \n",
      "####### Training Loss #######\n",
      "[0.20105291]\n",
      "Checking accuracy on test set\n",
      "Got 693 / 1000 correct (69.30)\n",
      "92 epoch,   500 iteration, loss:0.198\n",
      "Checking accuracy on test set\n",
      "Got 700 / 1000 correct (70.00)\n",
      "92 epoch,  1000 iteration, loss:0.181\n",
      " num 91 epoch \n",
      "####### Training Loss #######\n",
      "[0.18890682]\n",
      "Checking accuracy on test set\n",
      "Got 689 / 1000 correct (68.90)\n",
      "93 epoch,   500 iteration, loss:0.171\n",
      "Checking accuracy on test set\n",
      "Got 702 / 1000 correct (70.20)\n",
      "93 epoch,  1000 iteration, loss:0.189\n",
      " num 92 epoch \n",
      "####### Training Loss #######\n",
      "[0.17653743]\n",
      "Checking accuracy on test set\n",
      "Got 672 / 1000 correct (67.20)\n",
      "94 epoch,   500 iteration, loss:0.174\n",
      "Checking accuracy on test set\n",
      "Got 688 / 1000 correct (68.80)\n",
      "94 epoch,  1000 iteration, loss:0.188\n",
      " num 93 epoch \n",
      "####### Training Loss #######\n",
      "[0.17925693]\n",
      "Checking accuracy on test set\n",
      "Got 687 / 1000 correct (68.70)\n",
      "95 epoch,   500 iteration, loss:0.199\n",
      "Checking accuracy on test set\n",
      "Got 700 / 1000 correct (70.00)\n",
      "95 epoch,  1000 iteration, loss:0.176\n",
      " num 94 epoch \n",
      "####### Training Loss #######\n",
      "[0.18344408]\n",
      "Checking accuracy on test set\n",
      "Got 680 / 1000 correct (68.00)\n",
      "96 epoch,   500 iteration, loss:0.187\n",
      "Checking accuracy on test set\n",
      "Got 684 / 1000 correct (68.40)\n",
      "96 epoch,  1000 iteration, loss:0.182\n",
      " num 95 epoch \n",
      "####### Training Loss #######\n",
      "[0.18286465]\n",
      "Checking accuracy on test set\n",
      "Got 654 / 1000 correct (65.40)\n",
      "97 epoch,   500 iteration, loss:0.184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking accuracy on test set\n",
      "Got 704 / 1000 correct (70.40)\n",
      "97 epoch,  1000 iteration, loss:0.191\n",
      " num 96 epoch \n",
      "####### Training Loss #######\n",
      "[0.1865115]\n",
      "Checking accuracy on test set\n",
      "Got 694 / 1000 correct (69.40)\n",
      "98 epoch,   500 iteration, loss:0.190\n",
      "Checking accuracy on test set\n",
      "Got 673 / 1000 correct (67.30)\n",
      "98 epoch,  1000 iteration, loss:0.183\n",
      " num 97 epoch \n",
      "####### Training Loss #######\n",
      "[0.19183446]\n",
      "Checking accuracy on test set\n",
      "Got 683 / 1000 correct (68.30)\n",
      "99 epoch,   500 iteration, loss:0.199\n",
      "Checking accuracy on test set\n",
      "Got 681 / 1000 correct (68.10)\n",
      "99 epoch,  1000 iteration, loss:0.194\n",
      " num 98 epoch \n",
      "####### Training Loss #######\n",
      "[0.19505118]\n",
      "Checking accuracy on test set\n",
      "Got 682 / 1000 correct (68.20)\n",
      "100 epoch,   500 iteration, loss:0.168\n",
      "Checking accuracy on test set\n",
      "Got 689 / 1000 correct (68.90)\n",
      "100 epoch,  1000 iteration, loss:0.165\n",
      " num 99 epoch \n",
      "####### Training Loss #######\n",
      "[0.16596961]\n",
      "Checking accuracy on test set\n",
      "Got 693 / 1000 correct (69.30)\n",
      "101 epoch,   500 iteration, loss:0.160\n",
      "Checking accuracy on test set\n",
      "Got 694 / 1000 correct (69.40)\n",
      "101 epoch,  1000 iteration, loss:0.173\n",
      " num 100 epoch \n",
      "####### Training Loss #######\n",
      "[0.16908471]\n",
      "Checking accuracy on test set\n",
      "Got 663 / 1000 correct (66.30)\n",
      "102 epoch,   500 iteration, loss:0.183\n",
      "Checking accuracy on test set\n",
      "Got 692 / 1000 correct (69.20)\n",
      "102 epoch,  1000 iteration, loss:0.192\n",
      " num 101 epoch \n",
      "####### Training Loss #######\n",
      "[0.1866975]\n",
      "Checking accuracy on test set\n",
      "Got 695 / 1000 correct (69.50)\n",
      "103 epoch,   500 iteration, loss:0.145\n",
      "Checking accuracy on test set\n",
      "Got 686 / 1000 correct (68.60)\n",
      "103 epoch,  1000 iteration, loss:0.141\n",
      " num 102 epoch \n",
      "####### Training Loss #######\n",
      "[0.1454368]\n",
      "Checking accuracy on test set\n",
      "Got 672 / 1000 correct (67.20)\n",
      "104 epoch,   500 iteration, loss:0.151\n",
      "Checking accuracy on test set\n",
      "Got 696 / 1000 correct (69.60)\n",
      "104 epoch,  1000 iteration, loss:0.155\n",
      " num 103 epoch \n",
      "####### Training Loss #######\n",
      "[0.15217202]\n",
      "Checking accuracy on test set\n",
      "Got 694 / 1000 correct (69.40)\n",
      "105 epoch,   500 iteration, loss:0.164\n",
      "Checking accuracy on test set\n",
      "Got 697 / 1000 correct (69.70)\n",
      "105 epoch,  1000 iteration, loss:0.165\n",
      " num 104 epoch \n",
      "####### Training Loss #######\n",
      "[0.16676676]\n",
      "Checking accuracy on test set\n",
      "Got 653 / 1000 correct (65.30)\n",
      "106 epoch,   500 iteration, loss:0.183\n",
      "Checking accuracy on test set\n",
      "Got 694 / 1000 correct (69.40)\n",
      "106 epoch,  1000 iteration, loss:0.147\n",
      " num 105 epoch \n",
      "####### Training Loss #######\n",
      "[0.16969227]\n",
      "Checking accuracy on test set\n",
      "Got 689 / 1000 correct (68.90)\n",
      "107 epoch,   500 iteration, loss:0.174\n",
      "Checking accuracy on test set\n",
      "Got 686 / 1000 correct (68.60)\n",
      "107 epoch,  1000 iteration, loss:0.182\n",
      " num 106 epoch \n",
      "####### Training Loss #######\n",
      "[0.1712175]\n",
      "Checking accuracy on test set\n",
      "Got 681 / 1000 correct (68.10)\n",
      "108 epoch,   500 iteration, loss:0.140\n",
      "Checking accuracy on test set\n",
      "Got 678 / 1000 correct (67.80)\n",
      "108 epoch,  1000 iteration, loss:0.144\n",
      " num 107 epoch \n",
      "####### Training Loss #######\n",
      "[0.14635819]\n",
      "Checking accuracy on test set\n",
      "Got 666 / 1000 correct (66.60)\n",
      "109 epoch,   500 iteration, loss:0.151\n",
      "Checking accuracy on test set\n",
      "Got 688 / 1000 correct (68.80)\n",
      "109 epoch,  1000 iteration, loss:0.143\n",
      " num 108 epoch \n",
      "####### Training Loss #######\n",
      "[0.14415864]\n",
      "Checking accuracy on test set\n",
      "Got 649 / 1000 correct (64.90)\n",
      "110 epoch,   500 iteration, loss:0.171\n",
      "Checking accuracy on test set\n",
      "Got 692 / 1000 correct (69.20)\n",
      "110 epoch,  1000 iteration, loss:0.157\n",
      " num 109 epoch \n",
      "####### Training Loss #######\n",
      "[0.15850623]\n",
      "Checking accuracy on test set\n",
      "Got 688 / 1000 correct (68.80)\n",
      "111 epoch,   500 iteration, loss:0.163\n",
      "Checking accuracy on test set\n",
      "Got 697 / 1000 correct (69.70)\n",
      "111 epoch,  1000 iteration, loss:0.137\n",
      " num 110 epoch \n",
      "####### Training Loss #######\n",
      "[0.14956671]\n",
      "Checking accuracy on test set\n",
      "Got 683 / 1000 correct (68.30)\n",
      "112 epoch,   500 iteration, loss:0.118\n",
      "Checking accuracy on test set\n",
      "Got 689 / 1000 correct (68.90)\n",
      "112 epoch,  1000 iteration, loss:0.165\n",
      " num 111 epoch \n",
      "####### Training Loss #######\n",
      "[0.13879673]\n",
      "Checking accuracy on test set\n",
      "Got 683 / 1000 correct (68.30)\n",
      "113 epoch,   500 iteration, loss:0.145\n",
      "Checking accuracy on test set\n",
      "Got 716 / 1000 correct (71.60)\n",
      "113 epoch,  1000 iteration, loss:0.140\n",
      " num 112 epoch \n",
      "####### Training Loss #######\n",
      "[0.14206881]\n",
      "Checking accuracy on test set\n",
      "Got 689 / 1000 correct (68.90)\n",
      "114 epoch,   500 iteration, loss:0.137\n",
      "Checking accuracy on test set\n",
      "Got 704 / 1000 correct (70.40)\n",
      "114 epoch,  1000 iteration, loss:0.140\n",
      " num 113 epoch \n",
      "####### Training Loss #######\n",
      "[0.13377679]\n",
      "Checking accuracy on test set\n",
      "Got 685 / 1000 correct (68.50)\n",
      "115 epoch,   500 iteration, loss:0.163\n",
      "Checking accuracy on test set\n",
      "Got 701 / 1000 correct (70.10)\n",
      "115 epoch,  1000 iteration, loss:0.159\n",
      " num 114 epoch \n",
      "####### Training Loss #######\n",
      "[0.16331284]\n",
      "Checking accuracy on test set\n",
      "Got 701 / 1000 correct (70.10)\n",
      "116 epoch,   500 iteration, loss:0.157\n",
      "Checking accuracy on test set\n",
      "Got 687 / 1000 correct (68.70)\n",
      "116 epoch,  1000 iteration, loss:0.159\n",
      " num 115 epoch \n",
      "####### Training Loss #######\n",
      "[0.15460835]\n",
      "Checking accuracy on test set\n",
      "Got 661 / 1000 correct (66.10)\n",
      "117 epoch,   500 iteration, loss:0.152\n",
      "Checking accuracy on test set\n",
      "Got 692 / 1000 correct (69.20)\n",
      "117 epoch,  1000 iteration, loss:0.149\n",
      " num 116 epoch \n",
      "####### Training Loss #######\n",
      "[0.14381046]\n",
      "Checking accuracy on test set\n",
      "Got 704 / 1000 correct (70.40)\n",
      "118 epoch,   500 iteration, loss:0.150\n",
      "Checking accuracy on test set\n",
      "Got 715 / 1000 correct (71.50)\n",
      "118 epoch,  1000 iteration, loss:0.159\n",
      " num 117 epoch \n",
      "####### Training Loss #######\n",
      "[0.15921563]\n",
      "Checking accuracy on test set\n",
      "Got 686 / 1000 correct (68.60)\n",
      "119 epoch,   500 iteration, loss:0.126\n",
      "Checking accuracy on test set\n",
      "Got 709 / 1000 correct (70.90)\n",
      "119 epoch,  1000 iteration, loss:0.165\n",
      " num 118 epoch \n",
      "####### Training Loss #######\n",
      "[0.14328149]\n",
      "Checking accuracy on test set\n",
      "Got 687 / 1000 correct (68.70)\n",
      "120 epoch,   500 iteration, loss:0.126\n",
      "Checking accuracy on test set\n",
      "Got 702 / 1000 correct (70.20)\n",
      "120 epoch,  1000 iteration, loss:0.132\n",
      " num 119 epoch \n",
      "####### Training Loss #######\n",
      "[0.13117722]\n",
      "Checking accuracy on test set\n",
      "Got 693 / 1000 correct (69.30)\n",
      "121 epoch,   500 iteration, loss:0.148\n",
      "Checking accuracy on test set\n",
      "Got 672 / 1000 correct (67.20)\n",
      "121 epoch,  1000 iteration, loss:0.121\n",
      " num 120 epoch \n",
      "####### Training Loss #######\n",
      "[0.14153396]\n",
      "Checking accuracy on test set\n",
      "Got 674 / 1000 correct (67.40)\n",
      "122 epoch,   500 iteration, loss:0.137\n",
      "Checking accuracy on test set\n",
      "Got 695 / 1000 correct (69.50)\n",
      "122 epoch,  1000 iteration, loss:0.144\n",
      " num 121 epoch \n",
      "####### Training Loss #######\n",
      "[0.14361136]\n",
      "Checking accuracy on test set\n",
      "Got 685 / 1000 correct (68.50)\n",
      "123 epoch,   500 iteration, loss:0.133\n",
      "Checking accuracy on test set\n",
      "Got 692 / 1000 correct (69.20)\n",
      "123 epoch,  1000 iteration, loss:0.124\n",
      " num 122 epoch \n",
      "####### Training Loss #######\n",
      "[0.12656724]\n",
      "Checking accuracy on test set\n",
      "Got 679 / 1000 correct (67.90)\n",
      "124 epoch,   500 iteration, loss:0.129\n",
      "Checking accuracy on test set\n",
      "Got 683 / 1000 correct (68.30)\n",
      "124 epoch,  1000 iteration, loss:0.154\n",
      " num 123 epoch \n",
      "####### Training Loss #######\n",
      "[0.14057188]\n",
      "Checking accuracy on test set\n",
      "Got 669 / 1000 correct (66.90)\n",
      "125 epoch,   500 iteration, loss:0.121\n",
      "Checking accuracy on test set\n",
      "Got 717 / 1000 correct (71.70)\n",
      "125 epoch,  1000 iteration, loss:0.114\n",
      " num 124 epoch \n",
      "####### Training Loss #######\n",
      "[0.11988715]\n",
      "Checking accuracy on test set\n",
      "Got 694 / 1000 correct (69.40)\n",
      "126 epoch,   500 iteration, loss:0.153\n",
      "Checking accuracy on test set\n",
      "Got 707 / 1000 correct (70.70)\n",
      "126 epoch,  1000 iteration, loss:0.119\n",
      " num 125 epoch \n",
      "####### Training Loss #######\n",
      "[0.1399034]\n",
      "Checking accuracy on test set\n",
      "Got 703 / 1000 correct (70.30)\n",
      "127 epoch,   500 iteration, loss:0.145\n",
      "Checking accuracy on test set\n",
      "Got 686 / 1000 correct (68.60)\n",
      "127 epoch,  1000 iteration, loss:0.161\n",
      " num 126 epoch \n",
      "####### Training Loss #######\n",
      "[0.14979274]\n",
      "Checking accuracy on test set\n",
      "Got 684 / 1000 correct (68.40)\n",
      "128 epoch,   500 iteration, loss:0.154\n",
      "Checking accuracy on test set\n",
      "Got 687 / 1000 correct (68.70)\n",
      "128 epoch,  1000 iteration, loss:0.106\n",
      " num 127 epoch \n",
      "####### Training Loss #######\n",
      "[0.12842004]\n",
      "Checking accuracy on test set\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 701 / 1000 correct (70.10)\n",
      "129 epoch,   500 iteration, loss:0.137\n",
      "Checking accuracy on test set\n",
      "Got 664 / 1000 correct (66.40)\n",
      "129 epoch,  1000 iteration, loss:0.128\n",
      " num 128 epoch \n",
      "####### Training Loss #######\n",
      "[0.13351032]\n",
      "Checking accuracy on test set\n",
      "Got 696 / 1000 correct (69.60)\n",
      "130 epoch,   500 iteration, loss:0.158\n",
      "Checking accuracy on test set\n",
      "Got 704 / 1000 correct (70.40)\n",
      "130 epoch,  1000 iteration, loss:0.145\n",
      " num 129 epoch \n",
      "####### Training Loss #######\n",
      "[0.14621312]\n",
      "Checking accuracy on test set\n",
      "Got 666 / 1000 correct (66.60)\n",
      "131 epoch,   500 iteration, loss:0.126\n",
      "Checking accuracy on test set\n",
      "Got 702 / 1000 correct (70.20)\n",
      "131 epoch,  1000 iteration, loss:0.137\n",
      " num 130 epoch \n",
      "####### Training Loss #######\n",
      "[0.1310518]\n",
      "Checking accuracy on test set\n",
      "Got 683 / 1000 correct (68.30)\n",
      "132 epoch,   500 iteration, loss:0.118\n",
      "Checking accuracy on test set\n",
      "Got 696 / 1000 correct (69.60)\n",
      "132 epoch,  1000 iteration, loss:0.097\n",
      " num 131 epoch \n",
      "####### Training Loss #######\n",
      "[0.11981686]\n",
      "Checking accuracy on test set\n",
      "Got 685 / 1000 correct (68.50)\n",
      "133 epoch,   500 iteration, loss:0.126\n",
      "Checking accuracy on test set\n",
      "Got 690 / 1000 correct (69.00)\n",
      "133 epoch,  1000 iteration, loss:0.106\n",
      " num 132 epoch \n",
      "####### Training Loss #######\n",
      "[0.11811176]\n",
      "Checking accuracy on test set\n",
      "Got 690 / 1000 correct (69.00)\n",
      "134 epoch,   500 iteration, loss:0.137\n",
      "Checking accuracy on test set\n",
      "Got 707 / 1000 correct (70.70)\n",
      "134 epoch,  1000 iteration, loss:0.126\n",
      " num 133 epoch \n",
      "####### Training Loss #######\n",
      "[0.1258033]\n",
      "Checking accuracy on test set\n",
      "Got 659 / 1000 correct (65.90)\n",
      "135 epoch,   500 iteration, loss:0.122\n",
      "Checking accuracy on test set\n",
      "Got 676 / 1000 correct (67.60)\n",
      "135 epoch,  1000 iteration, loss:0.140\n",
      " num 134 epoch \n",
      "####### Training Loss #######\n",
      "[0.12749011]\n",
      "Checking accuracy on test set\n",
      "Got 685 / 1000 correct (68.50)\n",
      "136 epoch,   500 iteration, loss:0.132\n",
      "Checking accuracy on test set\n",
      "Got 706 / 1000 correct (70.60)\n",
      "136 epoch,  1000 iteration, loss:0.147\n",
      " num 135 epoch \n",
      "####### Training Loss #######\n",
      "[0.13839861]\n",
      "Checking accuracy on test set\n",
      "Got 696 / 1000 correct (69.60)\n",
      "137 epoch,   500 iteration, loss:0.134\n",
      "Checking accuracy on test set\n",
      "Got 706 / 1000 correct (70.60)\n",
      "137 epoch,  1000 iteration, loss:0.112\n",
      " num 136 epoch \n",
      "####### Training Loss #######\n",
      "[0.11971345]\n",
      "Checking accuracy on test set\n",
      "Got 707 / 1000 correct (70.70)\n",
      "138 epoch,   500 iteration, loss:0.130\n",
      "Checking accuracy on test set\n",
      "Got 702 / 1000 correct (70.20)\n",
      "138 epoch,  1000 iteration, loss:0.111\n",
      " num 137 epoch \n",
      "####### Training Loss #######\n",
      "[0.12518006]\n",
      "Checking accuracy on test set\n",
      "Got 677 / 1000 correct (67.70)\n",
      "139 epoch,   500 iteration, loss:0.135\n",
      "Checking accuracy on test set\n",
      "Got 701 / 1000 correct (70.10)\n",
      "139 epoch,  1000 iteration, loss:0.126\n",
      " num 138 epoch \n",
      "####### Training Loss #######\n",
      "[0.13011603]\n",
      "Checking accuracy on test set\n",
      "Got 695 / 1000 correct (69.50)\n",
      "140 epoch,   500 iteration, loss:0.131\n",
      "Checking accuracy on test set\n",
      "Got 713 / 1000 correct (71.30)\n",
      "140 epoch,  1000 iteration, loss:0.125\n",
      " num 139 epoch \n",
      "####### Training Loss #######\n",
      "[0.12599921]\n",
      "Checking accuracy on test set\n",
      "Got 695 / 1000 correct (69.50)\n",
      "141 epoch,   500 iteration, loss:0.148\n",
      "Checking accuracy on test set\n",
      "Got 699 / 1000 correct (69.90)\n",
      "141 epoch,  1000 iteration, loss:0.108\n",
      " num 140 epoch \n",
      "####### Training Loss #######\n",
      "[0.13011176]\n",
      "Checking accuracy on test set\n",
      "Got 697 / 1000 correct (69.70)\n",
      "142 epoch,   500 iteration, loss:0.102\n",
      "Checking accuracy on test set\n",
      "Got 693 / 1000 correct (69.30)\n",
      "142 epoch,  1000 iteration, loss:0.125\n",
      " num 141 epoch \n",
      "####### Training Loss #######\n",
      "[0.11650288]\n",
      "Checking accuracy on test set\n",
      "Got 685 / 1000 correct (68.50)\n",
      "143 epoch,   500 iteration, loss:0.137\n",
      "Checking accuracy on test set\n",
      "Got 698 / 1000 correct (69.80)\n",
      "143 epoch,  1000 iteration, loss:0.144\n",
      " num 142 epoch \n",
      "####### Training Loss #######\n",
      "[0.13659852]\n",
      "Checking accuracy on test set\n",
      "Got 698 / 1000 correct (69.80)\n",
      "144 epoch,   500 iteration, loss:0.123\n",
      "Checking accuracy on test set\n",
      "Got 695 / 1000 correct (69.50)\n",
      "144 epoch,  1000 iteration, loss:0.117\n",
      " num 143 epoch \n",
      "####### Training Loss #######\n",
      "[0.11853336]\n",
      "Checking accuracy on test set\n",
      "Got 685 / 1000 correct (68.50)\n",
      "145 epoch,   500 iteration, loss:0.113\n",
      "Checking accuracy on test set\n",
      "Got 689 / 1000 correct (68.90)\n",
      "145 epoch,  1000 iteration, loss:0.134\n",
      " num 144 epoch \n",
      "####### Training Loss #######\n",
      "[0.12845607]\n",
      "Checking accuracy on test set\n",
      "Got 666 / 1000 correct (66.60)\n",
      "146 epoch,   500 iteration, loss:0.114\n",
      "Checking accuracy on test set\n",
      "Got 686 / 1000 correct (68.60)\n",
      "146 epoch,  1000 iteration, loss:0.132\n",
      " num 145 epoch \n",
      "####### Training Loss #######\n",
      "[0.12155861]\n",
      "Checking accuracy on test set\n",
      "Got 677 / 1000 correct (67.70)\n",
      "147 epoch,   500 iteration, loss:0.138\n",
      "Checking accuracy on test set\n",
      "Got 683 / 1000 correct (68.30)\n",
      "147 epoch,  1000 iteration, loss:0.146\n",
      " num 146 epoch \n",
      "####### Training Loss #######\n",
      "[0.13905169]\n",
      "Checking accuracy on test set\n",
      "Got 681 / 1000 correct (68.10)\n",
      "148 epoch,   500 iteration, loss:0.105\n",
      "Checking accuracy on test set\n",
      "Got 687 / 1000 correct (68.70)\n",
      "148 epoch,  1000 iteration, loss:0.137\n",
      " num 147 epoch \n",
      "####### Training Loss #######\n",
      "[0.11865242]\n",
      "Checking accuracy on test set\n",
      "Got 691 / 1000 correct (69.10)\n",
      "149 epoch,   500 iteration, loss:0.131\n",
      "Checking accuracy on test set\n",
      "Got 700 / 1000 correct (70.00)\n",
      "149 epoch,  1000 iteration, loss:0.122\n",
      " num 148 epoch \n",
      "####### Training Loss #######\n",
      "[0.12449953]\n",
      "Checking accuracy on test set\n",
      "Got 699 / 1000 correct (69.90)\n",
      "150 epoch,   500 iteration, loss:0.105\n",
      "Checking accuracy on test set\n",
      "Got 693 / 1000 correct (69.30)\n",
      "150 epoch,  1000 iteration, loss:0.140\n",
      " num 149 epoch \n",
      "####### Training Loss #######\n",
      "[0.13253134]\n",
      "Checking accuracy on test set\n",
      "Got 679 / 1000 correct (67.90)\n",
      "151 epoch,   500 iteration, loss:0.135\n",
      "Checking accuracy on test set\n",
      "Got 670 / 1000 correct (67.00)\n",
      "151 epoch,  1000 iteration, loss:0.134\n",
      " num 150 epoch \n",
      "####### Training Loss #######\n",
      "[0.12962368]\n",
      "Checking accuracy on test set\n",
      "Got 671 / 1000 correct (67.10)\n",
      "152 epoch,   500 iteration, loss:0.104\n",
      "Checking accuracy on test set\n",
      "Got 694 / 1000 correct (69.40)\n",
      "152 epoch,  1000 iteration, loss:0.111\n",
      " num 151 epoch \n",
      "####### Training Loss #######\n",
      "[0.11628019]\n",
      "Checking accuracy on test set\n",
      "Got 678 / 1000 correct (67.80)\n",
      "153 epoch,   500 iteration, loss:0.108\n",
      "Checking accuracy on test set\n",
      "Got 699 / 1000 correct (69.90)\n",
      "153 epoch,  1000 iteration, loss:0.110\n",
      " num 152 epoch \n",
      "####### Training Loss #######\n",
      "[0.11714698]\n",
      "Checking accuracy on test set\n",
      "Got 681 / 1000 correct (68.10)\n",
      "154 epoch,   500 iteration, loss:0.129\n",
      "Checking accuracy on test set\n",
      "Got 701 / 1000 correct (70.10)\n",
      "154 epoch,  1000 iteration, loss:0.114\n",
      " num 153 epoch \n",
      "####### Training Loss #######\n",
      "[0.1165395]\n",
      "Checking accuracy on test set\n",
      "Got 686 / 1000 correct (68.60)\n",
      "155 epoch,   500 iteration, loss:0.090\n",
      "Checking accuracy on test set\n",
      "Got 677 / 1000 correct (67.70)\n",
      "155 epoch,  1000 iteration, loss:0.105\n",
      " num 154 epoch \n",
      "####### Training Loss #######\n",
      "[0.10249924]\n",
      "Checking accuracy on test set\n",
      "Got 687 / 1000 correct (68.70)\n",
      "156 epoch,   500 iteration, loss:0.121\n",
      "Checking accuracy on test set\n",
      "Got 690 / 1000 correct (69.00)\n",
      "156 epoch,  1000 iteration, loss:0.111\n",
      " num 155 epoch \n",
      "####### Training Loss #######\n",
      "[0.1149889]\n",
      "Checking accuracy on test set\n",
      "Got 679 / 1000 correct (67.90)\n",
      "157 epoch,   500 iteration, loss:0.102\n",
      "Checking accuracy on test set\n",
      "Got 696 / 1000 correct (69.60)\n",
      "157 epoch,  1000 iteration, loss:0.104\n",
      " num 156 epoch \n",
      "####### Training Loss #######\n",
      "[0.10762857]\n",
      "Checking accuracy on test set\n",
      "Got 698 / 1000 correct (69.80)\n",
      "158 epoch,   500 iteration, loss:0.121\n",
      "Checking accuracy on test set\n",
      "Got 685 / 1000 correct (68.50)\n",
      "158 epoch,  1000 iteration, loss:0.123\n",
      " num 157 epoch \n",
      "####### Training Loss #######\n",
      "[0.12329932]\n",
      "Checking accuracy on test set\n",
      "Got 682 / 1000 correct (68.20)\n",
      "159 epoch,   500 iteration, loss:0.119\n",
      "Checking accuracy on test set\n",
      "Got 684 / 1000 correct (68.40)\n",
      "159 epoch,  1000 iteration, loss:0.117\n",
      " num 158 epoch \n",
      "####### Training Loss #######\n",
      "[0.11915263]\n",
      "Checking accuracy on test set\n",
      "Got 680 / 1000 correct (68.00)\n",
      "160 epoch,   500 iteration, loss:0.128\n",
      "Checking accuracy on test set\n",
      "Got 691 / 1000 correct (69.10)\n",
      "160 epoch,  1000 iteration, loss:0.107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " num 159 epoch \n",
      "####### Training Loss #######\n",
      "[0.11819376]\n",
      "Checking accuracy on test set\n",
      "Got 683 / 1000 correct (68.30)\n",
      "161 epoch,   500 iteration, loss:0.123\n",
      "Checking accuracy on test set\n",
      "Got 684 / 1000 correct (68.40)\n",
      "161 epoch,  1000 iteration, loss:0.121\n",
      " num 160 epoch \n",
      "####### Training Loss #######\n",
      "[0.120248]\n",
      "Checking accuracy on test set\n",
      "Got 683 / 1000 correct (68.30)\n",
      "162 epoch,   500 iteration, loss:0.096\n",
      "Checking accuracy on test set\n",
      "Got 698 / 1000 correct (69.80)\n",
      "162 epoch,  1000 iteration, loss:0.118\n",
      " num 161 epoch \n",
      "####### Training Loss #######\n",
      "[0.10649652]\n",
      "Checking accuracy on test set\n",
      "Got 686 / 1000 correct (68.60)\n",
      "163 epoch,   500 iteration, loss:0.112\n",
      "Checking accuracy on test set\n",
      "Got 685 / 1000 correct (68.50)\n",
      "163 epoch,  1000 iteration, loss:0.117\n",
      " num 162 epoch \n",
      "####### Training Loss #######\n",
      "[0.11346504]\n",
      "Checking accuracy on test set\n",
      "Got 699 / 1000 correct (69.90)\n",
      "164 epoch,   500 iteration, loss:0.123\n",
      "Checking accuracy on test set\n",
      "Got 701 / 1000 correct (70.10)\n",
      "164 epoch,  1000 iteration, loss:0.135\n",
      " num 163 epoch \n",
      "####### Training Loss #######\n",
      "[0.12735036]\n",
      "Checking accuracy on test set\n",
      "Got 666 / 1000 correct (66.60)\n",
      "165 epoch,   500 iteration, loss:0.114\n",
      "Checking accuracy on test set\n",
      "Got 688 / 1000 correct (68.80)\n",
      "165 epoch,  1000 iteration, loss:0.100\n",
      " num 164 epoch \n",
      "####### Training Loss #######\n",
      "[0.11367743]\n",
      "Checking accuracy on test set\n",
      "Got 704 / 1000 correct (70.40)\n",
      "166 epoch,   500 iteration, loss:0.088\n",
      "Checking accuracy on test set\n",
      "Got 692 / 1000 correct (69.20)\n",
      "166 epoch,  1000 iteration, loss:0.111\n",
      " num 165 epoch \n",
      "####### Training Loss #######\n",
      "[0.10806084]\n",
      "Checking accuracy on test set\n",
      "Got 675 / 1000 correct (67.50)\n",
      "167 epoch,   500 iteration, loss:0.101\n",
      "Checking accuracy on test set\n",
      "Got 674 / 1000 correct (67.40)\n",
      "167 epoch,  1000 iteration, loss:0.112\n",
      " num 166 epoch \n",
      "####### Training Loss #######\n",
      "[0.10767721]\n",
      "Checking accuracy on test set\n",
      "Got 692 / 1000 correct (69.20)\n",
      "168 epoch,   500 iteration, loss:0.105\n",
      "Checking accuracy on test set\n",
      "Got 712 / 1000 correct (71.20)\n",
      "168 epoch,  1000 iteration, loss:0.104\n",
      " num 167 epoch \n",
      "####### Training Loss #######\n",
      "[0.10349303]\n",
      "Checking accuracy on test set\n",
      "Got 674 / 1000 correct (67.40)\n",
      "169 epoch,   500 iteration, loss:0.128\n",
      "Checking accuracy on test set\n",
      "Got 714 / 1000 correct (71.40)\n",
      "169 epoch,  1000 iteration, loss:0.108\n",
      " num 168 epoch \n",
      "####### Training Loss #######\n",
      "[0.11788972]\n",
      "Checking accuracy on test set\n",
      "Got 690 / 1000 correct (69.00)\n",
      "170 epoch,   500 iteration, loss:0.122\n",
      "Checking accuracy on test set\n",
      "Got 699 / 1000 correct (69.90)\n",
      "170 epoch,  1000 iteration, loss:0.122\n",
      " num 169 epoch \n",
      "####### Training Loss #######\n",
      "[0.12577197]\n",
      "Checking accuracy on test set\n",
      "Got 677 / 1000 correct (67.70)\n",
      "171 epoch,   500 iteration, loss:0.097\n",
      "Checking accuracy on test set\n",
      "Got 695 / 1000 correct (69.50)\n",
      "171 epoch,  1000 iteration, loss:0.098\n",
      " num 170 epoch \n",
      "####### Training Loss #######\n",
      "[0.10078588]\n",
      "Checking accuracy on test set\n",
      "Got 698 / 1000 correct (69.80)\n",
      "172 epoch,   500 iteration, loss:0.107\n",
      "Checking accuracy on test set\n",
      "Got 709 / 1000 correct (70.90)\n",
      "172 epoch,  1000 iteration, loss:0.086\n",
      " num 171 epoch \n",
      "####### Training Loss #######\n",
      "[0.10106885]\n",
      "Checking accuracy on test set\n",
      "Got 673 / 1000 correct (67.30)\n",
      "173 epoch,   500 iteration, loss:0.098\n",
      "Checking accuracy on test set\n",
      "Got 686 / 1000 correct (68.60)\n",
      "173 epoch,  1000 iteration, loss:0.104\n",
      " num 172 epoch \n",
      "####### Training Loss #######\n",
      "[0.11078262]\n",
      "Checking accuracy on test set\n",
      "Got 685 / 1000 correct (68.50)\n",
      "174 epoch,   500 iteration, loss:0.104\n",
      "Checking accuracy on test set\n",
      "Got 703 / 1000 correct (70.30)\n",
      "174 epoch,  1000 iteration, loss:0.137\n",
      " num 173 epoch \n",
      "####### Training Loss #######\n",
      "[0.11651993]\n",
      "Checking accuracy on test set\n",
      "Got 681 / 1000 correct (68.10)\n",
      "175 epoch,   500 iteration, loss:0.094\n",
      "Checking accuracy on test set\n",
      "Got 709 / 1000 correct (70.90)\n",
      "175 epoch,  1000 iteration, loss:0.107\n",
      " num 174 epoch \n",
      "####### Training Loss #######\n",
      "[0.10036753]\n",
      "Checking accuracy on test set\n",
      "Got 693 / 1000 correct (69.30)\n",
      "176 epoch,   500 iteration, loss:0.081\n",
      "Checking accuracy on test set\n",
      "Got 689 / 1000 correct (68.90)\n",
      "176 epoch,  1000 iteration, loss:0.107\n",
      " num 175 epoch \n",
      "####### Training Loss #######\n",
      "[0.10029124]\n",
      "Checking accuracy on test set\n",
      "Got 695 / 1000 correct (69.50)\n",
      "177 epoch,   500 iteration, loss:0.122\n",
      "Checking accuracy on test set\n",
      "Got 709 / 1000 correct (70.90)\n",
      "177 epoch,  1000 iteration, loss:0.099\n",
      " num 176 epoch \n",
      "####### Training Loss #######\n",
      "[0.11173783]\n",
      "Checking accuracy on test set\n",
      "Got 662 / 1000 correct (66.20)\n",
      "178 epoch,   500 iteration, loss:0.100\n",
      "Checking accuracy on test set\n",
      "Got 701 / 1000 correct (70.10)\n",
      "178 epoch,  1000 iteration, loss:0.105\n",
      " num 177 epoch \n",
      "####### Training Loss #######\n",
      "[0.10479174]\n",
      "Checking accuracy on test set\n",
      "Got 710 / 1000 correct (71.00)\n",
      "179 epoch,   500 iteration, loss:0.105\n",
      "Checking accuracy on test set\n",
      "Got 699 / 1000 correct (69.90)\n",
      "179 epoch,  1000 iteration, loss:0.111\n",
      " num 178 epoch \n",
      "####### Training Loss #######\n",
      "[0.10956737]\n",
      "Checking accuracy on test set\n",
      "Got 677 / 1000 correct (67.70)\n",
      "180 epoch,   500 iteration, loss:0.099\n",
      "Checking accuracy on test set\n",
      "Got 688 / 1000 correct (68.80)\n",
      "180 epoch,  1000 iteration, loss:0.127\n",
      " num 179 epoch \n",
      "####### Training Loss #######\n",
      "[0.11765882]\n",
      "Checking accuracy on test set\n",
      "Got 686 / 1000 correct (68.60)\n",
      "181 epoch,   500 iteration, loss:0.108\n",
      "Checking accuracy on test set\n",
      "Got 691 / 1000 correct (69.10)\n",
      "181 epoch,  1000 iteration, loss:0.114\n",
      " num 180 epoch \n",
      "####### Training Loss #######\n",
      "[0.1163298]\n",
      "Checking accuracy on test set\n",
      "Got 695 / 1000 correct (69.50)\n",
      "182 epoch,   500 iteration, loss:0.103\n",
      "Checking accuracy on test set\n",
      "Got 683 / 1000 correct (68.30)\n",
      "182 epoch,  1000 iteration, loss:0.120\n",
      " num 181 epoch \n",
      "####### Training Loss #######\n",
      "[0.10983672]\n",
      "Checking accuracy on test set\n",
      "Got 682 / 1000 correct (68.20)\n",
      "183 epoch,   500 iteration, loss:0.109\n",
      "Checking accuracy on test set\n",
      "Got 710 / 1000 correct (71.00)\n",
      "183 epoch,  1000 iteration, loss:0.099\n",
      " num 182 epoch \n",
      "####### Training Loss #######\n",
      "[0.10263365]\n",
      "Checking accuracy on test set\n",
      "Got 677 / 1000 correct (67.70)\n",
      "184 epoch,   500 iteration, loss:0.106\n",
      "Checking accuracy on test set\n",
      "Got 705 / 1000 correct (70.50)\n",
      "184 epoch,  1000 iteration, loss:0.076\n",
      " num 183 epoch \n",
      "####### Training Loss #######\n",
      "[0.09286304]\n",
      "Checking accuracy on test set\n",
      "Got 686 / 1000 correct (68.60)\n",
      "185 epoch,   500 iteration, loss:0.117\n",
      "Checking accuracy on test set\n",
      "Got 698 / 1000 correct (69.80)\n",
      "185 epoch,  1000 iteration, loss:0.110\n",
      " num 184 epoch \n",
      "####### Training Loss #######\n",
      "[0.11185564]\n",
      "Checking accuracy on test set\n",
      "Got 689 / 1000 correct (68.90)\n",
      "186 epoch,   500 iteration, loss:0.110\n",
      "Checking accuracy on test set\n",
      "Got 702 / 1000 correct (70.20)\n",
      "186 epoch,  1000 iteration, loss:0.113\n",
      " num 185 epoch \n",
      "####### Training Loss #######\n",
      "[0.11146688]\n",
      "Checking accuracy on test set\n",
      "Got 700 / 1000 correct (70.00)\n",
      "187 epoch,   500 iteration, loss:0.107\n",
      "Checking accuracy on test set\n",
      "Got 677 / 1000 correct (67.70)\n",
      "187 epoch,  1000 iteration, loss:0.092\n",
      " num 186 epoch \n",
      "####### Training Loss #######\n",
      "[0.10606788]\n",
      "Checking accuracy on test set\n",
      "Got 685 / 1000 correct (68.50)\n",
      "188 epoch,   500 iteration, loss:0.137\n",
      "Checking accuracy on test set\n",
      "Got 692 / 1000 correct (69.20)\n",
      "188 epoch,  1000 iteration, loss:0.090\n",
      " num 187 epoch \n",
      "####### Training Loss #######\n",
      "[0.11322908]\n",
      "Checking accuracy on test set\n",
      "Got 679 / 1000 correct (67.90)\n",
      "189 epoch,   500 iteration, loss:0.110\n",
      "Checking accuracy on test set\n",
      "Got 685 / 1000 correct (68.50)\n",
      "189 epoch,  1000 iteration, loss:0.112\n",
      " num 188 epoch \n",
      "####### Training Loss #######\n",
      "[0.1061725]\n",
      "Checking accuracy on test set\n",
      "Got 691 / 1000 correct (69.10)\n",
      "190 epoch,   500 iteration, loss:0.104\n",
      "Checking accuracy on test set\n",
      "Got 707 / 1000 correct (70.70)\n",
      "190 epoch,  1000 iteration, loss:0.108\n",
      " num 189 epoch \n",
      "####### Training Loss #######\n",
      "[0.10464446]\n",
      "Checking accuracy on test set\n",
      "Got 708 / 1000 correct (70.80)\n",
      "191 epoch,   500 iteration, loss:0.103\n",
      "Checking accuracy on test set\n",
      "Got 715 / 1000 correct (71.50)\n",
      "191 epoch,  1000 iteration, loss:0.104\n",
      " num 190 epoch \n",
      "####### Training Loss #######\n",
      "[0.10058811]\n",
      "Checking accuracy on test set\n",
      "Got 697 / 1000 correct (69.70)\n",
      "192 epoch,   500 iteration, loss:0.095\n",
      "Checking accuracy on test set\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 704 / 1000 correct (70.40)\n",
      "192 epoch,  1000 iteration, loss:0.115\n",
      " num 191 epoch \n",
      "####### Training Loss #######\n",
      "[0.10970909]\n",
      "Checking accuracy on test set\n",
      "Got 702 / 1000 correct (70.20)\n",
      "193 epoch,   500 iteration, loss:0.112\n",
      "Checking accuracy on test set\n",
      "Got 691 / 1000 correct (69.10)\n",
      "193 epoch,  1000 iteration, loss:0.092\n",
      " num 192 epoch \n",
      "####### Training Loss #######\n",
      "[0.09831027]\n",
      "Checking accuracy on test set\n",
      "Got 686 / 1000 correct (68.60)\n",
      "194 epoch,   500 iteration, loss:0.108\n",
      "Checking accuracy on test set\n",
      "Got 679 / 1000 correct (67.90)\n",
      "194 epoch,  1000 iteration, loss:0.112\n",
      " num 193 epoch \n",
      "####### Training Loss #######\n",
      "[0.11514903]\n",
      "Checking accuracy on test set\n",
      "Got 691 / 1000 correct (69.10)\n",
      "195 epoch,   500 iteration, loss:0.101\n",
      "Checking accuracy on test set\n",
      "Got 680 / 1000 correct (68.00)\n",
      "195 epoch,  1000 iteration, loss:0.082\n",
      " num 194 epoch \n",
      "####### Training Loss #######\n",
      "[0.0945466]\n",
      "Checking accuracy on test set\n",
      "Got 679 / 1000 correct (67.90)\n",
      "196 epoch,   500 iteration, loss:0.089\n",
      "Checking accuracy on test set\n",
      "Got 662 / 1000 correct (66.20)\n",
      "196 epoch,  1000 iteration, loss:0.085\n",
      " num 195 epoch \n",
      "####### Training Loss #######\n",
      "[0.09813823]\n",
      "Checking accuracy on test set\n",
      "Got 686 / 1000 correct (68.60)\n",
      "197 epoch,   500 iteration, loss:0.095\n",
      "Checking accuracy on test set\n",
      "Got 702 / 1000 correct (70.20)\n",
      "197 epoch,  1000 iteration, loss:0.095\n",
      " num 196 epoch \n",
      "####### Training Loss #######\n",
      "[0.09482646]\n",
      "Checking accuracy on test set\n",
      "Got 688 / 1000 correct (68.80)\n",
      "198 epoch,   500 iteration, loss:0.131\n",
      "Checking accuracy on test set\n",
      "Got 686 / 1000 correct (68.60)\n",
      "198 epoch,  1000 iteration, loss:0.124\n",
      " num 197 epoch \n",
      "####### Training Loss #######\n",
      "[0.11950007]\n",
      "Checking accuracy on test set\n",
      "Got 686 / 1000 correct (68.60)\n",
      "199 epoch,   500 iteration, loss:0.105\n",
      "Checking accuracy on test set\n",
      "Got 703 / 1000 correct (70.30)\n",
      "199 epoch,  1000 iteration, loss:0.077\n",
      " num 198 epoch \n",
      "####### Training Loss #######\n",
      "[0.09644752]\n",
      "Checking accuracy on test set\n",
      "Got 694 / 1000 correct (69.40)\n",
      "200 epoch,   500 iteration, loss:0.088\n",
      "Checking accuracy on test set\n",
      "Got 680 / 1000 correct (68.00)\n",
      "200 epoch,  1000 iteration, loss:0.111\n",
      " num 199 epoch \n",
      "####### Training Loss #######\n",
      "[0.09782428]\n",
      "Checking accuracy on test set\n",
      "Got 698 / 1000 correct (69.80)\n",
      "201 epoch,   500 iteration, loss:0.105\n",
      "Checking accuracy on test set\n",
      "Got 692 / 1000 correct (69.20)\n",
      "201 epoch,  1000 iteration, loss:0.116\n",
      " num 200 epoch \n",
      "####### Training Loss #######\n",
      "[0.1073443]\n",
      "Checking accuracy on test set\n",
      "Got 681 / 1000 correct (68.10)\n",
      "202 epoch,   500 iteration, loss:0.102\n",
      "Checking accuracy on test set\n",
      "Got 704 / 1000 correct (70.40)\n",
      "202 epoch,  1000 iteration, loss:0.098\n",
      " num 201 epoch \n",
      "####### Training Loss #######\n",
      "[0.09702125]\n",
      "Checking accuracy on test set\n",
      "Got 690 / 1000 correct (69.00)\n",
      "203 epoch,   500 iteration, loss:0.137\n",
      "Checking accuracy on test set\n",
      "Got 705 / 1000 correct (70.50)\n",
      "203 epoch,  1000 iteration, loss:0.093\n",
      " num 202 epoch \n",
      "####### Training Loss #######\n",
      "[0.10760303]\n",
      "Checking accuracy on test set\n",
      "Got 706 / 1000 correct (70.60)\n",
      "204 epoch,   500 iteration, loss:0.058\n",
      "Checking accuracy on test set\n",
      "Got 686 / 1000 correct (68.60)\n",
      "204 epoch,  1000 iteration, loss:0.130\n",
      " num 203 epoch \n",
      "####### Training Loss #######\n",
      "[0.09773422]\n",
      "Checking accuracy on test set\n",
      "Got 698 / 1000 correct (69.80)\n",
      "205 epoch,   500 iteration, loss:0.110\n",
      "Checking accuracy on test set\n",
      "Got 677 / 1000 correct (67.70)\n",
      "205 epoch,  1000 iteration, loss:0.089\n",
      " num 204 epoch \n",
      "####### Training Loss #######\n",
      "[0.10774273]\n",
      "Checking accuracy on test set\n",
      "Got 689 / 1000 correct (68.90)\n",
      "206 epoch,   500 iteration, loss:0.102\n",
      "Checking accuracy on test set\n",
      "Got 699 / 1000 correct (69.90)\n",
      "206 epoch,  1000 iteration, loss:0.092\n",
      " num 205 epoch \n",
      "####### Training Loss #######\n",
      "[0.09891573]\n",
      "Checking accuracy on test set\n",
      "Got 695 / 1000 correct (69.50)\n",
      "207 epoch,   500 iteration, loss:0.106\n",
      "Checking accuracy on test set\n",
      "Got 695 / 1000 correct (69.50)\n",
      "207 epoch,  1000 iteration, loss:0.089\n",
      " num 206 epoch \n",
      "####### Training Loss #######\n",
      "[0.09626604]\n",
      "Checking accuracy on test set\n",
      "Got 698 / 1000 correct (69.80)\n",
      "208 epoch,   500 iteration, loss:0.102\n",
      "Checking accuracy on test set\n",
      "Got 690 / 1000 correct (69.00)\n",
      "208 epoch,  1000 iteration, loss:0.100\n",
      " num 207 epoch \n",
      "####### Training Loss #######\n",
      "[0.10289352]\n",
      "Checking accuracy on test set\n",
      "Got 699 / 1000 correct (69.90)\n",
      "209 epoch,   500 iteration, loss:0.080\n",
      "Checking accuracy on test set\n",
      "Got 681 / 1000 correct (68.10)\n",
      "209 epoch,  1000 iteration, loss:0.085\n",
      " num 208 epoch \n",
      "####### Training Loss #######\n",
      "[0.09023151]\n",
      "Checking accuracy on test set\n",
      "Got 699 / 1000 correct (69.90)\n",
      "210 epoch,   500 iteration, loss:0.090\n",
      "Checking accuracy on test set\n",
      "Got 717 / 1000 correct (71.70)\n",
      "210 epoch,  1000 iteration, loss:0.104\n",
      " num 209 epoch \n",
      "####### Training Loss #######\n",
      "[0.0926843]\n",
      "Checking accuracy on test set\n",
      "Got 688 / 1000 correct (68.80)\n",
      "211 epoch,   500 iteration, loss:0.098\n",
      "Checking accuracy on test set\n",
      "Got 680 / 1000 correct (68.00)\n",
      "211 epoch,  1000 iteration, loss:0.106\n",
      " num 210 epoch \n",
      "####### Training Loss #######\n",
      "[0.10153683]\n",
      "Checking accuracy on test set\n",
      "Got 688 / 1000 correct (68.80)\n",
      "212 epoch,   500 iteration, loss:0.104\n",
      "Checking accuracy on test set\n",
      "Got 711 / 1000 correct (71.10)\n",
      "212 epoch,  1000 iteration, loss:0.110\n",
      " num 211 epoch \n",
      "####### Training Loss #######\n",
      "[0.10361617]\n",
      "Checking accuracy on test set\n",
      "Got 701 / 1000 correct (70.10)\n",
      "213 epoch,   500 iteration, loss:0.098\n",
      "Checking accuracy on test set\n",
      "Got 681 / 1000 correct (68.10)\n",
      "213 epoch,  1000 iteration, loss:0.107\n",
      " num 212 epoch \n",
      "####### Training Loss #######\n",
      "[0.10466557]\n",
      "Checking accuracy on test set\n",
      "Got 683 / 1000 correct (68.30)\n",
      "214 epoch,   500 iteration, loss:0.114\n",
      "Checking accuracy on test set\n",
      "Got 717 / 1000 correct (71.70)\n",
      "214 epoch,  1000 iteration, loss:0.108\n",
      " num 213 epoch \n",
      "####### Training Loss #######\n",
      "[0.11307566]\n",
      "Checking accuracy on test set\n",
      "Got 719 / 1000 correct (71.90)\n",
      "215 epoch,   500 iteration, loss:0.079\n",
      "Checking accuracy on test set\n",
      "Got 724 / 1000 correct (72.40)\n",
      "215 epoch,  1000 iteration, loss:0.097\n",
      " num 214 epoch \n",
      "####### Training Loss #######\n",
      "[0.09200385]\n",
      "Checking accuracy on test set\n",
      "Got 698 / 1000 correct (69.80)\n",
      "216 epoch,   500 iteration, loss:0.091\n",
      "Checking accuracy on test set\n",
      "Got 690 / 1000 correct (69.00)\n",
      "216 epoch,  1000 iteration, loss:0.106\n",
      " num 215 epoch \n",
      "####### Training Loss #######\n",
      "[0.10273888]\n",
      "Checking accuracy on test set\n",
      "Got 674 / 1000 correct (67.40)\n",
      "217 epoch,   500 iteration, loss:0.086\n",
      "Checking accuracy on test set\n",
      "Got 688 / 1000 correct (68.80)\n",
      "217 epoch,  1000 iteration, loss:0.104\n",
      " num 216 epoch \n",
      "####### Training Loss #######\n",
      "[0.09103058]\n",
      "Checking accuracy on test set\n",
      "Got 670 / 1000 correct (67.00)\n",
      "218 epoch,   500 iteration, loss:0.096\n",
      "Checking accuracy on test set\n",
      "Got 692 / 1000 correct (69.20)\n",
      "218 epoch,  1000 iteration, loss:0.069\n",
      " num 217 epoch \n",
      "####### Training Loss #######\n",
      "[0.08035313]\n",
      "Checking accuracy on test set\n",
      "Got 675 / 1000 correct (67.50)\n",
      "219 epoch,   500 iteration, loss:0.147\n",
      "Checking accuracy on test set\n",
      "Got 708 / 1000 correct (70.80)\n",
      "219 epoch,  1000 iteration, loss:0.112\n",
      " num 218 epoch \n",
      "####### Training Loss #######\n",
      "[0.11888389]\n",
      "Checking accuracy on test set\n",
      "Got 686 / 1000 correct (68.60)\n",
      "220 epoch,   500 iteration, loss:0.102\n",
      "Checking accuracy on test set\n",
      "Got 694 / 1000 correct (69.40)\n",
      "220 epoch,  1000 iteration, loss:0.088\n",
      " num 219 epoch \n",
      "####### Training Loss #######\n",
      "[0.09310829]\n",
      "Checking accuracy on test set\n",
      "Got 693 / 1000 correct (69.30)\n",
      "221 epoch,   500 iteration, loss:0.079\n",
      "Checking accuracy on test set\n",
      "Got 680 / 1000 correct (68.00)\n",
      "221 epoch,  1000 iteration, loss:0.109\n",
      " num 220 epoch \n",
      "####### Training Loss #######\n",
      "[0.10067696]\n",
      "Checking accuracy on test set\n",
      "Got 684 / 1000 correct (68.40)\n",
      "222 epoch,   500 iteration, loss:0.091\n",
      "Checking accuracy on test set\n",
      "Got 691 / 1000 correct (69.10)\n",
      "222 epoch,  1000 iteration, loss:0.101\n",
      " num 221 epoch \n",
      "####### Training Loss #######\n",
      "[0.10357099]\n",
      "Checking accuracy on test set\n",
      "Got 700 / 1000 correct (70.00)\n",
      "223 epoch,   500 iteration, loss:0.100\n",
      "Checking accuracy on test set\n",
      "Got 708 / 1000 correct (70.80)\n",
      "223 epoch,  1000 iteration, loss:0.114\n",
      " num 222 epoch \n",
      "####### Training Loss #######\n",
      "[0.1108383]\n",
      "Checking accuracy on test set\n",
      "Got 690 / 1000 correct (69.00)\n",
      "224 epoch,   500 iteration, loss:0.110\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking accuracy on test set\n",
      "Got 702 / 1000 correct (70.20)\n",
      "224 epoch,  1000 iteration, loss:0.104\n",
      " num 223 epoch \n",
      "####### Training Loss #######\n",
      "[0.10294403]\n",
      "Checking accuracy on test set\n",
      "Got 676 / 1000 correct (67.60)\n",
      "225 epoch,   500 iteration, loss:0.103\n",
      "Checking accuracy on test set\n",
      "Got 700 / 1000 correct (70.00)\n",
      "225 epoch,  1000 iteration, loss:0.084\n",
      " num 224 epoch \n",
      "####### Training Loss #######\n",
      "[0.09825013]\n",
      "Checking accuracy on test set\n",
      "Got 702 / 1000 correct (70.20)\n",
      "226 epoch,   500 iteration, loss:0.099\n",
      "Checking accuracy on test set\n",
      "Got 683 / 1000 correct (68.30)\n",
      "226 epoch,  1000 iteration, loss:0.110\n",
      " num 225 epoch \n",
      "####### Training Loss #######\n",
      "[0.10545648]\n",
      "Checking accuracy on test set\n",
      "Got 703 / 1000 correct (70.30)\n",
      "227 epoch,   500 iteration, loss:0.092\n",
      "Checking accuracy on test set\n",
      "Got 693 / 1000 correct (69.30)\n",
      "227 epoch,  1000 iteration, loss:0.108\n",
      " num 226 epoch \n",
      "####### Training Loss #######\n",
      "[0.10125704]\n",
      "Checking accuracy on test set\n",
      "Got 683 / 1000 correct (68.30)\n",
      "228 epoch,   500 iteration, loss:0.075\n",
      "Checking accuracy on test set\n",
      "Got 687 / 1000 correct (68.70)\n",
      "228 epoch,  1000 iteration, loss:0.105\n",
      " num 227 epoch \n",
      "####### Training Loss #######\n",
      "[0.09523529]\n",
      "Checking accuracy on test set\n",
      "Got 675 / 1000 correct (67.50)\n",
      "229 epoch,   500 iteration, loss:0.084\n",
      "Checking accuracy on test set\n",
      "Got 696 / 1000 correct (69.60)\n",
      "229 epoch,  1000 iteration, loss:0.125\n",
      " num 228 epoch \n",
      "####### Training Loss #######\n",
      "[0.10237016]\n",
      "Checking accuracy on test set\n",
      "Got 684 / 1000 correct (68.40)\n",
      "230 epoch,   500 iteration, loss:0.091\n",
      "Checking accuracy on test set\n",
      "Got 676 / 1000 correct (67.60)\n",
      "230 epoch,  1000 iteration, loss:0.101\n",
      " num 229 epoch \n",
      "####### Training Loss #######\n",
      "[0.10127246]\n",
      "Checking accuracy on test set\n",
      "Got 691 / 1000 correct (69.10)\n",
      "231 epoch,   500 iteration, loss:0.085\n",
      "Checking accuracy on test set\n",
      "Got 711 / 1000 correct (71.10)\n",
      "231 epoch,  1000 iteration, loss:0.076\n",
      " num 230 epoch \n",
      "####### Training Loss #######\n",
      "[0.08165548]\n",
      "Checking accuracy on test set\n",
      "Got 701 / 1000 correct (70.10)\n",
      "232 epoch,   500 iteration, loss:0.075\n",
      "Checking accuracy on test set\n",
      "Got 666 / 1000 correct (66.60)\n",
      "232 epoch,  1000 iteration, loss:0.083\n",
      " num 231 epoch \n",
      "####### Training Loss #######\n",
      "[0.08572582]\n",
      "Checking accuracy on test set\n",
      "Got 694 / 1000 correct (69.40)\n",
      "233 epoch,   500 iteration, loss:0.113\n",
      "Checking accuracy on test set\n",
      "Got 683 / 1000 correct (68.30)\n",
      "233 epoch,  1000 iteration, loss:0.119\n",
      " num 232 epoch \n",
      "####### Training Loss #######\n",
      "[0.11916853]\n",
      "Checking accuracy on test set\n",
      "Got 711 / 1000 correct (71.10)\n",
      "234 epoch,   500 iteration, loss:0.102\n",
      "Checking accuracy on test set\n",
      "Got 698 / 1000 correct (69.80)\n",
      "234 epoch,  1000 iteration, loss:0.093\n",
      " num 233 epoch \n",
      "####### Training Loss #######\n",
      "[0.09930344]\n",
      "Checking accuracy on test set\n",
      "Got 675 / 1000 correct (67.50)\n",
      "235 epoch,   500 iteration, loss:0.074\n",
      "Checking accuracy on test set\n",
      "Got 709 / 1000 correct (70.90)\n",
      "235 epoch,  1000 iteration, loss:0.079\n",
      " num 234 epoch \n",
      "####### Training Loss #######\n",
      "[0.07989641]\n",
      "Checking accuracy on test set\n",
      "Got 685 / 1000 correct (68.50)\n",
      "236 epoch,   500 iteration, loss:0.099\n",
      "Checking accuracy on test set\n",
      "Got 700 / 1000 correct (70.00)\n",
      "236 epoch,  1000 iteration, loss:0.091\n",
      " num 235 epoch \n",
      "####### Training Loss #######\n",
      "[0.09969779]\n",
      "Checking accuracy on test set\n",
      "Got 693 / 1000 correct (69.30)\n",
      "237 epoch,   500 iteration, loss:0.083\n",
      "Checking accuracy on test set\n",
      "Got 694 / 1000 correct (69.40)\n",
      "237 epoch,  1000 iteration, loss:0.068\n",
      " num 236 epoch \n",
      "####### Training Loss #######\n",
      "[0.08121535]\n",
      "Checking accuracy on test set\n",
      "Got 702 / 1000 correct (70.20)\n",
      "238 epoch,   500 iteration, loss:0.106\n",
      "Checking accuracy on test set\n",
      "Got 707 / 1000 correct (70.70)\n",
      "238 epoch,  1000 iteration, loss:0.131\n",
      " num 237 epoch \n",
      "####### Training Loss #######\n",
      "[0.10995582]\n",
      "Checking accuracy on test set\n",
      "Got 705 / 1000 correct (70.50)\n",
      "239 epoch,   500 iteration, loss:0.093\n",
      "Checking accuracy on test set\n",
      "Got 701 / 1000 correct (70.10)\n",
      "239 epoch,  1000 iteration, loss:0.095\n",
      " num 238 epoch \n",
      "####### Training Loss #######\n",
      "[0.09615977]\n",
      "Checking accuracy on test set\n",
      "Got 688 / 1000 correct (68.80)\n",
      "240 epoch,   500 iteration, loss:0.099\n",
      "Checking accuracy on test set\n",
      "Got 679 / 1000 correct (67.90)\n",
      "240 epoch,  1000 iteration, loss:0.099\n",
      " num 239 epoch \n",
      "####### Training Loss #######\n",
      "[0.10176812]\n",
      "Checking accuracy on test set\n",
      "Got 695 / 1000 correct (69.50)\n",
      "241 epoch,   500 iteration, loss:0.103\n",
      "Checking accuracy on test set\n",
      "Got 683 / 1000 correct (68.30)\n",
      "241 epoch,  1000 iteration, loss:0.084\n",
      " num 240 epoch \n",
      "####### Training Loss #######\n",
      "[0.09618209]\n",
      "Checking accuracy on test set\n",
      "Got 703 / 1000 correct (70.30)\n",
      "242 epoch,   500 iteration, loss:0.080\n",
      "Checking accuracy on test set\n",
      "Got 697 / 1000 correct (69.70)\n",
      "242 epoch,  1000 iteration, loss:0.089\n",
      " num 241 epoch \n",
      "####### Training Loss #######\n",
      "[0.08812477]\n",
      "Checking accuracy on test set\n",
      "Got 693 / 1000 correct (69.30)\n",
      "243 epoch,   500 iteration, loss:0.091\n",
      "Checking accuracy on test set\n",
      "Got 695 / 1000 correct (69.50)\n",
      "243 epoch,  1000 iteration, loss:0.111\n",
      " num 242 epoch \n",
      "####### Training Loss #######\n",
      "[0.10279027]\n",
      "Checking accuracy on test set\n",
      "Got 679 / 1000 correct (67.90)\n",
      "244 epoch,   500 iteration, loss:0.089\n",
      "Checking accuracy on test set\n",
      "Got 712 / 1000 correct (71.20)\n",
      "244 epoch,  1000 iteration, loss:0.095\n",
      " num 243 epoch \n",
      "####### Training Loss #######\n",
      "[0.09399273]\n",
      "Checking accuracy on test set\n",
      "Got 689 / 1000 correct (68.90)\n",
      "245 epoch,   500 iteration, loss:0.094\n",
      "Checking accuracy on test set\n",
      "Got 702 / 1000 correct (70.20)\n",
      "245 epoch,  1000 iteration, loss:0.090\n",
      " num 244 epoch \n",
      "####### Training Loss #######\n",
      "[0.09141918]\n",
      "Checking accuracy on test set\n",
      "Got 681 / 1000 correct (68.10)\n",
      "246 epoch,   500 iteration, loss:0.096\n",
      "Checking accuracy on test set\n",
      "Got 666 / 1000 correct (66.60)\n",
      "246 epoch,  1000 iteration, loss:0.083\n",
      " num 245 epoch \n",
      "####### Training Loss #######\n",
      "[0.09286345]\n",
      "Checking accuracy on test set\n",
      "Got 679 / 1000 correct (67.90)\n",
      "247 epoch,   500 iteration, loss:0.115\n",
      "Checking accuracy on test set\n",
      "Got 695 / 1000 correct (69.50)\n",
      "247 epoch,  1000 iteration, loss:0.111\n",
      " num 246 epoch \n",
      "####### Training Loss #######\n",
      "[0.10931601]\n",
      "Checking accuracy on test set\n",
      "Got 680 / 1000 correct (68.00)\n",
      "248 epoch,   500 iteration, loss:0.097\n",
      "Checking accuracy on test set\n",
      "Got 691 / 1000 correct (69.10)\n",
      "248 epoch,  1000 iteration, loss:0.111\n",
      " num 247 epoch \n",
      "####### Training Loss #######\n",
      "[0.09895255]\n",
      "Checking accuracy on test set\n",
      "Got 704 / 1000 correct (70.40)\n",
      "249 epoch,   500 iteration, loss:0.100\n",
      "Checking accuracy on test set\n",
      "Got 694 / 1000 correct (69.40)\n",
      "249 epoch,  1000 iteration, loss:0.090\n",
      " num 248 epoch \n",
      "####### Training Loss #######\n",
      "[0.09217441]\n",
      "Checking accuracy on test set\n",
      "Got 683 / 1000 correct (68.30)\n",
      "250 epoch,   500 iteration, loss:0.107\n",
      "Checking accuracy on test set\n",
      "Got 686 / 1000 correct (68.60)\n",
      "250 epoch,  1000 iteration, loss:0.076\n",
      " num 249 epoch \n",
      "####### Training Loss #######\n",
      "[0.09135499]\n",
      "Checking accuracy on test set\n",
      "Got 700 / 1000 correct (70.00)\n",
      "251 epoch,   500 iteration, loss:0.075\n",
      "Checking accuracy on test set\n",
      "Got 710 / 1000 correct (71.00)\n",
      "251 epoch,  1000 iteration, loss:0.117\n",
      " num 250 epoch \n",
      "####### Training Loss #######\n",
      "[0.09535778]\n",
      "Checking accuracy on test set\n",
      "Got 704 / 1000 correct (70.40)\n",
      "252 epoch,   500 iteration, loss:0.107\n",
      "Checking accuracy on test set\n",
      "Got 700 / 1000 correct (70.00)\n",
      "252 epoch,  1000 iteration, loss:0.088\n",
      " num 251 epoch \n",
      "####### Training Loss #######\n",
      "[0.09216048]\n",
      "Checking accuracy on test set\n",
      "Got 684 / 1000 correct (68.40)\n",
      "253 epoch,   500 iteration, loss:0.089\n",
      "Checking accuracy on test set\n",
      "Got 683 / 1000 correct (68.30)\n",
      "253 epoch,  1000 iteration, loss:0.105\n",
      " num 252 epoch \n",
      "####### Training Loss #######\n",
      "[0.09895302]\n",
      "Checking accuracy on test set\n",
      "Got 689 / 1000 correct (68.90)\n",
      "254 epoch,   500 iteration, loss:0.076\n",
      "Checking accuracy on test set\n",
      "Got 688 / 1000 correct (68.80)\n",
      "254 epoch,  1000 iteration, loss:0.100\n",
      " num 253 epoch \n",
      "####### Training Loss #######\n",
      "[0.08708553]\n",
      "Checking accuracy on test set\n",
      "Got 703 / 1000 correct (70.30)\n",
      "255 epoch,   500 iteration, loss:0.092\n",
      "Checking accuracy on test set\n",
      "Got 690 / 1000 correct (69.00)\n",
      "255 epoch,  1000 iteration, loss:0.079\n",
      " num 254 epoch \n",
      "####### Training Loss #######\n",
      "[0.08826845]\n",
      "Checking accuracy on test set\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 692 / 1000 correct (69.20)\n",
      "256 epoch,   500 iteration, loss:0.107\n",
      "Checking accuracy on test set\n",
      "Got 708 / 1000 correct (70.80)\n",
      "256 epoch,  1000 iteration, loss:0.090\n",
      " num 255 epoch \n",
      "####### Training Loss #######\n",
      "[0.09435841]\n",
      "Checking accuracy on test set\n",
      "Got 708 / 1000 correct (70.80)\n",
      "257 epoch,   500 iteration, loss:0.087\n",
      "Checking accuracy on test set\n",
      "Got 686 / 1000 correct (68.60)\n",
      "257 epoch,  1000 iteration, loss:0.097\n",
      " num 256 epoch \n",
      "####### Training Loss #######\n",
      "[0.09414317]\n",
      "Checking accuracy on test set\n",
      "Got 689 / 1000 correct (68.90)\n",
      "258 epoch,   500 iteration, loss:0.092\n",
      "Checking accuracy on test set\n",
      "Got 688 / 1000 correct (68.80)\n",
      "258 epoch,  1000 iteration, loss:0.090\n",
      " num 257 epoch \n",
      "####### Training Loss #######\n",
      "[0.09472037]\n",
      "Checking accuracy on test set\n",
      "Got 674 / 1000 correct (67.40)\n",
      "259 epoch,   500 iteration, loss:0.101\n",
      "Checking accuracy on test set\n",
      "Got 687 / 1000 correct (68.70)\n",
      "259 epoch,  1000 iteration, loss:0.089\n",
      " num 258 epoch \n",
      "####### Training Loss #######\n",
      "[0.09355735]\n",
      "Checking accuracy on test set\n",
      "Got 700 / 1000 correct (70.00)\n",
      "260 epoch,   500 iteration, loss:0.093\n",
      "Checking accuracy on test set\n",
      "Got 711 / 1000 correct (71.10)\n",
      "260 epoch,  1000 iteration, loss:0.104\n",
      " num 259 epoch \n",
      "####### Training Loss #######\n",
      "[0.10037417]\n",
      "Checking accuracy on test set\n",
      "Got 704 / 1000 correct (70.40)\n",
      "261 epoch,   500 iteration, loss:0.096\n",
      "Checking accuracy on test set\n",
      "Got 688 / 1000 correct (68.80)\n",
      "261 epoch,  1000 iteration, loss:0.087\n",
      " num 260 epoch \n",
      "####### Training Loss #######\n",
      "[0.09203919]\n",
      "Checking accuracy on test set\n",
      "Got 690 / 1000 correct (69.00)\n",
      "262 epoch,   500 iteration, loss:0.093\n",
      "Checking accuracy on test set\n",
      "Got 709 / 1000 correct (70.90)\n",
      "262 epoch,  1000 iteration, loss:0.086\n",
      " num 261 epoch \n",
      "####### Training Loss #######\n",
      "[0.09421179]\n",
      "Checking accuracy on test set\n",
      "Got 694 / 1000 correct (69.40)\n",
      "263 epoch,   500 iteration, loss:0.096\n",
      "Checking accuracy on test set\n",
      "Got 699 / 1000 correct (69.90)\n",
      "263 epoch,  1000 iteration, loss:0.107\n",
      " num 262 epoch \n",
      "####### Training Loss #######\n",
      "[0.10066001]\n",
      "Checking accuracy on test set\n",
      "Got 674 / 1000 correct (67.40)\n",
      "264 epoch,   500 iteration, loss:0.079\n",
      "Checking accuracy on test set\n",
      "Got 693 / 1000 correct (69.30)\n",
      "264 epoch,  1000 iteration, loss:0.100\n",
      " num 263 epoch \n",
      "####### Training Loss #######\n",
      "[0.09206036]\n",
      "Checking accuracy on test set\n",
      "Got 675 / 1000 correct (67.50)\n",
      "265 epoch,   500 iteration, loss:0.110\n",
      "Checking accuracy on test set\n",
      "Got 704 / 1000 correct (70.40)\n",
      "265 epoch,  1000 iteration, loss:0.093\n",
      " num 264 epoch \n",
      "####### Training Loss #######\n",
      "[0.09773703]\n",
      "Checking accuracy on test set\n",
      "Got 670 / 1000 correct (67.00)\n",
      "266 epoch,   500 iteration, loss:0.106\n",
      "Checking accuracy on test set\n",
      "Got 718 / 1000 correct (71.80)\n",
      "266 epoch,  1000 iteration, loss:0.096\n",
      " num 265 epoch \n",
      "####### Training Loss #######\n",
      "[0.09762647]\n",
      "Checking accuracy on test set\n",
      "Got 684 / 1000 correct (68.40)\n",
      "267 epoch,   500 iteration, loss:0.102\n",
      "Checking accuracy on test set\n",
      "Got 694 / 1000 correct (69.40)\n",
      "267 epoch,  1000 iteration, loss:0.082\n",
      " num 266 epoch \n",
      "####### Training Loss #######\n",
      "[0.09454687]\n",
      "Checking accuracy on test set\n",
      "Got 694 / 1000 correct (69.40)\n",
      "268 epoch,   500 iteration, loss:0.085\n",
      "Checking accuracy on test set\n",
      "Got 676 / 1000 correct (67.60)\n",
      "268 epoch,  1000 iteration, loss:0.099\n",
      " num 267 epoch \n",
      "####### Training Loss #######\n",
      "[0.0930173]\n",
      "Checking accuracy on test set\n",
      "Got 689 / 1000 correct (68.90)\n",
      "269 epoch,   500 iteration, loss:0.093\n",
      "Checking accuracy on test set\n",
      "Got 714 / 1000 correct (71.40)\n",
      "269 epoch,  1000 iteration, loss:0.078\n",
      " num 268 epoch \n",
      "####### Training Loss #######\n",
      "[0.09178995]\n",
      "Checking accuracy on test set\n",
      "Got 661 / 1000 correct (66.10)\n",
      "270 epoch,   500 iteration, loss:0.116\n",
      "Checking accuracy on test set\n",
      "Got 706 / 1000 correct (70.60)\n",
      "270 epoch,  1000 iteration, loss:0.101\n",
      " num 269 epoch \n",
      "####### Training Loss #######\n",
      "[0.11006457]\n",
      "Checking accuracy on test set\n",
      "Got 692 / 1000 correct (69.20)\n",
      "271 epoch,   500 iteration, loss:0.084\n",
      "Checking accuracy on test set\n",
      "Got 702 / 1000 correct (70.20)\n",
      "271 epoch,  1000 iteration, loss:0.065\n",
      " num 270 epoch \n",
      "####### Training Loss #######\n",
      "[0.07417287]\n",
      "Checking accuracy on test set\n",
      "Got 680 / 1000 correct (68.00)\n",
      "272 epoch,   500 iteration, loss:0.084\n",
      "Checking accuracy on test set\n",
      "Got 716 / 1000 correct (71.60)\n",
      "272 epoch,  1000 iteration, loss:0.112\n",
      " num 271 epoch \n",
      "####### Training Loss #######\n",
      "[0.09565858]\n",
      "Checking accuracy on test set\n",
      "Got 701 / 1000 correct (70.10)\n",
      "273 epoch,   500 iteration, loss:0.102\n",
      "Checking accuracy on test set\n",
      "Got 693 / 1000 correct (69.30)\n",
      "273 epoch,  1000 iteration, loss:0.116\n",
      " num 272 epoch \n",
      "####### Training Loss #######\n",
      "[0.11073239]\n",
      "Checking accuracy on test set\n",
      "Got 683 / 1000 correct (68.30)\n",
      "274 epoch,   500 iteration, loss:0.078\n",
      "Checking accuracy on test set\n",
      "Got 693 / 1000 correct (69.30)\n",
      "274 epoch,  1000 iteration, loss:0.107\n",
      " num 273 epoch \n",
      "####### Training Loss #######\n",
      "[0.09221672]\n",
      "Checking accuracy on test set\n",
      "Got 672 / 1000 correct (67.20)\n",
      "275 epoch,   500 iteration, loss:0.104\n",
      "Checking accuracy on test set\n",
      "Got 694 / 1000 correct (69.40)\n",
      "275 epoch,  1000 iteration, loss:0.100\n",
      " num 274 epoch \n",
      "####### Training Loss #######\n",
      "[0.10132463]\n",
      "Checking accuracy on test set\n",
      "Got 664 / 1000 correct (66.40)\n",
      "276 epoch,   500 iteration, loss:0.097\n",
      "Checking accuracy on test set\n",
      "Got 687 / 1000 correct (68.70)\n",
      "276 epoch,  1000 iteration, loss:0.095\n",
      " num 275 epoch \n",
      "####### Training Loss #######\n",
      "[0.09552782]\n",
      "Checking accuracy on test set\n",
      "Got 686 / 1000 correct (68.60)\n",
      "277 epoch,   500 iteration, loss:0.097\n",
      "Checking accuracy on test set\n",
      "Got 685 / 1000 correct (68.50)\n",
      "277 epoch,  1000 iteration, loss:0.114\n",
      " num 276 epoch \n",
      "####### Training Loss #######\n",
      "[0.1054334]\n",
      "Checking accuracy on test set\n",
      "Got 695 / 1000 correct (69.50)\n",
      "278 epoch,   500 iteration, loss:0.078\n",
      "Checking accuracy on test set\n",
      "Got 694 / 1000 correct (69.40)\n",
      "278 epoch,  1000 iteration, loss:0.096\n",
      " num 277 epoch \n",
      "####### Training Loss #######\n",
      "[0.09525493]\n",
      "Checking accuracy on test set\n",
      "Got 690 / 1000 correct (69.00)\n",
      "279 epoch,   500 iteration, loss:0.097\n",
      "Checking accuracy on test set\n",
      "Got 680 / 1000 correct (68.00)\n",
      "279 epoch,  1000 iteration, loss:0.084\n",
      " num 278 epoch \n",
      "####### Training Loss #######\n",
      "[0.09019385]\n",
      "Checking accuracy on test set\n",
      "Got 661 / 1000 correct (66.10)\n",
      "280 epoch,   500 iteration, loss:0.082\n",
      "Checking accuracy on test set\n",
      "Got 691 / 1000 correct (69.10)\n",
      "280 epoch,  1000 iteration, loss:0.077\n",
      " num 279 epoch \n",
      "####### Training Loss #######\n",
      "[0.08261084]\n",
      "Checking accuracy on test set\n",
      "Got 705 / 1000 correct (70.50)\n",
      "281 epoch,   500 iteration, loss:0.097\n",
      "Checking accuracy on test set\n",
      "Got 700 / 1000 correct (70.00)\n",
      "281 epoch,  1000 iteration, loss:0.081\n",
      " num 280 epoch \n",
      "####### Training Loss #######\n",
      "[0.09058365]\n",
      "Checking accuracy on test set\n",
      "Got 700 / 1000 correct (70.00)\n",
      "282 epoch,   500 iteration, loss:0.105\n",
      "Checking accuracy on test set\n",
      "Got 697 / 1000 correct (69.70)\n",
      "282 epoch,  1000 iteration, loss:0.108\n",
      " num 281 epoch \n",
      "####### Training Loss #######\n",
      "[0.10293858]\n",
      "Checking accuracy on test set\n",
      "Got 682 / 1000 correct (68.20)\n",
      "283 epoch,   500 iteration, loss:0.096\n",
      "Checking accuracy on test set\n",
      "Got 698 / 1000 correct (69.80)\n",
      "283 epoch,  1000 iteration, loss:0.108\n",
      " num 282 epoch \n",
      "####### Training Loss #######\n",
      "[0.1030413]\n",
      "Checking accuracy on test set\n",
      "Got 702 / 1000 correct (70.20)\n",
      "284 epoch,   500 iteration, loss:0.072\n",
      "Checking accuracy on test set\n",
      "Got 696 / 1000 correct (69.60)\n",
      "284 epoch,  1000 iteration, loss:0.077\n",
      " num 283 epoch \n",
      "####### Training Loss #######\n",
      "[0.07792189]\n",
      "Checking accuracy on test set\n",
      "Got 704 / 1000 correct (70.40)\n",
      "285 epoch,   500 iteration, loss:0.078\n",
      "Checking accuracy on test set\n",
      "Got 700 / 1000 correct (70.00)\n",
      "285 epoch,  1000 iteration, loss:0.107\n",
      " num 284 epoch \n",
      "####### Training Loss #######\n",
      "[0.08805118]\n",
      "Checking accuracy on test set\n",
      "Got 664 / 1000 correct (66.40)\n",
      "286 epoch,   500 iteration, loss:0.105\n",
      "Checking accuracy on test set\n",
      "Got 707 / 1000 correct (70.70)\n",
      "286 epoch,  1000 iteration, loss:0.117\n",
      " num 285 epoch \n",
      "####### Training Loss #######\n",
      "[0.10389763]\n",
      "Checking accuracy on test set\n",
      "Got 686 / 1000 correct (68.60)\n",
      "287 epoch,   500 iteration, loss:0.092\n",
      "Checking accuracy on test set\n",
      "Got 699 / 1000 correct (69.90)\n",
      "287 epoch,  1000 iteration, loss:0.107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " num 286 epoch \n",
      "####### Training Loss #######\n",
      "[0.09933603]\n",
      "Checking accuracy on test set\n",
      "Got 705 / 1000 correct (70.50)\n",
      "288 epoch,   500 iteration, loss:0.080\n",
      "Checking accuracy on test set\n",
      "Got 709 / 1000 correct (70.90)\n",
      "288 epoch,  1000 iteration, loss:0.090\n",
      " num 287 epoch \n",
      "####### Training Loss #######\n",
      "[0.08316147]\n",
      "Checking accuracy on test set\n",
      "Got 710 / 1000 correct (71.00)\n",
      "289 epoch,   500 iteration, loss:0.102\n",
      "Checking accuracy on test set\n",
      "Got 700 / 1000 correct (70.00)\n",
      "289 epoch,  1000 iteration, loss:0.088\n",
      " num 288 epoch \n",
      "####### Training Loss #######\n",
      "[0.09781145]\n",
      "Checking accuracy on test set\n",
      "Got 669 / 1000 correct (66.90)\n",
      "290 epoch,   500 iteration, loss:0.068\n",
      "Checking accuracy on test set\n",
      "Got 668 / 1000 correct (66.80)\n",
      "290 epoch,  1000 iteration, loss:0.088\n",
      " num 289 epoch \n",
      "####### Training Loss #######\n",
      "[0.08599992]\n",
      "Checking accuracy on test set\n",
      "Got 702 / 1000 correct (70.20)\n",
      "291 epoch,   500 iteration, loss:0.082\n",
      "Checking accuracy on test set\n",
      "Got 695 / 1000 correct (69.50)\n",
      "291 epoch,  1000 iteration, loss:0.089\n",
      " num 290 epoch \n",
      "####### Training Loss #######\n",
      "[0.09287243]\n",
      "Checking accuracy on test set\n",
      "Got 704 / 1000 correct (70.40)\n",
      "292 epoch,   500 iteration, loss:0.092\n",
      "Checking accuracy on test set\n",
      "Got 708 / 1000 correct (70.80)\n",
      "292 epoch,  1000 iteration, loss:0.090\n",
      " num 291 epoch \n",
      "####### Training Loss #######\n",
      "[0.08771826]\n",
      "Checking accuracy on test set\n",
      "Got 683 / 1000 correct (68.30)\n",
      "293 epoch,   500 iteration, loss:0.119\n",
      "Checking accuracy on test set\n",
      "Got 710 / 1000 correct (71.00)\n",
      "293 epoch,  1000 iteration, loss:0.094\n",
      " num 292 epoch \n",
      "####### Training Loss #######\n",
      "[0.10461528]\n",
      "Checking accuracy on test set\n",
      "Got 692 / 1000 correct (69.20)\n",
      "294 epoch,   500 iteration, loss:0.092\n",
      "Checking accuracy on test set\n",
      "Got 694 / 1000 correct (69.40)\n",
      "294 epoch,  1000 iteration, loss:0.087\n",
      " num 293 epoch \n",
      "####### Training Loss #######\n",
      "[0.09533088]\n",
      "Checking accuracy on test set\n",
      "Got 683 / 1000 correct (68.30)\n",
      "295 epoch,   500 iteration, loss:0.080\n",
      "Checking accuracy on test set\n",
      "Got 687 / 1000 correct (68.70)\n",
      "295 epoch,  1000 iteration, loss:0.063\n",
      " num 294 epoch \n",
      "####### Training Loss #######\n",
      "[0.07385318]\n",
      "Checking accuracy on test set\n",
      "Got 681 / 1000 correct (68.10)\n",
      "296 epoch,   500 iteration, loss:0.101\n",
      "Checking accuracy on test set\n",
      "Got 688 / 1000 correct (68.80)\n",
      "296 epoch,  1000 iteration, loss:0.101\n",
      " num 295 epoch \n",
      "####### Training Loss #######\n",
      "[0.09539251]\n",
      "Checking accuracy on test set\n",
      "Got 692 / 1000 correct (69.20)\n",
      "297 epoch,   500 iteration, loss:0.100\n",
      "Checking accuracy on test set\n",
      "Got 680 / 1000 correct (68.00)\n",
      "297 epoch,  1000 iteration, loss:0.076\n",
      " num 296 epoch \n",
      "####### Training Loss #######\n",
      "[0.08958163]\n",
      "Checking accuracy on test set\n",
      "Got 686 / 1000 correct (68.60)\n",
      "298 epoch,   500 iteration, loss:0.090\n",
      "Checking accuracy on test set\n",
      "Got 709 / 1000 correct (70.90)\n",
      "298 epoch,  1000 iteration, loss:0.099\n",
      " num 297 epoch \n",
      "####### Training Loss #######\n",
      "[0.09742073]\n",
      "Checking accuracy on test set\n",
      "Got 703 / 1000 correct (70.30)\n",
      "299 epoch,   500 iteration, loss:0.085\n",
      "Checking accuracy on test set\n",
      "Got 697 / 1000 correct (69.70)\n",
      "299 epoch,  1000 iteration, loss:0.085\n",
      " num 298 epoch \n",
      "####### Training Loss #######\n",
      "[0.08474493]\n",
      "Checking accuracy on test set\n",
      "Got 676 / 1000 correct (67.60)\n",
      "300 epoch,   500 iteration, loss:0.095\n",
      "Checking accuracy on test set\n",
      "Got 694 / 1000 correct (69.40)\n",
      "300 epoch,  1000 iteration, loss:0.088\n",
      " num 299 epoch \n",
      "####### Training Loss #######\n",
      "[0.09515093]\n",
      "Checking accuracy on test set\n",
      "Got 677 / 1000 correct (67.70)\n",
      "301 epoch,   500 iteration, loss:0.094\n",
      "Checking accuracy on test set\n",
      "Got 698 / 1000 correct (69.80)\n",
      "301 epoch,  1000 iteration, loss:0.086\n",
      " num 300 epoch \n",
      "####### Training Loss #######\n",
      "[0.08906978]\n",
      "Checking accuracy on test set\n",
      "Got 663 / 1000 correct (66.30)\n",
      "302 epoch,   500 iteration, loss:0.086\n",
      "Checking accuracy on test set\n",
      "Got 707 / 1000 correct (70.70)\n",
      "302 epoch,  1000 iteration, loss:0.101\n",
      " num 301 epoch \n",
      "####### Training Loss #######\n",
      "[0.09593622]\n",
      "Checking accuracy on test set\n",
      "Got 705 / 1000 correct (70.50)\n",
      "303 epoch,   500 iteration, loss:0.082\n",
      "Checking accuracy on test set\n",
      "Got 713 / 1000 correct (71.30)\n",
      "303 epoch,  1000 iteration, loss:0.091\n",
      " num 302 epoch \n",
      "####### Training Loss #######\n",
      "[0.09161291]\n",
      "Checking accuracy on test set\n",
      "Got 720 / 1000 correct (72.00)\n",
      "304 epoch,   500 iteration, loss:0.082\n",
      "Checking accuracy on test set\n",
      "Got 708 / 1000 correct (70.80)\n",
      "304 epoch,  1000 iteration, loss:0.093\n",
      " num 303 epoch \n",
      "####### Training Loss #######\n",
      "[0.08319549]\n",
      "Checking accuracy on test set\n",
      "Got 701 / 1000 correct (70.10)\n",
      "305 epoch,   500 iteration, loss:0.095\n",
      "Checking accuracy on test set\n",
      "Got 709 / 1000 correct (70.90)\n",
      "305 epoch,  1000 iteration, loss:0.094\n",
      " num 304 epoch \n",
      "####### Training Loss #######\n",
      "[0.09461995]\n",
      "Checking accuracy on test set\n",
      "Got 711 / 1000 correct (71.10)\n",
      "306 epoch,   500 iteration, loss:0.101\n",
      "Checking accuracy on test set\n",
      "Got 713 / 1000 correct (71.30)\n",
      "306 epoch,  1000 iteration, loss:0.081\n",
      " num 305 epoch \n",
      "####### Training Loss #######\n",
      "[0.08828846]\n",
      "Checking accuracy on test set\n",
      "Got 698 / 1000 correct (69.80)\n",
      "307 epoch,   500 iteration, loss:0.081\n",
      "Checking accuracy on test set\n",
      "Got 685 / 1000 correct (68.50)\n",
      "307 epoch,  1000 iteration, loss:0.096\n",
      " num 306 epoch \n",
      "####### Training Loss #######\n",
      "[0.0892169]\n",
      "Checking accuracy on test set\n",
      "Got 690 / 1000 correct (69.00)\n",
      "308 epoch,   500 iteration, loss:0.072\n",
      "Checking accuracy on test set\n",
      "Got 707 / 1000 correct (70.70)\n",
      "308 epoch,  1000 iteration, loss:0.085\n",
      " num 307 epoch \n",
      "####### Training Loss #######\n",
      "[0.07888329]\n",
      "Checking accuracy on test set\n",
      "Got 687 / 1000 correct (68.70)\n",
      "309 epoch,   500 iteration, loss:0.088\n",
      "Checking accuracy on test set\n",
      "Got 709 / 1000 correct (70.90)\n",
      "309 epoch,  1000 iteration, loss:0.073\n",
      " num 308 epoch \n",
      "####### Training Loss #######\n",
      "[0.08236314]\n",
      "Checking accuracy on test set\n",
      "Got 687 / 1000 correct (68.70)\n",
      "310 epoch,   500 iteration, loss:0.085\n",
      "Checking accuracy on test set\n",
      "Got 710 / 1000 correct (71.00)\n",
      "310 epoch,  1000 iteration, loss:0.076\n",
      " num 309 epoch \n",
      "####### Training Loss #######\n",
      "[0.0817919]\n",
      "Checking accuracy on test set\n",
      "Got 642 / 1000 correct (64.20)\n",
      "311 epoch,   500 iteration, loss:0.083\n",
      "Checking accuracy on test set\n",
      "Got 690 / 1000 correct (69.00)\n",
      "311 epoch,  1000 iteration, loss:0.082\n",
      " num 310 epoch \n",
      "####### Training Loss #######\n",
      "[0.08264457]\n",
      "Checking accuracy on test set\n",
      "Got 686 / 1000 correct (68.60)\n",
      "312 epoch,   500 iteration, loss:0.099\n",
      "Checking accuracy on test set\n",
      "Got 691 / 1000 correct (69.10)\n",
      "312 epoch,  1000 iteration, loss:0.084\n",
      " num 311 epoch \n",
      "####### Training Loss #######\n",
      "[0.0925829]\n",
      "Checking accuracy on test set\n",
      "Got 680 / 1000 correct (68.00)\n",
      "313 epoch,   500 iteration, loss:0.099\n",
      "Checking accuracy on test set\n",
      "Got 711 / 1000 correct (71.10)\n",
      "313 epoch,  1000 iteration, loss:0.089\n",
      " num 312 epoch \n",
      "####### Training Loss #######\n",
      "[0.09371859]\n",
      "Checking accuracy on test set\n",
      "Got 705 / 1000 correct (70.50)\n",
      "314 epoch,   500 iteration, loss:0.091\n",
      "Checking accuracy on test set\n",
      "Got 691 / 1000 correct (69.10)\n",
      "314 epoch,  1000 iteration, loss:0.064\n",
      " num 313 epoch \n",
      "####### Training Loss #######\n",
      "[0.07653039]\n",
      "Checking accuracy on test set\n",
      "Got 709 / 1000 correct (70.90)\n",
      "315 epoch,   500 iteration, loss:0.086\n",
      "Checking accuracy on test set\n",
      "Got 688 / 1000 correct (68.80)\n",
      "315 epoch,  1000 iteration, loss:0.062\n",
      " num 314 epoch \n",
      "####### Training Loss #######\n",
      "[0.07356819]\n",
      "Checking accuracy on test set\n",
      "Got 669 / 1000 correct (66.90)\n",
      "316 epoch,   500 iteration, loss:0.092\n",
      "Checking accuracy on test set\n",
      "Got 657 / 1000 correct (65.70)\n",
      "316 epoch,  1000 iteration, loss:0.094\n",
      " num 315 epoch \n",
      "####### Training Loss #######\n",
      "[0.09081648]\n",
      "Checking accuracy on test set\n",
      "Got 701 / 1000 correct (70.10)\n",
      "317 epoch,   500 iteration, loss:0.098\n",
      "Checking accuracy on test set\n",
      "Got 719 / 1000 correct (71.90)\n",
      "317 epoch,  1000 iteration, loss:0.089\n",
      " num 316 epoch \n",
      "####### Training Loss #######\n",
      "[0.09221171]\n",
      "Checking accuracy on test set\n",
      "Got 694 / 1000 correct (69.40)\n",
      "318 epoch,   500 iteration, loss:0.061\n",
      "Checking accuracy on test set\n",
      "Got 715 / 1000 correct (71.50)\n",
      "318 epoch,  1000 iteration, loss:0.078\n",
      " num 317 epoch \n",
      "####### Training Loss #######\n",
      "[0.07104585]\n",
      "Checking accuracy on test set\n",
      "Got 687 / 1000 correct (68.70)\n",
      "319 epoch,   500 iteration, loss:0.065\n",
      "Checking accuracy on test set\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 698 / 1000 correct (69.80)\n",
      "319 epoch,  1000 iteration, loss:0.084\n",
      " num 318 epoch \n",
      "####### Training Loss #######\n",
      "[0.07747762]\n",
      "Checking accuracy on test set\n",
      "Got 681 / 1000 correct (68.10)\n",
      "320 epoch,   500 iteration, loss:0.057\n",
      "Checking accuracy on test set\n",
      "Got 710 / 1000 correct (71.00)\n",
      "320 epoch,  1000 iteration, loss:0.079\n",
      " num 319 epoch \n",
      "####### Training Loss #######\n",
      "[0.07500545]\n",
      "Checking accuracy on test set\n",
      "Got 703 / 1000 correct (70.30)\n",
      "321 epoch,   500 iteration, loss:0.101\n",
      "Checking accuracy on test set\n",
      "Got 700 / 1000 correct (70.00)\n",
      "321 epoch,  1000 iteration, loss:0.112\n",
      " num 320 epoch \n",
      "####### Training Loss #######\n",
      "[0.10907869]\n",
      "Checking accuracy on test set\n",
      "Got 695 / 1000 correct (69.50)\n",
      "322 epoch,   500 iteration, loss:0.078\n",
      "Checking accuracy on test set\n",
      "Got 707 / 1000 correct (70.70)\n",
      "322 epoch,  1000 iteration, loss:0.067\n",
      " num 321 epoch \n",
      "####### Training Loss #######\n",
      "[0.07266229]\n",
      "Checking accuracy on test set\n",
      "Got 697 / 1000 correct (69.70)\n",
      "323 epoch,   500 iteration, loss:0.087\n",
      "Checking accuracy on test set\n",
      "Got 694 / 1000 correct (69.40)\n",
      "323 epoch,  1000 iteration, loss:0.085\n",
      " num 322 epoch \n",
      "####### Training Loss #######\n",
      "[0.09367417]\n",
      "Checking accuracy on test set\n",
      "Got 695 / 1000 correct (69.50)\n",
      "324 epoch,   500 iteration, loss:0.085\n",
      "Checking accuracy on test set\n",
      "Got 691 / 1000 correct (69.10)\n",
      "324 epoch,  1000 iteration, loss:0.070\n",
      " num 323 epoch \n",
      "####### Training Loss #######\n",
      "[0.08549453]\n",
      "Checking accuracy on test set\n",
      "Got 701 / 1000 correct (70.10)\n",
      "325 epoch,   500 iteration, loss:0.085\n",
      "Checking accuracy on test set\n",
      "Got 707 / 1000 correct (70.70)\n",
      "325 epoch,  1000 iteration, loss:0.086\n",
      " num 324 epoch \n",
      "####### Training Loss #######\n",
      "[0.0879239]\n",
      "Checking accuracy on test set\n",
      "Got 696 / 1000 correct (69.60)\n",
      "326 epoch,   500 iteration, loss:0.093\n",
      "Checking accuracy on test set\n",
      "Got 705 / 1000 correct (70.50)\n",
      "326 epoch,  1000 iteration, loss:0.110\n",
      " num 325 epoch \n",
      "####### Training Loss #######\n",
      "[0.09599673]\n",
      "Checking accuracy on test set\n",
      "Got 681 / 1000 correct (68.10)\n",
      "327 epoch,   500 iteration, loss:0.082\n",
      "Checking accuracy on test set\n",
      "Got 672 / 1000 correct (67.20)\n",
      "327 epoch,  1000 iteration, loss:0.095\n",
      " num 326 epoch \n",
      "####### Training Loss #######\n",
      "[0.09018841]\n",
      "Checking accuracy on test set\n",
      "Got 695 / 1000 correct (69.50)\n",
      "328 epoch,   500 iteration, loss:0.127\n",
      "Checking accuracy on test set\n",
      "Got 708 / 1000 correct (70.80)\n",
      "328 epoch,  1000 iteration, loss:0.078\n",
      " num 327 epoch \n",
      "####### Training Loss #######\n",
      "[0.09692735]\n",
      "Checking accuracy on test set\n",
      "Got 685 / 1000 correct (68.50)\n",
      "329 epoch,   500 iteration, loss:0.077\n",
      "Checking accuracy on test set\n",
      "Got 711 / 1000 correct (71.10)\n",
      "329 epoch,  1000 iteration, loss:0.081\n",
      " num 328 epoch \n",
      "####### Training Loss #######\n",
      "[0.08127653]\n",
      "Checking accuracy on test set\n",
      "Got 701 / 1000 correct (70.10)\n",
      "330 epoch,   500 iteration, loss:0.099\n",
      "Checking accuracy on test set\n",
      "Got 701 / 1000 correct (70.10)\n",
      "330 epoch,  1000 iteration, loss:0.082\n",
      " num 329 epoch \n",
      "####### Training Loss #######\n",
      "[0.08582331]\n",
      "Checking accuracy on test set\n",
      "Got 696 / 1000 correct (69.60)\n",
      "331 epoch,   500 iteration, loss:0.098\n",
      "Checking accuracy on test set\n",
      "Got 700 / 1000 correct (70.00)\n",
      "331 epoch,  1000 iteration, loss:0.083\n",
      " num 330 epoch \n",
      "####### Training Loss #######\n",
      "[0.08831225]\n",
      "Checking accuracy on test set\n",
      "Got 698 / 1000 correct (69.80)\n",
      "332 epoch,   500 iteration, loss:0.078\n",
      "Checking accuracy on test set\n",
      "Got 697 / 1000 correct (69.70)\n",
      "332 epoch,  1000 iteration, loss:0.075\n",
      " num 331 epoch \n",
      "####### Training Loss #######\n",
      "[0.07592456]\n",
      "Checking accuracy on test set\n",
      "Got 690 / 1000 correct (69.00)\n",
      "333 epoch,   500 iteration, loss:0.097\n",
      "Checking accuracy on test set\n",
      "Got 719 / 1000 correct (71.90)\n",
      "333 epoch,  1000 iteration, loss:0.118\n",
      " num 332 epoch \n",
      "####### Training Loss #######\n",
      "[0.10010487]\n",
      "Checking accuracy on test set\n",
      "Got 666 / 1000 correct (66.60)\n",
      "334 epoch,   500 iteration, loss:0.082\n",
      "Checking accuracy on test set\n",
      "Got 681 / 1000 correct (68.10)\n",
      "334 epoch,  1000 iteration, loss:0.098\n",
      " num 333 epoch \n",
      "####### Training Loss #######\n",
      "[0.08876318]\n",
      "Checking accuracy on test set\n",
      "Got 701 / 1000 correct (70.10)\n",
      "335 epoch,   500 iteration, loss:0.082\n",
      "Checking accuracy on test set\n",
      "Got 692 / 1000 correct (69.20)\n",
      "335 epoch,  1000 iteration, loss:0.104\n",
      " num 334 epoch \n",
      "####### Training Loss #######\n",
      "[0.09041907]\n",
      "Checking accuracy on test set\n",
      "Got 700 / 1000 correct (70.00)\n",
      "336 epoch,   500 iteration, loss:0.082\n",
      "Checking accuracy on test set\n",
      "Got 671 / 1000 correct (67.10)\n",
      "336 epoch,  1000 iteration, loss:0.085\n",
      " num 335 epoch \n",
      "####### Training Loss #######\n",
      "[0.08370496]\n",
      "Checking accuracy on test set\n",
      "Got 665 / 1000 correct (66.50)\n",
      "337 epoch,   500 iteration, loss:0.062\n",
      "Checking accuracy on test set\n",
      "Got 719 / 1000 correct (71.90)\n",
      "337 epoch,  1000 iteration, loss:0.084\n",
      " num 336 epoch \n",
      "####### Training Loss #######\n",
      "[0.07586314]\n",
      "Checking accuracy on test set\n",
      "Got 691 / 1000 correct (69.10)\n",
      "338 epoch,   500 iteration, loss:0.090\n",
      "Checking accuracy on test set\n",
      "Got 704 / 1000 correct (70.40)\n",
      "338 epoch,  1000 iteration, loss:0.070\n",
      " num 337 epoch \n",
      "####### Training Loss #######\n",
      "[0.07921587]\n",
      "Checking accuracy on test set\n",
      "Got 686 / 1000 correct (68.60)\n",
      "339 epoch,   500 iteration, loss:0.077\n",
      "Checking accuracy on test set\n",
      "Got 691 / 1000 correct (69.10)\n",
      "339 epoch,  1000 iteration, loss:0.098\n",
      " num 338 epoch \n",
      "####### Training Loss #######\n",
      "[0.09511022]\n",
      "Checking accuracy on test set\n",
      "Got 703 / 1000 correct (70.30)\n",
      "340 epoch,   500 iteration, loss:0.077\n",
      "Checking accuracy on test set\n",
      "Got 691 / 1000 correct (69.10)\n",
      "340 epoch,  1000 iteration, loss:0.092\n",
      " num 339 epoch \n",
      "####### Training Loss #######\n",
      "[0.08855749]\n",
      "Checking accuracy on test set\n",
      "Got 692 / 1000 correct (69.20)\n",
      "341 epoch,   500 iteration, loss:0.079\n",
      "Checking accuracy on test set\n",
      "Got 712 / 1000 correct (71.20)\n",
      "341 epoch,  1000 iteration, loss:0.088\n",
      " num 340 epoch \n",
      "####### Training Loss #######\n",
      "[0.08101494]\n",
      "Checking accuracy on test set\n",
      "Got 702 / 1000 correct (70.20)\n",
      "342 epoch,   500 iteration, loss:0.087\n",
      "Checking accuracy on test set\n",
      "Got 704 / 1000 correct (70.40)\n",
      "342 epoch,  1000 iteration, loss:0.093\n",
      " num 341 epoch \n",
      "####### Training Loss #######\n",
      "[0.08572646]\n",
      "Checking accuracy on test set\n",
      "Got 693 / 1000 correct (69.30)\n",
      "343 epoch,   500 iteration, loss:0.077\n",
      "Checking accuracy on test set\n",
      "Got 687 / 1000 correct (68.70)\n",
      "343 epoch,  1000 iteration, loss:0.088\n",
      " num 342 epoch \n",
      "####### Training Loss #######\n",
      "[0.08312954]\n",
      "Checking accuracy on test set\n",
      "Got 725 / 1000 correct (72.50)\n",
      "344 epoch,   500 iteration, loss:0.079\n",
      "Checking accuracy on test set\n",
      "Got 692 / 1000 correct (69.20)\n",
      "344 epoch,  1000 iteration, loss:0.079\n",
      " num 343 epoch \n",
      "####### Training Loss #######\n",
      "[0.0919408]\n",
      "Checking accuracy on test set\n",
      "Got 670 / 1000 correct (67.00)\n",
      "345 epoch,   500 iteration, loss:0.073\n",
      "Checking accuracy on test set\n",
      "Got 705 / 1000 correct (70.50)\n",
      "345 epoch,  1000 iteration, loss:0.100\n",
      " num 344 epoch \n",
      "####### Training Loss #######\n",
      "[0.08818722]\n",
      "Checking accuracy on test set\n",
      "Got 688 / 1000 correct (68.80)\n",
      "346 epoch,   500 iteration, loss:0.103\n",
      "Checking accuracy on test set\n",
      "Got 688 / 1000 correct (68.80)\n",
      "346 epoch,  1000 iteration, loss:0.071\n",
      " num 345 epoch \n",
      "####### Training Loss #######\n",
      "[0.08422934]\n",
      "Checking accuracy on test set\n",
      "Got 693 / 1000 correct (69.30)\n",
      "347 epoch,   500 iteration, loss:0.072\n",
      "Checking accuracy on test set\n",
      "Got 705 / 1000 correct (70.50)\n",
      "347 epoch,  1000 iteration, loss:0.082\n",
      " num 346 epoch \n",
      "####### Training Loss #######\n",
      "[0.07666529]\n",
      "Checking accuracy on test set\n",
      "Got 694 / 1000 correct (69.40)\n",
      "348 epoch,   500 iteration, loss:0.080\n",
      "Checking accuracy on test set\n",
      "Got 711 / 1000 correct (71.10)\n",
      "348 epoch,  1000 iteration, loss:0.089\n",
      " num 347 epoch \n",
      "####### Training Loss #######\n",
      "[0.08348149]\n",
      "Checking accuracy on test set\n",
      "Got 703 / 1000 correct (70.30)\n",
      "349 epoch,   500 iteration, loss:0.098\n",
      "Checking accuracy on test set\n",
      "Got 709 / 1000 correct (70.90)\n",
      "349 epoch,  1000 iteration, loss:0.073\n",
      " num 348 epoch \n",
      "####### Training Loss #######\n",
      "[0.08281988]\n",
      "Checking accuracy on test set\n",
      "Got 692 / 1000 correct (69.20)\n",
      "350 epoch,   500 iteration, loss:0.068\n",
      "Checking accuracy on test set\n",
      "Got 677 / 1000 correct (67.70)\n",
      "350 epoch,  1000 iteration, loss:0.089\n",
      " num 349 epoch \n",
      "####### Training Loss #######\n",
      "[0.08059052]\n",
      "finish training \n",
      "\n",
      "now begin saving datum for next step plotting\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_acc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-1eabc65f617c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnet_name2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'ALL_CNN_C_Class2'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m all_cnn_c_class2 = running_model_B(run_num, all_cnn_c_class2, net_name2, \n\u001b[0;32m----> 4\u001b[0;31m                         lr, epoch, loaderB_train, loaderB_test)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-4a83535c2f7e>\u001b[0m in \u001b[0;36mrunning_model_B\u001b[0;34m(run_num, net, net_name, lr_list, epoch_list, loader_train, loader_test)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/test_acc.save'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m     \u001b[0mcPickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcPickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHIGHEST_PROTOCOL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_acc' is not defined"
     ]
    }
   ],
   "source": [
    "all_cnn_c_class2 = new_ALL_Conv.Stride_CNN_C()\n",
    "net_name2 = 'ALL_CNN_C_Class2'\n",
    "all_cnn_c_class2 = running_model_B(run_num, all_cnn_c_class2, net_name2, \n",
    "                        lr, epoch, loaderB_train, loaderB_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from six.moves import cPickle\n",
    "\n",
    "save_path = '../datum_for_plotting/run_num_10/ALL_CNN_C_Class2'\n",
    "f =open(save_path + '/train_loss.save' , 'rb')\n",
    "loss = cPickle.load(f )\n",
    "f.close()\n",
    "f =open(save_path + '/array_epoch_acc.save' , 'rb')\n",
    "acc_array = cPickle.load(f )\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'loss')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "max_epoch = 350\n",
    "itern_axis_train = np.array(np.linspace(1,max_epoch,num=max_epoch))\n",
    "\n",
    "plt.plot(itern_axis_train, loss,'-b.', label='Train')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = np.concatenate(acc_array)\n",
    "length = a.shape[0]\n",
    "a /= 100\n",
    "acc_axis_test = np.array(np.linspace(1,length, num=length))\n",
    "plt.plot(acc_axis_test, a.reshape(-1,1), '--r', label='Test')\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('loss')\n",
    "plt.savefig(save_path + '/validation_accuracy' + str(max_epoch) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n",
      "begin training\n",
      "Checking accuracy on test set\n",
      "Got 204 / 1000 correct (20.40)\n",
      "1 epoch,   500 iteration, loss:2.278\n",
      "Checking accuracy on test set\n",
      "Got 225 / 1000 correct (22.50)\n",
      "1 epoch,  1000 iteration, loss:2.187\n",
      " num 0 epoch \n",
      "####### Training Loss #######\n",
      "[2.21077985]\n",
      "Checking accuracy on test set\n",
      "Got 272 / 1000 correct (27.20)\n",
      "2 epoch,   500 iteration, loss:2.088\n",
      "Checking accuracy on test set\n",
      "Got 350 / 1000 correct (35.00)\n",
      "2 epoch,  1000 iteration, loss:2.042\n",
      " num 1 epoch \n",
      "####### Training Loss #######\n",
      "[2.04229618]\n",
      "Checking accuracy on test set\n",
      "Got 318 / 1000 correct (31.80)\n",
      "3 epoch,   500 iteration, loss:1.962\n",
      "Checking accuracy on test set\n",
      "Got 367 / 1000 correct (36.70)\n",
      "3 epoch,  1000 iteration, loss:1.904\n",
      " num 2 epoch \n",
      "####### Training Loss #######\n",
      "[1.90382036]\n",
      "Checking accuracy on test set\n",
      "Got 394 / 1000 correct (39.40)\n",
      "4 epoch,   500 iteration, loss:1.840\n",
      "Checking accuracy on test set\n",
      "Got 417 / 1000 correct (41.70)\n",
      "4 epoch,  1000 iteration, loss:1.788\n",
      " num 3 epoch \n",
      "####### Training Loss #######\n",
      "[1.79092711]\n",
      "Checking accuracy on test set\n",
      "Got 419 / 1000 correct (41.90)\n",
      "5 epoch,   500 iteration, loss:1.734\n",
      "Checking accuracy on test set\n",
      "Got 439 / 1000 correct (43.90)\n",
      "5 epoch,  1000 iteration, loss:1.691\n",
      " num 4 epoch \n",
      "####### Training Loss #######\n",
      "[1.68318742]\n",
      "Checking accuracy on test set\n",
      "Got 431 / 1000 correct (43.10)\n",
      "6 epoch,   500 iteration, loss:1.648\n",
      "Checking accuracy on test set\n",
      "Got 491 / 1000 correct (49.10)\n",
      "6 epoch,  1000 iteration, loss:1.607\n",
      " num 5 epoch \n",
      "####### Training Loss #######\n",
      "[1.60301791]\n",
      "Checking accuracy on test set\n",
      "Got 448 / 1000 correct (44.80)\n",
      "7 epoch,   500 iteration, loss:1.553\n",
      "Checking accuracy on test set\n",
      "Got 489 / 1000 correct (48.90)\n",
      "7 epoch,  1000 iteration, loss:1.525\n",
      " num 6 epoch \n",
      "####### Training Loss #######\n",
      "[1.5155015]\n",
      "Checking accuracy on test set\n",
      "Got 467 / 1000 correct (46.70)\n",
      "8 epoch,   500 iteration, loss:1.499\n",
      "Checking accuracy on test set\n",
      "Got 523 / 1000 correct (52.30)\n",
      "8 epoch,  1000 iteration, loss:1.443\n",
      " num 7 epoch \n",
      "####### Training Loss #######\n",
      "[1.44497425]\n",
      "Checking accuracy on test set\n",
      "Got 517 / 1000 correct (51.70)\n",
      "9 epoch,   500 iteration, loss:1.434\n",
      "Checking accuracy on test set\n",
      "Got 515 / 1000 correct (51.50)\n",
      "9 epoch,  1000 iteration, loss:1.405\n",
      " num 8 epoch \n",
      "####### Training Loss #######\n",
      "[1.3862505]\n",
      "Checking accuracy on test set\n",
      "Got 518 / 1000 correct (51.80)\n",
      "10 epoch,   500 iteration, loss:1.374\n",
      "Checking accuracy on test set\n",
      "Got 543 / 1000 correct (54.30)\n",
      "10 epoch,  1000 iteration, loss:1.331\n",
      " num 9 epoch \n",
      "####### Training Loss #######\n",
      "[1.32693609]\n",
      "Checking accuracy on test set\n",
      "Got 555 / 1000 correct (55.50)\n",
      "11 epoch,   500 iteration, loss:1.309\n",
      "Checking accuracy on test set\n",
      "Got 554 / 1000 correct (55.40)\n",
      "11 epoch,  1000 iteration, loss:1.304\n",
      " num 10 epoch \n",
      "####### Training Loss #######\n",
      "[1.28280752]\n",
      "Checking accuracy on test set\n",
      "Got 562 / 1000 correct (56.20)\n",
      "12 epoch,   500 iteration, loss:1.280\n",
      "Checking accuracy on test set\n",
      "Got 573 / 1000 correct (57.30)\n",
      "12 epoch,  1000 iteration, loss:1.257\n",
      " num 11 epoch \n",
      "####### Training Loss #######\n",
      "[1.23969941]\n",
      "Checking accuracy on test set\n",
      "Got 587 / 1000 correct (58.70)\n",
      "13 epoch,   500 iteration, loss:1.229\n",
      "Checking accuracy on test set\n",
      "Got 576 / 1000 correct (57.60)\n",
      "13 epoch,  1000 iteration, loss:1.225\n",
      " num 12 epoch \n",
      "####### Training Loss #######\n",
      "[1.19901961]\n",
      "Checking accuracy on test set\n",
      "Got 592 / 1000 correct (59.20)\n",
      "14 epoch,   500 iteration, loss:1.186\n",
      "Checking accuracy on test set\n",
      "Got 576 / 1000 correct (57.60)\n",
      "14 epoch,  1000 iteration, loss:1.157\n",
      " num 13 epoch \n",
      "####### Training Loss #######\n",
      "[1.15338767]\n",
      "Checking accuracy on test set\n",
      "Got 607 / 1000 correct (60.70)\n",
      "15 epoch,   500 iteration, loss:1.156\n",
      "Checking accuracy on test set\n",
      "Got 616 / 1000 correct (61.60)\n",
      "15 epoch,  1000 iteration, loss:1.112\n",
      " num 14 epoch \n",
      "####### Training Loss #######\n",
      "[1.11243364]\n",
      "Checking accuracy on test set\n",
      "Got 594 / 1000 correct (59.40)\n",
      "16 epoch,   500 iteration, loss:1.112\n",
      "Checking accuracy on test set\n",
      "Got 600 / 1000 correct (60.00)\n",
      "16 epoch,  1000 iteration, loss:1.109\n",
      " num 15 epoch \n",
      "####### Training Loss #######\n",
      "[1.08490109]\n",
      "Checking accuracy on test set\n",
      "Got 608 / 1000 correct (60.80)\n",
      "17 epoch,   500 iteration, loss:1.075\n",
      "Checking accuracy on test set\n",
      "Got 607 / 1000 correct (60.70)\n",
      "17 epoch,  1000 iteration, loss:1.053\n",
      " num 16 epoch \n",
      "####### Training Loss #######\n",
      "[1.04288299]\n",
      "Checking accuracy on test set\n",
      "Got 631 / 1000 correct (63.10)\n",
      "18 epoch,   500 iteration, loss:1.032\n",
      "Checking accuracy on test set\n",
      "Got 621 / 1000 correct (62.10)\n",
      "18 epoch,  1000 iteration, loss:1.039\n",
      " num 17 epoch \n",
      "####### Training Loss #######\n",
      "[1.01167229]\n",
      "Checking accuracy on test set\n",
      "Got 630 / 1000 correct (63.00)\n",
      "19 epoch,   500 iteration, loss:1.022\n",
      "Checking accuracy on test set\n",
      "Got 605 / 1000 correct (60.50)\n",
      "19 epoch,  1000 iteration, loss:0.998\n",
      " num 18 epoch \n",
      "####### Training Loss #######\n",
      "[0.97681249]\n",
      "Checking accuracy on test set\n",
      "Got 623 / 1000 correct (62.30)\n",
      "20 epoch,   500 iteration, loss:0.978\n",
      "Checking accuracy on test set\n",
      "Got 608 / 1000 correct (60.80)\n",
      "20 epoch,  1000 iteration, loss:0.933\n",
      " num 19 epoch \n",
      "####### Training Loss #######\n",
      "[0.93614114]\n",
      "Checking accuracy on test set\n",
      "Got 603 / 1000 correct (60.30)\n",
      "21 epoch,   500 iteration, loss:0.918\n",
      "Checking accuracy on test set\n",
      "Got 616 / 1000 correct (61.60)\n",
      "21 epoch,  1000 iteration, loss:0.934\n",
      " num 20 epoch \n",
      "####### Training Loss #######\n",
      "[0.90104708]\n",
      "Checking accuracy on test set\n",
      "Got 627 / 1000 correct (62.70)\n",
      "22 epoch,   500 iteration, loss:0.896\n",
      "Checking accuracy on test set\n",
      "Got 611 / 1000 correct (61.10)\n",
      "22 epoch,  1000 iteration, loss:0.886\n",
      " num 21 epoch \n",
      "####### Training Loss #######\n",
      "[0.87223848]\n",
      "Checking accuracy on test set\n",
      "Got 636 / 1000 correct (63.60)\n",
      "23 epoch,   500 iteration, loss:0.893\n",
      "Checking accuracy on test set\n",
      "Got 620 / 1000 correct (62.00)\n",
      "23 epoch,  1000 iteration, loss:0.864\n",
      " num 22 epoch \n",
      "####### Training Loss #######\n",
      "[0.86166332]\n",
      "Checking accuracy on test set\n",
      "Got 634 / 1000 correct (63.40)\n",
      "24 epoch,   500 iteration, loss:0.827\n",
      "Checking accuracy on test set\n",
      "Got 624 / 1000 correct (62.40)\n",
      "24 epoch,  1000 iteration, loss:0.834\n",
      " num 23 epoch \n",
      "####### Training Loss #######\n",
      "[0.80856687]\n",
      "Checking accuracy on test set\n",
      "Got 636 / 1000 correct (63.60)\n",
      "25 epoch,   500 iteration, loss:0.799\n",
      "Checking accuracy on test set\n",
      "Got 613 / 1000 correct (61.30)\n",
      "25 epoch,  1000 iteration, loss:0.791\n",
      " num 24 epoch \n",
      "####### Training Loss #######\n",
      "[0.77912981]\n",
      "Checking accuracy on test set\n",
      "Got 654 / 1000 correct (65.40)\n",
      "26 epoch,   500 iteration, loss:0.787\n",
      "Checking accuracy on test set\n",
      "Got 607 / 1000 correct (60.70)\n",
      "26 epoch,  1000 iteration, loss:0.787\n",
      " num 25 epoch \n",
      "####### Training Loss #######\n",
      "[0.7641979]\n",
      "Checking accuracy on test set\n",
      "Got 662 / 1000 correct (66.20)\n",
      "27 epoch,   500 iteration, loss:0.762\n",
      "Checking accuracy on test set\n",
      "Got 634 / 1000 correct (63.40)\n",
      "27 epoch,  1000 iteration, loss:0.726\n",
      " num 26 epoch \n",
      "####### Training Loss #######\n",
      "[0.73087649]\n",
      "Checking accuracy on test set\n",
      "Got 663 / 1000 correct (66.30)\n",
      "28 epoch,   500 iteration, loss:0.732\n",
      "Checking accuracy on test set\n",
      "Got 646 / 1000 correct (64.60)\n",
      "28 epoch,  1000 iteration, loss:0.686\n",
      " num 27 epoch \n",
      "####### Training Loss #######\n",
      "[0.68305437]\n",
      "Checking accuracy on test set\n",
      "Got 650 / 1000 correct (65.00)\n",
      "29 epoch,   500 iteration, loss:0.720\n",
      "Checking accuracy on test set\n",
      "Got 671 / 1000 correct (67.10)\n",
      "29 epoch,  1000 iteration, loss:0.681\n",
      " num 28 epoch \n",
      "####### Training Loss #######\n",
      "[0.67661086]\n",
      "Checking accuracy on test set\n",
      "Got 674 / 1000 correct (67.40)\n",
      "30 epoch,   500 iteration, loss:0.714\n",
      "Checking accuracy on test set\n",
      "Got 640 / 1000 correct (64.00)\n",
      "30 epoch,  1000 iteration, loss:0.637\n",
      " num 29 epoch \n",
      "####### Training Loss #######\n",
      "[0.66477099]\n",
      "Checking accuracy on test set\n",
      "Got 651 / 1000 correct (65.10)\n",
      "31 epoch,   500 iteration, loss:0.685\n",
      "Checking accuracy on test set\n",
      "Got 627 / 1000 correct (62.70)\n",
      "31 epoch,  1000 iteration, loss:0.621\n",
      " num 30 epoch \n",
      "####### Training Loss #######\n",
      "[0.63709877]\n",
      "Checking accuracy on test set\n",
      "Got 668 / 1000 correct (66.80)\n",
      "32 epoch,   500 iteration, loss:0.627\n",
      "Checking accuracy on test set\n",
      "Got 638 / 1000 correct (63.80)\n",
      "32 epoch,  1000 iteration, loss:0.610\n",
      " num 31 epoch \n",
      "####### Training Loss #######\n",
      "[0.60924173]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking accuracy on test set\n",
      "Got 675 / 1000 correct (67.50)\n",
      "33 epoch,   500 iteration, loss:0.607\n",
      "Checking accuracy on test set\n",
      "Got 659 / 1000 correct (65.90)\n",
      "33 epoch,  1000 iteration, loss:0.569\n",
      " num 32 epoch \n",
      "####### Training Loss #######\n",
      "[0.57597492]\n",
      "Checking accuracy on test set\n",
      "Got 665 / 1000 correct (66.50)\n",
      "34 epoch,   500 iteration, loss:0.543\n",
      "Checking accuracy on test set\n",
      "Got 643 / 1000 correct (64.30)\n",
      "34 epoch,  1000 iteration, loss:0.551\n",
      " num 33 epoch \n",
      "####### Training Loss #######\n",
      "[0.53388961]\n",
      "Checking accuracy on test set\n",
      "Got 665 / 1000 correct (66.50)\n",
      "35 epoch,   500 iteration, loss:0.544\n",
      "Checking accuracy on test set\n",
      "Got 646 / 1000 correct (64.60)\n",
      "35 epoch,  1000 iteration, loss:0.548\n",
      " num 34 epoch \n",
      "####### Training Loss #######\n",
      "[0.54022706]\n",
      "Checking accuracy on test set\n",
      "Got 635 / 1000 correct (63.50)\n",
      "36 epoch,   500 iteration, loss:0.536\n",
      "Checking accuracy on test set\n",
      "Got 662 / 1000 correct (66.20)\n",
      "36 epoch,  1000 iteration, loss:0.528\n",
      " num 35 epoch \n",
      "####### Training Loss #######\n",
      "[0.52197621]\n",
      "Checking accuracy on test set\n",
      "Got 653 / 1000 correct (65.30)\n",
      "37 epoch,   500 iteration, loss:0.548\n",
      "Checking accuracy on test set\n",
      "Got 653 / 1000 correct (65.30)\n",
      "37 epoch,  1000 iteration, loss:0.532\n",
      " num 36 epoch \n",
      "####### Training Loss #######\n",
      "[0.52337407]\n",
      "Checking accuracy on test set\n",
      "Got 654 / 1000 correct (65.40)\n",
      "38 epoch,   500 iteration, loss:0.485\n",
      "Checking accuracy on test set\n",
      "Got 674 / 1000 correct (67.40)\n",
      "38 epoch,  1000 iteration, loss:0.494\n",
      " num 37 epoch \n",
      "####### Training Loss #######\n",
      "[0.47999612]\n",
      "Checking accuracy on test set\n",
      "Got 653 / 1000 correct (65.30)\n",
      "39 epoch,   500 iteration, loss:0.466\n",
      "Checking accuracy on test set\n",
      "Got 671 / 1000 correct (67.10)\n",
      "39 epoch,  1000 iteration, loss:0.477\n",
      " num 38 epoch \n",
      "####### Training Loss #######\n",
      "[0.46468447]\n",
      "Checking accuracy on test set\n",
      "Got 624 / 1000 correct (62.40)\n",
      "40 epoch,   500 iteration, loss:0.457\n",
      "Checking accuracy on test set\n",
      "Got 651 / 1000 correct (65.10)\n",
      "40 epoch,  1000 iteration, loss:0.450\n",
      " num 39 epoch \n",
      "####### Training Loss #######\n",
      "[0.449327]\n",
      "Checking accuracy on test set\n",
      "Got 649 / 1000 correct (64.90)\n",
      "41 epoch,   500 iteration, loss:0.419\n",
      "Checking accuracy on test set\n",
      "Got 663 / 1000 correct (66.30)\n",
      "41 epoch,  1000 iteration, loss:0.431\n",
      " num 40 epoch \n",
      "####### Training Loss #######\n",
      "[0.42168136]\n",
      "Checking accuracy on test set\n",
      "Got 677 / 1000 correct (67.70)\n",
      "42 epoch,   500 iteration, loss:0.433\n",
      "Checking accuracy on test set\n",
      "Got 668 / 1000 correct (66.80)\n",
      "42 epoch,  1000 iteration, loss:0.446\n",
      " num 41 epoch \n",
      "####### Training Loss #######\n",
      "[0.43277353]\n",
      "Checking accuracy on test set\n",
      "Got 658 / 1000 correct (65.80)\n",
      "43 epoch,   500 iteration, loss:0.391\n",
      "Checking accuracy on test set\n",
      "Got 658 / 1000 correct (65.80)\n",
      "43 epoch,  1000 iteration, loss:0.413\n",
      " num 42 epoch \n",
      "####### Training Loss #######\n",
      "[0.39620392]\n",
      "Checking accuracy on test set\n",
      "Got 670 / 1000 correct (67.00)\n",
      "44 epoch,   500 iteration, loss:0.377\n",
      "Checking accuracy on test set\n",
      "Got 678 / 1000 correct (67.80)\n",
      "44 epoch,  1000 iteration, loss:0.389\n",
      " num 43 epoch \n",
      "####### Training Loss #######\n",
      "[0.37543154]\n",
      "Checking accuracy on test set\n",
      "Got 649 / 1000 correct (64.90)\n",
      "45 epoch,   500 iteration, loss:0.383\n",
      "Checking accuracy on test set\n",
      "Got 675 / 1000 correct (67.50)\n",
      "45 epoch,  1000 iteration, loss:0.400\n",
      " num 44 epoch \n",
      "####### Training Loss #######\n",
      "[0.383623]\n",
      "Checking accuracy on test set\n",
      "Got 667 / 1000 correct (66.70)\n",
      "46 epoch,   500 iteration, loss:0.363\n",
      "Checking accuracy on test set\n",
      "Got 681 / 1000 correct (68.10)\n",
      "46 epoch,  1000 iteration, loss:0.357\n",
      " num 45 epoch \n",
      "####### Training Loss #######\n",
      "[0.35736284]\n",
      "Checking accuracy on test set\n",
      "Got 675 / 1000 correct (67.50)\n",
      "47 epoch,   500 iteration, loss:0.384\n",
      "Checking accuracy on test set\n",
      "Got 673 / 1000 correct (67.30)\n",
      "47 epoch,  1000 iteration, loss:0.357\n",
      " num 46 epoch \n",
      "####### Training Loss #######\n",
      "[0.3643141]\n",
      "Checking accuracy on test set\n",
      "Got 654 / 1000 correct (65.40)\n",
      "48 epoch,   500 iteration, loss:0.346\n",
      "Checking accuracy on test set\n",
      "Got 679 / 1000 correct (67.90)\n",
      "48 epoch,  1000 iteration, loss:0.339\n",
      " num 47 epoch \n",
      "####### Training Loss #######\n",
      "[0.33906958]\n",
      "Checking accuracy on test set\n",
      "Got 678 / 1000 correct (67.80)\n",
      "49 epoch,   500 iteration, loss:0.347\n",
      "Checking accuracy on test set\n",
      "Got 639 / 1000 correct (63.90)\n",
      "49 epoch,  1000 iteration, loss:0.307\n",
      " num 48 epoch \n",
      "####### Training Loss #######\n",
      "[0.3219216]\n",
      "Checking accuracy on test set\n",
      "Got 677 / 1000 correct (67.70)\n",
      "50 epoch,   500 iteration, loss:0.329\n",
      "Checking accuracy on test set\n",
      "Got 662 / 1000 correct (66.20)\n",
      "50 epoch,  1000 iteration, loss:0.299\n",
      " num 49 epoch \n",
      "####### Training Loss #######\n",
      "[0.3099172]\n",
      "Checking accuracy on test set\n",
      "Got 660 / 1000 correct (66.00)\n",
      "51 epoch,   500 iteration, loss:0.299\n",
      "Checking accuracy on test set\n",
      "Got 644 / 1000 correct (64.40)\n",
      "51 epoch,  1000 iteration, loss:0.310\n",
      " num 50 epoch \n",
      "####### Training Loss #######\n",
      "[0.2966502]\n",
      "Checking accuracy on test set\n",
      "Got 644 / 1000 correct (64.40)\n",
      "52 epoch,   500 iteration, loss:0.315\n",
      "Checking accuracy on test set\n",
      "Got 661 / 1000 correct (66.10)\n",
      "52 epoch,  1000 iteration, loss:0.318\n",
      " num 51 epoch \n",
      "####### Training Loss #######\n",
      "[0.31016544]\n",
      "Checking accuracy on test set\n",
      "Got 671 / 1000 correct (67.10)\n",
      "53 epoch,   500 iteration, loss:0.319\n",
      "Checking accuracy on test set\n",
      "Got 672 / 1000 correct (67.20)\n",
      "53 epoch,  1000 iteration, loss:0.278\n",
      " num 52 epoch \n",
      "####### Training Loss #######\n",
      "[0.30257162]\n",
      "Checking accuracy on test set\n",
      "Got 656 / 1000 correct (65.60)\n",
      "54 epoch,   500 iteration, loss:0.272\n",
      "Checking accuracy on test set\n",
      "Got 682 / 1000 correct (68.20)\n",
      "54 epoch,  1000 iteration, loss:0.326\n",
      " num 53 epoch \n",
      "####### Training Loss #######\n",
      "[0.29812482]\n",
      "Checking accuracy on test set\n",
      "Got 659 / 1000 correct (65.90)\n",
      "55 epoch,   500 iteration, loss:0.279\n",
      "Checking accuracy on test set\n",
      "Got 608 / 1000 correct (60.80)\n",
      "55 epoch,  1000 iteration, loss:0.295\n",
      " num 54 epoch \n",
      "####### Training Loss #######\n",
      "[0.29039899]\n",
      "Checking accuracy on test set\n",
      "Got 682 / 1000 correct (68.20)\n",
      "56 epoch,   500 iteration, loss:0.306\n",
      "Checking accuracy on test set\n",
      "Got 679 / 1000 correct (67.90)\n",
      "56 epoch,  1000 iteration, loss:0.261\n",
      " num 55 epoch \n",
      "####### Training Loss #######\n",
      "[0.27434221]\n",
      "Checking accuracy on test set\n",
      "Got 655 / 1000 correct (65.50)\n",
      "57 epoch,   500 iteration, loss:0.274\n",
      "Checking accuracy on test set\n",
      "Got 616 / 1000 correct (61.60)\n",
      "57 epoch,  1000 iteration, loss:0.297\n",
      " num 56 epoch \n",
      "####### Training Loss #######\n",
      "[0.26511788]\n",
      "Checking accuracy on test set\n",
      "Got 656 / 1000 correct (65.60)\n",
      "58 epoch,   500 iteration, loss:0.243\n",
      "Checking accuracy on test set\n",
      "Got 676 / 1000 correct (67.60)\n",
      "58 epoch,  1000 iteration, loss:0.262\n",
      " num 57 epoch \n",
      "####### Training Loss #######\n",
      "[0.24841005]\n",
      "Checking accuracy on test set\n",
      "Got 658 / 1000 correct (65.80)\n",
      "59 epoch,   500 iteration, loss:0.277\n",
      "Checking accuracy on test set\n",
      "Got 663 / 1000 correct (66.30)\n",
      "59 epoch,  1000 iteration, loss:0.234\n",
      " num 58 epoch \n",
      "####### Training Loss #######\n",
      "[0.26412356]\n",
      "Checking accuracy on test set\n",
      "Got 630 / 1000 correct (63.00)\n",
      "60 epoch,   500 iteration, loss:0.257\n",
      "Checking accuracy on test set\n",
      "Got 680 / 1000 correct (68.00)\n",
      "60 epoch,  1000 iteration, loss:0.268\n",
      " num 59 epoch \n",
      "####### Training Loss #######\n",
      "[0.25405569]\n",
      "Checking accuracy on test set\n",
      "Got 660 / 1000 correct (66.00)\n",
      "61 epoch,   500 iteration, loss:0.256\n",
      "Checking accuracy on test set\n",
      "Got 688 / 1000 correct (68.80)\n",
      "61 epoch,  1000 iteration, loss:0.228\n",
      " num 60 epoch \n",
      "####### Training Loss #######\n",
      "[0.23928418]\n",
      "Checking accuracy on test set\n",
      "Got 686 / 1000 correct (68.60)\n",
      "62 epoch,   500 iteration, loss:0.217\n",
      "Checking accuracy on test set\n",
      "Got 680 / 1000 correct (68.00)\n",
      "62 epoch,  1000 iteration, loss:0.207\n",
      " num 61 epoch \n",
      "####### Training Loss #######\n",
      "[0.21434177]\n",
      "Checking accuracy on test set\n",
      "Got 666 / 1000 correct (66.60)\n",
      "63 epoch,   500 iteration, loss:0.215\n",
      "Checking accuracy on test set\n",
      "Got 665 / 1000 correct (66.50)\n",
      "63 epoch,  1000 iteration, loss:0.207\n",
      " num 62 epoch \n",
      "####### Training Loss #######\n",
      "[0.22371036]\n",
      "Checking accuracy on test set\n",
      "Got 668 / 1000 correct (66.80)\n",
      "64 epoch,   500 iteration, loss:0.202\n",
      "Checking accuracy on test set\n",
      "Got 662 / 1000 correct (66.20)\n",
      "64 epoch,  1000 iteration, loss:0.213\n",
      " num 63 epoch \n",
      "####### Training Loss #######\n",
      "[0.2169352]\n",
      "Checking accuracy on test set\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 661 / 1000 correct (66.10)\n",
      "65 epoch,   500 iteration, loss:0.204\n",
      "Checking accuracy on test set\n",
      "Got 654 / 1000 correct (65.40)\n",
      "65 epoch,  1000 iteration, loss:0.191\n",
      " num 64 epoch \n",
      "####### Training Loss #######\n",
      "[0.19949999]\n",
      "Checking accuracy on test set\n",
      "Got 678 / 1000 correct (67.80)\n",
      "66 epoch,   500 iteration, loss:0.181\n",
      "Checking accuracy on test set\n",
      "Got 642 / 1000 correct (64.20)\n",
      "66 epoch,  1000 iteration, loss:0.194\n",
      " num 65 epoch \n",
      "####### Training Loss #######\n",
      "[0.19813137]\n",
      "Checking accuracy on test set\n",
      "Got 641 / 1000 correct (64.10)\n",
      "67 epoch,   500 iteration, loss:0.205\n",
      "Checking accuracy on test set\n",
      "Got 670 / 1000 correct (67.00)\n",
      "67 epoch,  1000 iteration, loss:0.227\n",
      " num 66 epoch \n",
      "####### Training Loss #######\n",
      "[0.21384067]\n",
      "Checking accuracy on test set\n",
      "Got 668 / 1000 correct (66.80)\n",
      "68 epoch,   500 iteration, loss:0.189\n",
      "Checking accuracy on test set\n",
      "Got 700 / 1000 correct (70.00)\n",
      "68 epoch,  1000 iteration, loss:0.184\n",
      " num 67 epoch \n",
      "####### Training Loss #######\n",
      "[0.18560464]\n",
      "Checking accuracy on test set\n",
      "Got 658 / 1000 correct (65.80)\n",
      "69 epoch,   500 iteration, loss:0.213\n",
      "Checking accuracy on test set\n",
      "Got 662 / 1000 correct (66.20)\n",
      "69 epoch,  1000 iteration, loss:0.187\n",
      " num 68 epoch \n",
      "####### Training Loss #######\n",
      "[0.21032011]\n",
      "Checking accuracy on test set\n",
      "Got 649 / 1000 correct (64.90)\n",
      "70 epoch,   500 iteration, loss:0.199\n",
      "Checking accuracy on test set\n",
      "Got 678 / 1000 correct (67.80)\n",
      "70 epoch,  1000 iteration, loss:0.172\n",
      " num 69 epoch \n",
      "####### Training Loss #######\n",
      "[0.17709795]\n",
      "Checking accuracy on test set\n",
      "Got 655 / 1000 correct (65.50)\n",
      "71 epoch,   500 iteration, loss:0.180\n",
      "Checking accuracy on test set\n",
      "Got 675 / 1000 correct (67.50)\n",
      "71 epoch,  1000 iteration, loss:0.213\n",
      " num 70 epoch \n",
      "####### Training Loss #######\n",
      "[0.18579004]\n",
      "Checking accuracy on test set\n",
      "Got 660 / 1000 correct (66.00)\n",
      "72 epoch,   500 iteration, loss:0.185\n",
      "Checking accuracy on test set\n",
      "Got 656 / 1000 correct (65.60)\n",
      "72 epoch,  1000 iteration, loss:0.184\n",
      " num 71 epoch \n",
      "####### Training Loss #######\n",
      "[0.18851057]\n",
      "Checking accuracy on test set\n",
      "Got 658 / 1000 correct (65.80)\n",
      "73 epoch,   500 iteration, loss:0.179\n",
      "Checking accuracy on test set\n",
      "Got 677 / 1000 correct (67.70)\n",
      "73 epoch,  1000 iteration, loss:0.185\n",
      " num 72 epoch \n",
      "####### Training Loss #######\n",
      "[0.17594397]\n",
      "Checking accuracy on test set\n",
      "Got 676 / 1000 correct (67.60)\n",
      "74 epoch,   500 iteration, loss:0.212\n",
      "Checking accuracy on test set\n",
      "Got 669 / 1000 correct (66.90)\n",
      "74 epoch,  1000 iteration, loss:0.191\n",
      " num 73 epoch \n",
      "####### Training Loss #######\n",
      "[0.1937135]\n",
      "Checking accuracy on test set\n",
      "Got 684 / 1000 correct (68.40)\n",
      "75 epoch,   500 iteration, loss:0.200\n",
      "Checking accuracy on test set\n",
      "Got 682 / 1000 correct (68.20)\n",
      "75 epoch,  1000 iteration, loss:0.138\n",
      " num 74 epoch \n",
      "####### Training Loss #######\n",
      "[0.16721913]\n",
      "Checking accuracy on test set\n",
      "Got 638 / 1000 correct (63.80)\n",
      "76 epoch,   500 iteration, loss:0.165\n",
      "Checking accuracy on test set\n",
      "Got 653 / 1000 correct (65.30)\n",
      "76 epoch,  1000 iteration, loss:0.170\n",
      " num 75 epoch \n",
      "####### Training Loss #######\n",
      "[0.1655979]\n",
      "Checking accuracy on test set\n",
      "Got 666 / 1000 correct (66.60)\n",
      "77 epoch,   500 iteration, loss:0.152\n",
      "Checking accuracy on test set\n",
      "Got 657 / 1000 correct (65.70)\n",
      "77 epoch,  1000 iteration, loss:0.171\n",
      " num 76 epoch \n",
      "####### Training Loss #######\n",
      "[0.16208521]\n",
      "Checking accuracy on test set\n",
      "Got 685 / 1000 correct (68.50)\n",
      "78 epoch,   500 iteration, loss:0.158\n",
      "Checking accuracy on test set\n",
      "Got 683 / 1000 correct (68.30)\n",
      "78 epoch,  1000 iteration, loss:0.139\n",
      " num 77 epoch \n",
      "####### Training Loss #######\n",
      "[0.15946816]\n",
      "Checking accuracy on test set\n",
      "Got 663 / 1000 correct (66.30)\n",
      "79 epoch,   500 iteration, loss:0.143\n",
      "Checking accuracy on test set\n",
      "Got 653 / 1000 correct (65.30)\n",
      "79 epoch,  1000 iteration, loss:0.145\n",
      " num 78 epoch \n",
      "####### Training Loss #######\n",
      "[0.14432351]\n",
      "Checking accuracy on test set\n",
      "Got 649 / 1000 correct (64.90)\n",
      "80 epoch,   500 iteration, loss:0.121\n",
      "Checking accuracy on test set\n",
      "Got 683 / 1000 correct (68.30)\n",
      "80 epoch,  1000 iteration, loss:0.144\n",
      " num 79 epoch \n",
      "####### Training Loss #######\n",
      "[0.12649335]\n",
      "Checking accuracy on test set\n",
      "Got 668 / 1000 correct (66.80)\n",
      "81 epoch,   500 iteration, loss:0.124\n",
      "Checking accuracy on test set\n",
      "Got 665 / 1000 correct (66.50)\n",
      "81 epoch,  1000 iteration, loss:0.171\n",
      " num 80 epoch \n",
      "####### Training Loss #######\n",
      "[0.15376627]\n",
      "Checking accuracy on test set\n",
      "Got 659 / 1000 correct (65.90)\n",
      "82 epoch,   500 iteration, loss:0.115\n",
      "Checking accuracy on test set\n",
      "Got 651 / 1000 correct (65.10)\n",
      "82 epoch,  1000 iteration, loss:0.143\n",
      " num 81 epoch \n",
      "####### Training Loss #######\n",
      "[0.13200236]\n",
      "Checking accuracy on test set\n",
      "Got 653 / 1000 correct (65.30)\n",
      "83 epoch,   500 iteration, loss:0.151\n",
      "Checking accuracy on test set\n",
      "Got 651 / 1000 correct (65.10)\n",
      "83 epoch,  1000 iteration, loss:0.119\n",
      " num 82 epoch \n",
      "####### Training Loss #######\n",
      "[0.14767063]\n",
      "Checking accuracy on test set\n",
      "Got 669 / 1000 correct (66.90)\n",
      "84 epoch,   500 iteration, loss:0.153\n",
      "Checking accuracy on test set\n",
      "Got 682 / 1000 correct (68.20)\n",
      "84 epoch,  1000 iteration, loss:0.159\n",
      " num 83 epoch \n",
      "####### Training Loss #######\n",
      "[0.14677904]\n",
      "Checking accuracy on test set\n",
      "Got 659 / 1000 correct (65.90)\n",
      "85 epoch,   500 iteration, loss:0.139\n",
      "Checking accuracy on test set\n",
      "Got 686 / 1000 correct (68.60)\n",
      "85 epoch,  1000 iteration, loss:0.125\n",
      " num 84 epoch \n",
      "####### Training Loss #######\n",
      "[0.13469678]\n",
      "Checking accuracy on test set\n",
      "Got 658 / 1000 correct (65.80)\n",
      "86 epoch,   500 iteration, loss:0.134\n",
      "Checking accuracy on test set\n",
      "Got 662 / 1000 correct (66.20)\n",
      "86 epoch,  1000 iteration, loss:0.160\n",
      " num 85 epoch \n",
      "####### Training Loss #######\n",
      "[0.13790695]\n",
      "Checking accuracy on test set\n",
      "Got 660 / 1000 correct (66.00)\n",
      "87 epoch,   500 iteration, loss:0.136\n",
      "Checking accuracy on test set\n",
      "Got 667 / 1000 correct (66.70)\n",
      "87 epoch,  1000 iteration, loss:0.129\n",
      " num 86 epoch \n",
      "####### Training Loss #######\n",
      "[0.13368547]\n",
      "Checking accuracy on test set\n",
      "Got 674 / 1000 correct (67.40)\n",
      "88 epoch,   500 iteration, loss:0.137\n",
      "Checking accuracy on test set\n",
      "Got 671 / 1000 correct (67.10)\n",
      "88 epoch,  1000 iteration, loss:0.172\n",
      " num 87 epoch \n",
      "####### Training Loss #######\n",
      "[0.15254366]\n",
      "Checking accuracy on test set\n",
      "Got 651 / 1000 correct (65.10)\n",
      "89 epoch,   500 iteration, loss:0.132\n",
      "Checking accuracy on test set\n",
      "Got 662 / 1000 correct (66.20)\n",
      "89 epoch,  1000 iteration, loss:0.121\n",
      " num 88 epoch \n",
      "####### Training Loss #######\n",
      "[0.13477674]\n",
      "Checking accuracy on test set\n",
      "Got 681 / 1000 correct (68.10)\n",
      "90 epoch,   500 iteration, loss:0.115\n",
      "Checking accuracy on test set\n",
      "Got 659 / 1000 correct (65.90)\n",
      "90 epoch,  1000 iteration, loss:0.123\n",
      " num 89 epoch \n",
      "####### Training Loss #######\n",
      "[0.11779026]\n",
      "Checking accuracy on test set\n",
      "Got 639 / 1000 correct (63.90)\n",
      "91 epoch,   500 iteration, loss:0.116\n",
      "Checking accuracy on test set\n",
      "Got 679 / 1000 correct (67.90)\n",
      "91 epoch,  1000 iteration, loss:0.116\n",
      " num 90 epoch \n",
      "####### Training Loss #######\n",
      "[0.11625857]\n",
      "Checking accuracy on test set\n",
      "Got 674 / 1000 correct (67.40)\n",
      "92 epoch,   500 iteration, loss:0.141\n",
      "Checking accuracy on test set\n",
      "Got 648 / 1000 correct (64.80)\n",
      "92 epoch,  1000 iteration, loss:0.148\n",
      " num 91 epoch \n",
      "####### Training Loss #######\n",
      "[0.15200079]\n",
      "Checking accuracy on test set\n",
      "Got 664 / 1000 correct (66.40)\n",
      "93 epoch,   500 iteration, loss:0.135\n",
      "Checking accuracy on test set\n",
      "Got 694 / 1000 correct (69.40)\n",
      "93 epoch,  1000 iteration, loss:0.130\n",
      " num 92 epoch \n",
      "####### Training Loss #######\n",
      "[0.13314945]\n",
      "Checking accuracy on test set\n",
      "Got 673 / 1000 correct (67.30)\n",
      "94 epoch,   500 iteration, loss:0.154\n",
      "Checking accuracy on test set\n",
      "Got 674 / 1000 correct (67.40)\n",
      "94 epoch,  1000 iteration, loss:0.119\n",
      " num 93 epoch \n",
      "####### Training Loss #######\n",
      "[0.13383367]\n",
      "Checking accuracy on test set\n",
      "Got 674 / 1000 correct (67.40)\n",
      "95 epoch,   500 iteration, loss:0.112\n",
      "Checking accuracy on test set\n",
      "Got 662 / 1000 correct (66.20)\n",
      "95 epoch,  1000 iteration, loss:0.096\n",
      " num 94 epoch \n",
      "####### Training Loss #######\n",
      "[0.10432721]\n",
      "Checking accuracy on test set\n",
      "Got 671 / 1000 correct (67.10)\n",
      "96 epoch,   500 iteration, loss:0.111\n",
      "Checking accuracy on test set\n",
      "Got 658 / 1000 correct (65.80)\n",
      "96 epoch,  1000 iteration, loss:0.104\n",
      " num 95 epoch \n",
      "####### Training Loss #######\n",
      "[0.10843536]\n",
      "Checking accuracy on test set\n",
      "Got 678 / 1000 correct (67.80)\n",
      "97 epoch,   500 iteration, loss:0.157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking accuracy on test set\n",
      "Got 685 / 1000 correct (68.50)\n",
      "97 epoch,  1000 iteration, loss:0.141\n",
      " num 96 epoch \n",
      "####### Training Loss #######\n",
      "[0.14251605]\n",
      "Checking accuracy on test set\n",
      "Got 638 / 1000 correct (63.80)\n",
      "98 epoch,   500 iteration, loss:0.144\n",
      "Checking accuracy on test set\n",
      "Got 655 / 1000 correct (65.50)\n",
      "98 epoch,  1000 iteration, loss:0.101\n",
      " num 97 epoch \n",
      "####### Training Loss #######\n",
      "[0.12958036]\n",
      "Checking accuracy on test set\n",
      "Got 660 / 1000 correct (66.00)\n",
      "99 epoch,   500 iteration, loss:0.138\n",
      "Checking accuracy on test set\n",
      "Got 673 / 1000 correct (67.30)\n",
      "99 epoch,  1000 iteration, loss:0.114\n",
      " num 98 epoch \n",
      "####### Training Loss #######\n",
      "[0.124078]\n",
      "Checking accuracy on test set\n",
      "Got 677 / 1000 correct (67.70)\n",
      "100 epoch,   500 iteration, loss:0.130\n",
      "Checking accuracy on test set\n",
      "Got 640 / 1000 correct (64.00)\n",
      "100 epoch,  1000 iteration, loss:0.112\n",
      " num 99 epoch \n",
      "####### Training Loss #######\n",
      "[0.12283226]\n",
      "Checking accuracy on test set\n",
      "Got 633 / 1000 correct (63.30)\n",
      "101 epoch,   500 iteration, loss:0.100\n",
      "Checking accuracy on test set\n",
      "Got 626 / 1000 correct (62.60)\n",
      "101 epoch,  1000 iteration, loss:0.123\n",
      " num 100 epoch \n",
      "####### Training Loss #######\n",
      "[0.12271956]\n",
      "Checking accuracy on test set\n",
      "Got 626 / 1000 correct (62.60)\n",
      "102 epoch,   500 iteration, loss:0.109\n",
      "Checking accuracy on test set\n",
      "Got 687 / 1000 correct (68.70)\n",
      "102 epoch,  1000 iteration, loss:0.149\n",
      " num 101 epoch \n",
      "####### Training Loss #######\n",
      "[0.12808719]\n",
      "Checking accuracy on test set\n",
      "Got 679 / 1000 correct (67.90)\n",
      "103 epoch,   500 iteration, loss:0.116\n",
      "Checking accuracy on test set\n",
      "Got 677 / 1000 correct (67.70)\n",
      "103 epoch,  1000 iteration, loss:0.142\n",
      " num 102 epoch \n",
      "####### Training Loss #######\n",
      "[0.11612249]\n",
      "Checking accuracy on test set\n",
      "Got 639 / 1000 correct (63.90)\n",
      "104 epoch,   500 iteration, loss:0.106\n",
      "Checking accuracy on test set\n",
      "Got 675 / 1000 correct (67.50)\n",
      "104 epoch,  1000 iteration, loss:0.123\n",
      " num 103 epoch \n",
      "####### Training Loss #######\n",
      "[0.11416442]\n",
      "Checking accuracy on test set\n",
      "Got 674 / 1000 correct (67.40)\n",
      "105 epoch,   500 iteration, loss:0.117\n",
      "Checking accuracy on test set\n",
      "Got 668 / 1000 correct (66.80)\n",
      "105 epoch,  1000 iteration, loss:0.107\n",
      " num 104 epoch \n",
      "####### Training Loss #######\n",
      "[0.11609246]\n",
      "Checking accuracy on test set\n",
      "Got 658 / 1000 correct (65.80)\n",
      "106 epoch,   500 iteration, loss:0.092\n",
      "Checking accuracy on test set\n",
      "Got 680 / 1000 correct (68.00)\n",
      "106 epoch,  1000 iteration, loss:0.114\n",
      " num 105 epoch \n",
      "####### Training Loss #######\n",
      "[0.10959165]\n",
      "Checking accuracy on test set\n",
      "Got 654 / 1000 correct (65.40)\n",
      "107 epoch,   500 iteration, loss:0.100\n",
      "Checking accuracy on test set\n",
      "Got 643 / 1000 correct (64.30)\n",
      "107 epoch,  1000 iteration, loss:0.113\n",
      " num 106 epoch \n",
      "####### Training Loss #######\n",
      "[0.11144504]\n",
      "Checking accuracy on test set\n",
      "Got 646 / 1000 correct (64.60)\n",
      "108 epoch,   500 iteration, loss:0.107\n",
      "Checking accuracy on test set\n",
      "Got 666 / 1000 correct (66.60)\n",
      "108 epoch,  1000 iteration, loss:0.099\n",
      " num 107 epoch \n",
      "####### Training Loss #######\n",
      "[0.10164866]\n",
      "Checking accuracy on test set\n",
      "Got 673 / 1000 correct (67.30)\n",
      "109 epoch,   500 iteration, loss:0.120\n",
      "Checking accuracy on test set\n",
      "Got 661 / 1000 correct (66.10)\n",
      "109 epoch,  1000 iteration, loss:0.119\n",
      " num 108 epoch \n",
      "####### Training Loss #######\n",
      "[0.1188965]\n",
      "Checking accuracy on test set\n",
      "Got 695 / 1000 correct (69.50)\n",
      "110 epoch,   500 iteration, loss:0.111\n",
      "Checking accuracy on test set\n",
      "Got 679 / 1000 correct (67.90)\n",
      "110 epoch,  1000 iteration, loss:0.113\n",
      " num 109 epoch \n",
      "####### Training Loss #######\n",
      "[0.11890771]\n",
      "Checking accuracy on test set\n",
      "Got 676 / 1000 correct (67.60)\n",
      "111 epoch,   500 iteration, loss:0.095\n",
      "Checking accuracy on test set\n",
      "Got 671 / 1000 correct (67.10)\n",
      "111 epoch,  1000 iteration, loss:0.082\n",
      " num 110 epoch \n",
      "####### Training Loss #######\n",
      "[0.08268499]\n",
      "Checking accuracy on test set\n",
      "Got 668 / 1000 correct (66.80)\n",
      "112 epoch,   500 iteration, loss:0.121\n",
      "Checking accuracy on test set\n",
      "Got 657 / 1000 correct (65.70)\n",
      "112 epoch,  1000 iteration, loss:0.092\n",
      " num 111 epoch \n",
      "####### Training Loss #######\n",
      "[0.10643244]\n",
      "Checking accuracy on test set\n",
      "Got 660 / 1000 correct (66.00)\n",
      "113 epoch,   500 iteration, loss:0.112\n",
      "Checking accuracy on test set\n",
      "Got 680 / 1000 correct (68.00)\n",
      "113 epoch,  1000 iteration, loss:0.132\n",
      " num 112 epoch \n",
      "####### Training Loss #######\n",
      "[0.11806908]\n",
      "Checking accuracy on test set\n",
      "Got 645 / 1000 correct (64.50)\n",
      "114 epoch,   500 iteration, loss:0.115\n",
      "Checking accuracy on test set\n",
      "Got 667 / 1000 correct (66.70)\n",
      "114 epoch,  1000 iteration, loss:0.124\n",
      " num 113 epoch \n",
      "####### Training Loss #######\n",
      "[0.1202855]\n",
      "Checking accuracy on test set\n",
      "Got 667 / 1000 correct (66.70)\n",
      "115 epoch,   500 iteration, loss:0.095\n",
      "Checking accuracy on test set\n",
      "Got 630 / 1000 correct (63.00)\n",
      "115 epoch,  1000 iteration, loss:0.092\n",
      " num 114 epoch \n",
      "####### Training Loss #######\n",
      "[0.09374698]\n",
      "Checking accuracy on test set\n",
      "Got 682 / 1000 correct (68.20)\n",
      "116 epoch,   500 iteration, loss:0.078\n",
      "Checking accuracy on test set\n",
      "Got 675 / 1000 correct (67.50)\n",
      "116 epoch,  1000 iteration, loss:0.075\n",
      " num 115 epoch \n",
      "####### Training Loss #######\n",
      "[0.08903336]\n",
      "Checking accuracy on test set\n",
      "Got 664 / 1000 correct (66.40)\n",
      "117 epoch,   500 iteration, loss:0.131\n",
      "Checking accuracy on test set\n",
      "Got 686 / 1000 correct (68.60)\n",
      "117 epoch,  1000 iteration, loss:0.116\n",
      " num 116 epoch \n",
      "####### Training Loss #######\n",
      "[0.12350369]\n",
      "Checking accuracy on test set\n",
      "Got 683 / 1000 correct (68.30)\n",
      "118 epoch,   500 iteration, loss:0.082\n",
      "Checking accuracy on test set\n",
      "Got 663 / 1000 correct (66.30)\n",
      "118 epoch,  1000 iteration, loss:0.132\n",
      " num 117 epoch \n",
      "####### Training Loss #######\n",
      "[0.10860503]\n",
      "Checking accuracy on test set\n",
      "Got 681 / 1000 correct (68.10)\n",
      "119 epoch,   500 iteration, loss:0.110\n",
      "Checking accuracy on test set\n",
      "Got 679 / 1000 correct (67.90)\n",
      "119 epoch,  1000 iteration, loss:0.099\n",
      " num 118 epoch \n",
      "####### Training Loss #######\n",
      "[0.0999321]\n",
      "Checking accuracy on test set\n",
      "Got 684 / 1000 correct (68.40)\n",
      "120 epoch,   500 iteration, loss:0.121\n",
      "Checking accuracy on test set\n",
      "Got 676 / 1000 correct (67.60)\n",
      "120 epoch,  1000 iteration, loss:0.097\n",
      " num 119 epoch \n",
      "####### Training Loss #######\n",
      "[0.11292703]\n",
      "Checking accuracy on test set\n",
      "Got 678 / 1000 correct (67.80)\n",
      "121 epoch,   500 iteration, loss:0.084\n",
      "Checking accuracy on test set\n",
      "Got 672 / 1000 correct (67.20)\n",
      "121 epoch,  1000 iteration, loss:0.090\n",
      " num 120 epoch \n",
      "####### Training Loss #######\n",
      "[0.0919178]\n",
      "Checking accuracy on test set\n",
      "Got 697 / 1000 correct (69.70)\n",
      "122 epoch,   500 iteration, loss:0.098\n",
      "Checking accuracy on test set\n",
      "Got 692 / 1000 correct (69.20)\n",
      "122 epoch,  1000 iteration, loss:0.086\n",
      " num 121 epoch \n",
      "####### Training Loss #######\n",
      "[0.09286044]\n",
      "Checking accuracy on test set\n",
      "Got 629 / 1000 correct (62.90)\n",
      "123 epoch,   500 iteration, loss:0.099\n",
      "Checking accuracy on test set\n",
      "Got 656 / 1000 correct (65.60)\n",
      "123 epoch,  1000 iteration, loss:0.122\n",
      " num 122 epoch \n",
      "####### Training Loss #######\n",
      "[0.10446294]\n",
      "Checking accuracy on test set\n",
      "Got 689 / 1000 correct (68.90)\n",
      "124 epoch,   500 iteration, loss:0.107\n",
      "Checking accuracy on test set\n",
      "Got 681 / 1000 correct (68.10)\n",
      "124 epoch,  1000 iteration, loss:0.091\n",
      " num 123 epoch \n",
      "####### Training Loss #######\n",
      "[0.09739077]\n",
      "Checking accuracy on test set\n",
      "Got 638 / 1000 correct (63.80)\n",
      "125 epoch,   500 iteration, loss:0.103\n",
      "Checking accuracy on test set\n",
      "Got 663 / 1000 correct (66.30)\n",
      "125 epoch,  1000 iteration, loss:0.106\n",
      " num 124 epoch \n",
      "####### Training Loss #######\n",
      "[0.09826512]\n",
      "Checking accuracy on test set\n",
      "Got 661 / 1000 correct (66.10)\n",
      "126 epoch,   500 iteration, loss:0.103\n",
      "Checking accuracy on test set\n",
      "Got 673 / 1000 correct (67.30)\n",
      "126 epoch,  1000 iteration, loss:0.114\n",
      " num 125 epoch \n",
      "####### Training Loss #######\n",
      "[0.10849397]\n",
      "Checking accuracy on test set\n",
      "Got 636 / 1000 correct (63.60)\n",
      "127 epoch,   500 iteration, loss:0.092\n",
      "Checking accuracy on test set\n",
      "Got 665 / 1000 correct (66.50)\n",
      "127 epoch,  1000 iteration, loss:0.126\n",
      " num 126 epoch \n",
      "####### Training Loss #######\n",
      "[0.10365875]\n",
      "Checking accuracy on test set\n",
      "Got 678 / 1000 correct (67.80)\n",
      "128 epoch,   500 iteration, loss:0.104\n",
      "Checking accuracy on test set\n",
      "Got 676 / 1000 correct (67.60)\n",
      "128 epoch,  1000 iteration, loss:0.084\n",
      " num 127 epoch \n",
      "####### Training Loss #######\n",
      "[0.0924515]\n",
      "Checking accuracy on test set\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 661 / 1000 correct (66.10)\n",
      "129 epoch,   500 iteration, loss:0.079\n",
      "Checking accuracy on test set\n",
      "Got 672 / 1000 correct (67.20)\n",
      "129 epoch,  1000 iteration, loss:0.070\n",
      " num 128 epoch \n",
      "####### Training Loss #######\n",
      "[0.07859035]\n",
      "Checking accuracy on test set\n",
      "Got 656 / 1000 correct (65.60)\n",
      "130 epoch,   500 iteration, loss:0.092\n",
      "Checking accuracy on test set\n",
      "Got 681 / 1000 correct (68.10)\n",
      "130 epoch,  1000 iteration, loss:0.086\n",
      " num 129 epoch \n",
      "####### Training Loss #######\n",
      "[0.08003168]\n",
      "Checking accuracy on test set\n",
      "Got 685 / 1000 correct (68.50)\n",
      "131 epoch,   500 iteration, loss:0.119\n",
      "Checking accuracy on test set\n",
      "Got 672 / 1000 correct (67.20)\n",
      "131 epoch,  1000 iteration, loss:0.080\n",
      " num 130 epoch \n",
      "####### Training Loss #######\n",
      "[0.10261001]\n",
      "Checking accuracy on test set\n",
      "Got 685 / 1000 correct (68.50)\n",
      "132 epoch,   500 iteration, loss:0.081\n",
      "Checking accuracy on test set\n",
      "Got 683 / 1000 correct (68.30)\n",
      "132 epoch,  1000 iteration, loss:0.099\n",
      " num 131 epoch \n",
      "####### Training Loss #######\n",
      "[0.09334353]\n",
      "Checking accuracy on test set\n",
      "Got 670 / 1000 correct (67.00)\n",
      "133 epoch,   500 iteration, loss:0.100\n",
      "Checking accuracy on test set\n",
      "Got 691 / 1000 correct (69.10)\n",
      "133 epoch,  1000 iteration, loss:0.111\n",
      " num 132 epoch \n",
      "####### Training Loss #######\n",
      "[0.09781669]\n",
      "Checking accuracy on test set\n",
      "Got 667 / 1000 correct (66.70)\n",
      "134 epoch,   500 iteration, loss:0.101\n",
      "Checking accuracy on test set\n",
      "Got 679 / 1000 correct (67.90)\n",
      "134 epoch,  1000 iteration, loss:0.112\n",
      " num 133 epoch \n",
      "####### Training Loss #######\n",
      "[0.10350152]\n",
      "Checking accuracy on test set\n",
      "Got 682 / 1000 correct (68.20)\n",
      "135 epoch,   500 iteration, loss:0.084\n",
      "Checking accuracy on test set\n",
      "Got 649 / 1000 correct (64.90)\n",
      "135 epoch,  1000 iteration, loss:0.095\n",
      " num 134 epoch \n",
      "####### Training Loss #######\n",
      "[0.09098809]\n",
      "Checking accuracy on test set\n",
      "Got 687 / 1000 correct (68.70)\n",
      "136 epoch,   500 iteration, loss:0.118\n",
      "Checking accuracy on test set\n",
      "Got 682 / 1000 correct (68.20)\n",
      "136 epoch,  1000 iteration, loss:0.078\n",
      " num 135 epoch \n",
      "####### Training Loss #######\n",
      "[0.09890949]\n",
      "Checking accuracy on test set\n",
      "Got 675 / 1000 correct (67.50)\n",
      "137 epoch,   500 iteration, loss:0.119\n",
      "Checking accuracy on test set\n",
      "Got 698 / 1000 correct (69.80)\n",
      "137 epoch,  1000 iteration, loss:0.097\n",
      " num 136 epoch \n",
      "####### Training Loss #######\n",
      "[0.10329531]\n",
      "Checking accuracy on test set\n",
      "Got 636 / 1000 correct (63.60)\n",
      "138 epoch,   500 iteration, loss:0.084\n",
      "Checking accuracy on test set\n",
      "Got 679 / 1000 correct (67.90)\n",
      "138 epoch,  1000 iteration, loss:0.089\n",
      " num 137 epoch \n",
      "####### Training Loss #######\n",
      "[0.08710269]\n",
      "Checking accuracy on test set\n",
      "Got 663 / 1000 correct (66.30)\n",
      "139 epoch,   500 iteration, loss:0.068\n",
      "Checking accuracy on test set\n",
      "Got 697 / 1000 correct (69.70)\n",
      "139 epoch,  1000 iteration, loss:0.090\n",
      " num 138 epoch \n",
      "####### Training Loss #######\n",
      "[0.0766782]\n",
      "Checking accuracy on test set\n",
      "Got 688 / 1000 correct (68.80)\n",
      "140 epoch,   500 iteration, loss:0.078\n",
      "Checking accuracy on test set\n",
      "Got 677 / 1000 correct (67.70)\n",
      "140 epoch,  1000 iteration, loss:0.091\n",
      " num 139 epoch \n",
      "####### Training Loss #######\n",
      "[0.08515791]\n",
      "Checking accuracy on test set\n",
      "Got 688 / 1000 correct (68.80)\n",
      "141 epoch,   500 iteration, loss:0.065\n",
      "Checking accuracy on test set\n",
      "Got 678 / 1000 correct (67.80)\n",
      "141 epoch,  1000 iteration, loss:0.114\n",
      " num 140 epoch \n",
      "####### Training Loss #######\n",
      "[0.0834916]\n",
      "Checking accuracy on test set\n",
      "Got 681 / 1000 correct (68.10)\n",
      "142 epoch,   500 iteration, loss:0.044\n",
      "Checking accuracy on test set\n",
      "Got 670 / 1000 correct (67.00)\n",
      "142 epoch,  1000 iteration, loss:0.082\n",
      " num 141 epoch \n",
      "####### Training Loss #######\n",
      "[0.06583345]\n",
      "Checking accuracy on test set\n",
      "Got 676 / 1000 correct (67.60)\n",
      "143 epoch,   500 iteration, loss:0.096\n",
      "Checking accuracy on test set\n",
      "Got 681 / 1000 correct (68.10)\n",
      "143 epoch,  1000 iteration, loss:0.081\n",
      " num 142 epoch \n",
      "####### Training Loss #######\n",
      "[0.08681768]\n",
      "Checking accuracy on test set\n",
      "Got 670 / 1000 correct (67.00)\n",
      "144 epoch,   500 iteration, loss:0.097\n",
      "Checking accuracy on test set\n",
      "Got 668 / 1000 correct (66.80)\n",
      "144 epoch,  1000 iteration, loss:0.082\n",
      " num 143 epoch \n",
      "####### Training Loss #######\n",
      "[0.09352295]\n",
      "Checking accuracy on test set\n",
      "Got 654 / 1000 correct (65.40)\n",
      "145 epoch,   500 iteration, loss:0.102\n",
      "Checking accuracy on test set\n",
      "Got 674 / 1000 correct (67.40)\n",
      "145 epoch,  1000 iteration, loss:0.061\n",
      " num 144 epoch \n",
      "####### Training Loss #######\n",
      "[0.07735376]\n",
      "Checking accuracy on test set\n",
      "Got 692 / 1000 correct (69.20)\n",
      "146 epoch,   500 iteration, loss:0.077\n",
      "Checking accuracy on test set\n",
      "Got 666 / 1000 correct (66.60)\n",
      "146 epoch,  1000 iteration, loss:0.090\n",
      " num 145 epoch \n",
      "####### Training Loss #######\n",
      "[0.0860059]\n",
      "Checking accuracy on test set\n",
      "Got 660 / 1000 correct (66.00)\n",
      "147 epoch,   500 iteration, loss:0.080\n",
      "Checking accuracy on test set\n",
      "Got 673 / 1000 correct (67.30)\n",
      "147 epoch,  1000 iteration, loss:0.110\n",
      " num 146 epoch \n",
      "####### Training Loss #######\n",
      "[0.10523234]\n",
      "Checking accuracy on test set\n",
      "Got 668 / 1000 correct (66.80)\n",
      "148 epoch,   500 iteration, loss:0.080\n",
      "Checking accuracy on test set\n",
      "Got 660 / 1000 correct (66.00)\n",
      "148 epoch,  1000 iteration, loss:0.080\n",
      " num 147 epoch \n",
      "####### Training Loss #######\n",
      "[0.09475649]\n",
      "Checking accuracy on test set\n",
      "Got 681 / 1000 correct (68.10)\n",
      "149 epoch,   500 iteration, loss:0.094\n",
      "Checking accuracy on test set\n",
      "Got 672 / 1000 correct (67.20)\n",
      "149 epoch,  1000 iteration, loss:0.082\n",
      " num 148 epoch \n",
      "####### Training Loss #######\n",
      "[0.08473938]\n",
      "Checking accuracy on test set\n",
      "Got 680 / 1000 correct (68.00)\n",
      "150 epoch,   500 iteration, loss:0.097\n",
      "Checking accuracy on test set\n",
      "Got 647 / 1000 correct (64.70)\n",
      "150 epoch,  1000 iteration, loss:0.117\n",
      " num 149 epoch \n",
      "####### Training Loss #######\n",
      "[0.10405141]\n",
      "Checking accuracy on test set\n",
      "Got 657 / 1000 correct (65.70)\n",
      "151 epoch,   500 iteration, loss:0.088\n",
      "Checking accuracy on test set\n",
      "Got 686 / 1000 correct (68.60)\n",
      "151 epoch,  1000 iteration, loss:0.075\n",
      " num 150 epoch \n",
      "####### Training Loss #######\n",
      "[0.07504005]\n",
      "Checking accuracy on test set\n",
      "Got 695 / 1000 correct (69.50)\n",
      "152 epoch,   500 iteration, loss:0.077\n",
      "Checking accuracy on test set\n",
      "Got 670 / 1000 correct (67.00)\n",
      "152 epoch,  1000 iteration, loss:0.089\n",
      " num 151 epoch \n",
      "####### Training Loss #######\n",
      "[0.08479668]\n",
      "Checking accuracy on test set\n",
      "Got 657 / 1000 correct (65.70)\n",
      "153 epoch,   500 iteration, loss:0.133\n",
      "Checking accuracy on test set\n",
      "Got 677 / 1000 correct (67.70)\n",
      "153 epoch,  1000 iteration, loss:0.079\n",
      " num 152 epoch \n",
      "####### Training Loss #######\n",
      "[0.09676793]\n",
      "Checking accuracy on test set\n",
      "Got 680 / 1000 correct (68.00)\n",
      "154 epoch,   500 iteration, loss:0.072\n",
      "Checking accuracy on test set\n",
      "Got 678 / 1000 correct (67.80)\n",
      "154 epoch,  1000 iteration, loss:0.073\n",
      " num 153 epoch \n",
      "####### Training Loss #######\n",
      "[0.06631044]\n",
      "Checking accuracy on test set\n",
      "Got 671 / 1000 correct (67.10)\n",
      "155 epoch,   500 iteration, loss:0.077\n",
      "Checking accuracy on test set\n",
      "Got 679 / 1000 correct (67.90)\n",
      "155 epoch,  1000 iteration, loss:0.056\n",
      " num 154 epoch \n",
      "####### Training Loss #######\n",
      "[0.0745201]\n",
      "Checking accuracy on test set\n",
      "Got 670 / 1000 correct (67.00)\n",
      "156 epoch,   500 iteration, loss:0.085\n",
      "Checking accuracy on test set\n",
      "Got 597 / 1000 correct (59.70)\n",
      "156 epoch,  1000 iteration, loss:0.133\n",
      " num 155 epoch \n",
      "####### Training Loss #######\n",
      "[0.10711811]\n",
      "Checking accuracy on test set\n",
      "Got 684 / 1000 correct (68.40)\n",
      "157 epoch,   500 iteration, loss:0.073\n",
      "Checking accuracy on test set\n",
      "Got 674 / 1000 correct (67.40)\n",
      "157 epoch,  1000 iteration, loss:0.084\n",
      " num 156 epoch \n",
      "####### Training Loss #######\n",
      "[0.08545038]\n",
      "Checking accuracy on test set\n",
      "Got 671 / 1000 correct (67.10)\n",
      "158 epoch,   500 iteration, loss:0.086\n",
      "Checking accuracy on test set\n",
      "Got 682 / 1000 correct (68.20)\n",
      "158 epoch,  1000 iteration, loss:0.110\n",
      " num 157 epoch \n",
      "####### Training Loss #######\n",
      "[0.10063524]\n",
      "Checking accuracy on test set\n",
      "Got 672 / 1000 correct (67.20)\n",
      "159 epoch,   500 iteration, loss:0.101\n",
      "Checking accuracy on test set\n",
      "Got 680 / 1000 correct (68.00)\n",
      "159 epoch,  1000 iteration, loss:0.077\n",
      " num 158 epoch \n",
      "####### Training Loss #######\n",
      "[0.08567034]\n",
      "Checking accuracy on test set\n",
      "Got 654 / 1000 correct (65.40)\n",
      "160 epoch,   500 iteration, loss:0.090\n",
      "Checking accuracy on test set\n",
      "Got 668 / 1000 correct (66.80)\n",
      "160 epoch,  1000 iteration, loss:0.091\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " num 159 epoch \n",
      "####### Training Loss #######\n",
      "[0.09003157]\n",
      "Checking accuracy on test set\n",
      "Got 681 / 1000 correct (68.10)\n",
      "161 epoch,   500 iteration, loss:0.089\n",
      "Checking accuracy on test set\n",
      "Got 701 / 1000 correct (70.10)\n",
      "161 epoch,  1000 iteration, loss:0.071\n",
      " num 160 epoch \n",
      "####### Training Loss #######\n",
      "[0.07404013]\n",
      "Checking accuracy on test set\n",
      "Got 664 / 1000 correct (66.40)\n",
      "162 epoch,   500 iteration, loss:0.083\n",
      "Checking accuracy on test set\n",
      "Got 669 / 1000 correct (66.90)\n",
      "162 epoch,  1000 iteration, loss:0.083\n",
      " num 161 epoch \n",
      "####### Training Loss #######\n",
      "[0.08836965]\n",
      "Checking accuracy on test set\n",
      "Got 679 / 1000 correct (67.90)\n",
      "163 epoch,   500 iteration, loss:0.092\n",
      "Checking accuracy on test set\n",
      "Got 684 / 1000 correct (68.40)\n",
      "163 epoch,  1000 iteration, loss:0.070\n",
      " num 162 epoch \n",
      "####### Training Loss #######\n",
      "[0.09065912]\n",
      "Checking accuracy on test set\n",
      "Got 664 / 1000 correct (66.40)\n",
      "164 epoch,   500 iteration, loss:0.076\n",
      "Checking accuracy on test set\n",
      "Got 680 / 1000 correct (68.00)\n",
      "164 epoch,  1000 iteration, loss:0.110\n",
      " num 163 epoch \n",
      "####### Training Loss #######\n",
      "[0.09002254]\n",
      "Checking accuracy on test set\n",
      "Got 683 / 1000 correct (68.30)\n",
      "165 epoch,   500 iteration, loss:0.076\n",
      "Checking accuracy on test set\n",
      "Got 681 / 1000 correct (68.10)\n",
      "165 epoch,  1000 iteration, loss:0.100\n",
      " num 164 epoch \n",
      "####### Training Loss #######\n",
      "[0.08825751]\n",
      "Checking accuracy on test set\n",
      "Got 675 / 1000 correct (67.50)\n",
      "166 epoch,   500 iteration, loss:0.101\n",
      "Checking accuracy on test set\n",
      "Got 670 / 1000 correct (67.00)\n",
      "166 epoch,  1000 iteration, loss:0.066\n",
      " num 165 epoch \n",
      "####### Training Loss #######\n",
      "[0.0926755]\n",
      "Checking accuracy on test set\n",
      "Got 698 / 1000 correct (69.80)\n",
      "167 epoch,   500 iteration, loss:0.095\n",
      "Checking accuracy on test set\n",
      "Got 657 / 1000 correct (65.70)\n",
      "167 epoch,  1000 iteration, loss:0.079\n",
      " num 166 epoch \n",
      "####### Training Loss #######\n",
      "[0.09431778]\n",
      "Checking accuracy on test set\n",
      "Got 647 / 1000 correct (64.70)\n",
      "168 epoch,   500 iteration, loss:0.098\n",
      "Checking accuracy on test set\n",
      "Got 692 / 1000 correct (69.20)\n",
      "168 epoch,  1000 iteration, loss:0.089\n",
      " num 167 epoch \n",
      "####### Training Loss #######\n",
      "[0.08589263]\n",
      "Checking accuracy on test set\n",
      "Got 680 / 1000 correct (68.00)\n",
      "169 epoch,   500 iteration, loss:0.073\n",
      "Checking accuracy on test set\n",
      "Got 645 / 1000 correct (64.50)\n",
      "169 epoch,  1000 iteration, loss:0.084\n",
      " num 168 epoch \n",
      "####### Training Loss #######\n",
      "[0.08692261]\n",
      "Checking accuracy on test set\n",
      "Got 672 / 1000 correct (67.20)\n",
      "170 epoch,   500 iteration, loss:0.075\n",
      "Checking accuracy on test set\n",
      "Got 669 / 1000 correct (66.90)\n",
      "170 epoch,  1000 iteration, loss:0.040\n",
      " num 169 epoch \n",
      "####### Training Loss #######\n",
      "[0.05455386]\n",
      "Checking accuracy on test set\n",
      "Got 673 / 1000 correct (67.30)\n",
      "171 epoch,   500 iteration, loss:0.041\n",
      "Checking accuracy on test set\n",
      "Got 661 / 1000 correct (66.10)\n",
      "171 epoch,  1000 iteration, loss:0.067\n",
      " num 170 epoch \n",
      "####### Training Loss #######\n",
      "[0.06331841]\n",
      "Checking accuracy on test set\n",
      "Got 690 / 1000 correct (69.00)\n",
      "172 epoch,   500 iteration, loss:0.049\n",
      "Checking accuracy on test set\n",
      "Got 679 / 1000 correct (67.90)\n",
      "172 epoch,  1000 iteration, loss:0.065\n",
      " num 171 epoch \n",
      "####### Training Loss #######\n",
      "[0.05945986]\n",
      "Checking accuracy on test set\n",
      "Got 688 / 1000 correct (68.80)\n",
      "173 epoch,   500 iteration, loss:0.061\n",
      "Checking accuracy on test set\n",
      "Got 682 / 1000 correct (68.20)\n",
      "173 epoch,  1000 iteration, loss:0.067\n",
      " num 172 epoch \n",
      "####### Training Loss #######\n",
      "[0.06722481]\n",
      "Checking accuracy on test set\n",
      "Got 657 / 1000 correct (65.70)\n",
      "174 epoch,   500 iteration, loss:0.093\n",
      "Checking accuracy on test set\n",
      "Got 685 / 1000 correct (68.50)\n",
      "174 epoch,  1000 iteration, loss:0.121\n",
      " num 173 epoch \n",
      "####### Training Loss #######\n",
      "[0.09742462]\n",
      "Checking accuracy on test set\n",
      "Got 703 / 1000 correct (70.30)\n",
      "175 epoch,   500 iteration, loss:0.076\n",
      "Checking accuracy on test set\n",
      "Got 676 / 1000 correct (67.60)\n",
      "175 epoch,  1000 iteration, loss:0.083\n",
      " num 174 epoch \n",
      "####### Training Loss #######\n",
      "[0.07059484]\n",
      "Checking accuracy on test set\n",
      "Got 665 / 1000 correct (66.50)\n",
      "176 epoch,   500 iteration, loss:0.082\n",
      "Checking accuracy on test set\n",
      "Got 682 / 1000 correct (68.20)\n",
      "176 epoch,  1000 iteration, loss:0.080\n",
      " num 175 epoch \n",
      "####### Training Loss #######\n",
      "[0.08689861]\n",
      "Checking accuracy on test set\n",
      "Got 668 / 1000 correct (66.80)\n",
      "177 epoch,   500 iteration, loss:0.107\n",
      "Checking accuracy on test set\n",
      "Got 669 / 1000 correct (66.90)\n",
      "177 epoch,  1000 iteration, loss:0.058\n",
      " num 176 epoch \n",
      "####### Training Loss #######\n",
      "[0.08283653]\n",
      "Checking accuracy on test set\n",
      "Got 677 / 1000 correct (67.70)\n",
      "178 epoch,   500 iteration, loss:0.110\n",
      "Checking accuracy on test set\n",
      "Got 681 / 1000 correct (68.10)\n",
      "178 epoch,  1000 iteration, loss:0.068\n",
      " num 177 epoch \n",
      "####### Training Loss #######\n",
      "[0.08533648]\n",
      "Checking accuracy on test set\n",
      "Got 669 / 1000 correct (66.90)\n",
      "179 epoch,   500 iteration, loss:0.063\n",
      "Checking accuracy on test set\n",
      "Got 673 / 1000 correct (67.30)\n",
      "179 epoch,  1000 iteration, loss:0.076\n",
      " num 178 epoch \n",
      "####### Training Loss #######\n",
      "[0.07195035]\n",
      "Checking accuracy on test set\n",
      "Got 668 / 1000 correct (66.80)\n",
      "180 epoch,   500 iteration, loss:0.090\n",
      "Checking accuracy on test set\n",
      "Got 699 / 1000 correct (69.90)\n",
      "180 epoch,  1000 iteration, loss:0.052\n",
      " num 179 epoch \n",
      "####### Training Loss #######\n",
      "[0.07042536]\n",
      "Checking accuracy on test set\n",
      "Got 638 / 1000 correct (63.80)\n",
      "181 epoch,   500 iteration, loss:0.065\n",
      "Checking accuracy on test set\n",
      "Got 672 / 1000 correct (67.20)\n",
      "181 epoch,  1000 iteration, loss:0.076\n",
      " num 180 epoch \n",
      "####### Training Loss #######\n",
      "[0.07165975]\n",
      "Checking accuracy on test set\n",
      "Got 692 / 1000 correct (69.20)\n",
      "182 epoch,   500 iteration, loss:0.077\n",
      "Checking accuracy on test set\n",
      "Got 687 / 1000 correct (68.70)\n",
      "182 epoch,  1000 iteration, loss:0.077\n",
      " num 181 epoch \n",
      "####### Training Loss #######\n",
      "[0.07169955]\n",
      "Checking accuracy on test set\n",
      "Got 699 / 1000 correct (69.90)\n",
      "183 epoch,   500 iteration, loss:0.075\n",
      "Checking accuracy on test set\n",
      "Got 670 / 1000 correct (67.00)\n",
      "183 epoch,  1000 iteration, loss:0.077\n",
      " num 182 epoch \n",
      "####### Training Loss #######\n",
      "[0.08447305]\n",
      "Checking accuracy on test set\n",
      "Got 681 / 1000 correct (68.10)\n",
      "184 epoch,   500 iteration, loss:0.074\n",
      "Checking accuracy on test set\n",
      "Got 686 / 1000 correct (68.60)\n",
      "184 epoch,  1000 iteration, loss:0.056\n",
      " num 183 epoch \n",
      "####### Training Loss #######\n",
      "[0.06882847]\n",
      "Checking accuracy on test set\n",
      "Got 675 / 1000 correct (67.50)\n",
      "185 epoch,   500 iteration, loss:0.081\n",
      "Checking accuracy on test set\n",
      "Got 674 / 1000 correct (67.40)\n",
      "185 epoch,  1000 iteration, loss:0.101\n",
      " num 184 epoch \n",
      "####### Training Loss #######\n",
      "[0.09020039]\n",
      "Checking accuracy on test set\n",
      "Got 683 / 1000 correct (68.30)\n",
      "186 epoch,   500 iteration, loss:0.078\n",
      "Checking accuracy on test set\n",
      "Got 652 / 1000 correct (65.20)\n",
      "186 epoch,  1000 iteration, loss:0.071\n",
      " num 185 epoch \n",
      "####### Training Loss #######\n",
      "[0.0687525]\n",
      "Checking accuracy on test set\n",
      "Got 675 / 1000 correct (67.50)\n",
      "187 epoch,   500 iteration, loss:0.082\n",
      "Checking accuracy on test set\n",
      "Got 664 / 1000 correct (66.40)\n",
      "187 epoch,  1000 iteration, loss:0.086\n",
      " num 186 epoch \n",
      "####### Training Loss #######\n",
      "[0.08420721]\n",
      "Checking accuracy on test set\n",
      "Got 665 / 1000 correct (66.50)\n",
      "188 epoch,   500 iteration, loss:0.123\n",
      "Checking accuracy on test set\n",
      "Got 679 / 1000 correct (67.90)\n",
      "188 epoch,  1000 iteration, loss:0.067\n",
      " num 187 epoch \n",
      "####### Training Loss #######\n",
      "[0.09038571]\n",
      "Checking accuracy on test set\n",
      "Got 699 / 1000 correct (69.90)\n",
      "189 epoch,   500 iteration, loss:0.048\n",
      "Checking accuracy on test set\n",
      "Got 682 / 1000 correct (68.20)\n",
      "189 epoch,  1000 iteration, loss:0.090\n",
      " num 188 epoch \n",
      "####### Training Loss #######\n",
      "[0.06648247]\n",
      "Checking accuracy on test set\n",
      "Got 656 / 1000 correct (65.60)\n",
      "190 epoch,   500 iteration, loss:0.063\n",
      "Checking accuracy on test set\n",
      "Got 687 / 1000 correct (68.70)\n",
      "190 epoch,  1000 iteration, loss:0.047\n",
      " num 189 epoch \n",
      "####### Training Loss #######\n",
      "[0.05222069]\n",
      "Checking accuracy on test set\n",
      "Got 691 / 1000 correct (69.10)\n",
      "191 epoch,   500 iteration, loss:0.077\n",
      "Checking accuracy on test set\n",
      "Got 670 / 1000 correct (67.00)\n",
      "191 epoch,  1000 iteration, loss:0.074\n",
      " num 190 epoch \n",
      "####### Training Loss #######\n",
      "[0.07581097]\n",
      "Checking accuracy on test set\n",
      "Got 672 / 1000 correct (67.20)\n",
      "192 epoch,   500 iteration, loss:0.062\n",
      "Checking accuracy on test set\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 663 / 1000 correct (66.30)\n",
      "192 epoch,  1000 iteration, loss:0.095\n",
      " num 191 epoch \n",
      "####### Training Loss #######\n",
      "[0.08219454]\n",
      "Checking accuracy on test set\n",
      "Got 658 / 1000 correct (65.80)\n",
      "193 epoch,   500 iteration, loss:0.132\n",
      "Checking accuracy on test set\n",
      "Got 689 / 1000 correct (68.90)\n",
      "193 epoch,  1000 iteration, loss:0.064\n",
      " num 192 epoch \n",
      "####### Training Loss #######\n",
      "[0.09014949]\n",
      "Checking accuracy on test set\n",
      "Got 691 / 1000 correct (69.10)\n",
      "194 epoch,   500 iteration, loss:0.079\n",
      "Checking accuracy on test set\n",
      "Got 680 / 1000 correct (68.00)\n",
      "194 epoch,  1000 iteration, loss:0.063\n",
      " num 193 epoch \n",
      "####### Training Loss #######\n",
      "[0.06458527]\n",
      "Checking accuracy on test set\n",
      "Got 691 / 1000 correct (69.10)\n",
      "195 epoch,   500 iteration, loss:0.111\n",
      "Checking accuracy on test set\n",
      "Got 663 / 1000 correct (66.30)\n",
      "195 epoch,  1000 iteration, loss:0.061\n",
      " num 194 epoch \n",
      "####### Training Loss #######\n",
      "[0.07889024]\n",
      "Checking accuracy on test set\n",
      "Got 670 / 1000 correct (67.00)\n",
      "196 epoch,   500 iteration, loss:0.065\n",
      "Checking accuracy on test set\n",
      "Got 664 / 1000 correct (66.40)\n",
      "196 epoch,  1000 iteration, loss:0.046\n",
      " num 195 epoch \n",
      "####### Training Loss #######\n",
      "[0.05624033]\n",
      "Checking accuracy on test set\n",
      "Got 670 / 1000 correct (67.00)\n",
      "197 epoch,   500 iteration, loss:0.071\n",
      "Checking accuracy on test set\n",
      "Got 663 / 1000 correct (66.30)\n",
      "197 epoch,  1000 iteration, loss:0.084\n",
      " num 196 epoch \n",
      "####### Training Loss #######\n",
      "[0.0890999]\n",
      "Checking accuracy on test set\n",
      "Got 692 / 1000 correct (69.20)\n",
      "198 epoch,   500 iteration, loss:0.103\n",
      "Checking accuracy on test set\n",
      "Got 662 / 1000 correct (66.20)\n",
      "198 epoch,  1000 iteration, loss:0.074\n",
      " num 197 epoch \n",
      "####### Training Loss #######\n",
      "[0.08726242]\n",
      "Checking accuracy on test set\n",
      "Got 679 / 1000 correct (67.90)\n",
      "199 epoch,   500 iteration, loss:0.056\n",
      "Checking accuracy on test set\n",
      "Got 679 / 1000 correct (67.90)\n",
      "199 epoch,  1000 iteration, loss:0.100\n",
      " num 198 epoch \n",
      "####### Training Loss #######\n",
      "[0.08609756]\n",
      "Checking accuracy on test set\n",
      "Got 691 / 1000 correct (69.10)\n",
      "200 epoch,   500 iteration, loss:0.071\n",
      "Checking accuracy on test set\n",
      "Got 691 / 1000 correct (69.10)\n",
      "200 epoch,  1000 iteration, loss:0.070\n",
      " num 199 epoch \n",
      "####### Training Loss #######\n",
      "[0.0756123]\n",
      "Checking accuracy on test set\n",
      "Got 702 / 1000 correct (70.20)\n",
      "201 epoch,   500 iteration, loss:0.071\n",
      "Checking accuracy on test set\n",
      "Got 679 / 1000 correct (67.90)\n",
      "201 epoch,  1000 iteration, loss:0.062\n",
      " num 200 epoch \n",
      "####### Training Loss #######\n",
      "[0.06283992]\n",
      "Checking accuracy on test set\n",
      "Got 674 / 1000 correct (67.40)\n",
      "202 epoch,   500 iteration, loss:0.076\n",
      "Checking accuracy on test set\n",
      "Got 665 / 1000 correct (66.50)\n",
      "202 epoch,  1000 iteration, loss:0.093\n",
      " num 201 epoch \n",
      "####### Training Loss #######\n",
      "[0.08220739]\n",
      "Checking accuracy on test set\n",
      "Got 682 / 1000 correct (68.20)\n",
      "203 epoch,   500 iteration, loss:0.067\n",
      "Checking accuracy on test set\n",
      "Got 681 / 1000 correct (68.10)\n",
      "203 epoch,  1000 iteration, loss:0.067\n",
      " num 202 epoch \n",
      "####### Training Loss #######\n",
      "[0.06408148]\n",
      "Checking accuracy on test set\n",
      "Got 687 / 1000 correct (68.70)\n",
      "204 epoch,   500 iteration, loss:0.069\n",
      "Checking accuracy on test set\n",
      "Got 669 / 1000 correct (66.90)\n",
      "204 epoch,  1000 iteration, loss:0.089\n",
      " num 203 epoch \n",
      "####### Training Loss #######\n",
      "[0.08854968]\n",
      "Checking accuracy on test set\n",
      "Got 683 / 1000 correct (68.30)\n",
      "205 epoch,   500 iteration, loss:0.078\n",
      "Checking accuracy on test set\n",
      "Got 600 / 1000 correct (60.00)\n",
      "205 epoch,  1000 iteration, loss:0.085\n",
      " num 204 epoch \n",
      "####### Training Loss #######\n",
      "[0.08061839]\n",
      "Checking accuracy on test set\n",
      "Got 650 / 1000 correct (65.00)\n",
      "206 epoch,   500 iteration, loss:0.103\n",
      "Checking accuracy on test set\n",
      "Got 692 / 1000 correct (69.20)\n",
      "206 epoch,  1000 iteration, loss:0.087\n",
      " num 205 epoch \n",
      "####### Training Loss #######\n",
      "[0.08680713]\n",
      "Checking accuracy on test set\n",
      "Got 689 / 1000 correct (68.90)\n",
      "207 epoch,   500 iteration, loss:0.041\n",
      "Checking accuracy on test set\n",
      "Got 682 / 1000 correct (68.20)\n",
      "207 epoch,  1000 iteration, loss:0.081\n",
      " num 206 epoch \n",
      "####### Training Loss #######\n",
      "[0.06273628]\n",
      "Checking accuracy on test set\n",
      "Got 686 / 1000 correct (68.60)\n",
      "208 epoch,   500 iteration, loss:0.065\n",
      "Checking accuracy on test set\n",
      "Got 673 / 1000 correct (67.30)\n",
      "208 epoch,  1000 iteration, loss:0.071\n",
      " num 207 epoch \n",
      "####### Training Loss #######\n",
      "[0.06189098]\n",
      "Checking accuracy on test set\n",
      "Got 677 / 1000 correct (67.70)\n",
      "209 epoch,   500 iteration, loss:0.088\n",
      "Checking accuracy on test set\n",
      "Got 661 / 1000 correct (66.10)\n",
      "209 epoch,  1000 iteration, loss:0.071\n",
      " num 208 epoch \n",
      "####### Training Loss #######\n",
      "[0.0837676]\n",
      "Checking accuracy on test set\n",
      "Got 659 / 1000 correct (65.90)\n",
      "210 epoch,   500 iteration, loss:0.108\n",
      "Checking accuracy on test set\n",
      "Got 667 / 1000 correct (66.70)\n",
      "210 epoch,  1000 iteration, loss:0.084\n",
      " num 209 epoch \n",
      "####### Training Loss #######\n",
      "[0.08883201]\n",
      "Checking accuracy on test set\n",
      "Got 636 / 1000 correct (63.60)\n",
      "211 epoch,   500 iteration, loss:0.077\n",
      "Checking accuracy on test set\n",
      "Got 665 / 1000 correct (66.50)\n",
      "211 epoch,  1000 iteration, loss:0.065\n",
      " num 210 epoch \n",
      "####### Training Loss #######\n",
      "[0.07167939]\n",
      "Checking accuracy on test set\n",
      "Got 654 / 1000 correct (65.40)\n",
      "212 epoch,   500 iteration, loss:0.111\n",
      "Checking accuracy on test set\n",
      "Got 691 / 1000 correct (69.10)\n",
      "212 epoch,  1000 iteration, loss:0.143\n",
      " num 211 epoch \n",
      "####### Training Loss #######\n",
      "[0.11471827]\n",
      "Checking accuracy on test set\n",
      "Got 698 / 1000 correct (69.80)\n",
      "213 epoch,   500 iteration, loss:0.061\n",
      "Checking accuracy on test set\n",
      "Got 684 / 1000 correct (68.40)\n",
      "213 epoch,  1000 iteration, loss:0.059\n",
      " num 212 epoch \n",
      "####### Training Loss #######\n",
      "[0.06663887]\n",
      "Checking accuracy on test set\n",
      "Got 692 / 1000 correct (69.20)\n",
      "214 epoch,   500 iteration, loss:0.070\n",
      "Checking accuracy on test set\n",
      "Got 692 / 1000 correct (69.20)\n",
      "214 epoch,  1000 iteration, loss:0.075\n",
      " num 213 epoch \n",
      "####### Training Loss #######\n",
      "[0.0729533]\n",
      "Checking accuracy on test set\n",
      "Got 685 / 1000 correct (68.50)\n",
      "215 epoch,   500 iteration, loss:0.065\n",
      "Checking accuracy on test set\n",
      "Got 701 / 1000 correct (70.10)\n",
      "215 epoch,  1000 iteration, loss:0.093\n",
      " num 214 epoch \n",
      "####### Training Loss #######\n",
      "[0.07137923]\n",
      "Checking accuracy on test set\n",
      "Got 687 / 1000 correct (68.70)\n",
      "216 epoch,   500 iteration, loss:0.086\n",
      "Checking accuracy on test set\n",
      "Got 698 / 1000 correct (69.80)\n",
      "216 epoch,  1000 iteration, loss:0.062\n",
      " num 215 epoch \n",
      "####### Training Loss #######\n",
      "[0.09044225]\n",
      "Checking accuracy on test set\n",
      "Got 642 / 1000 correct (64.20)\n",
      "217 epoch,   500 iteration, loss:0.066\n",
      "Checking accuracy on test set\n",
      "Got 699 / 1000 correct (69.90)\n",
      "217 epoch,  1000 iteration, loss:0.093\n",
      " num 216 epoch \n",
      "####### Training Loss #######\n",
      "[0.07459445]\n",
      "Checking accuracy on test set\n",
      "Got 676 / 1000 correct (67.60)\n",
      "218 epoch,   500 iteration, loss:0.093\n",
      "Checking accuracy on test set\n",
      "Got 671 / 1000 correct (67.10)\n",
      "218 epoch,  1000 iteration, loss:0.087\n",
      " num 217 epoch \n",
      "####### Training Loss #######\n",
      "[0.08363319]\n",
      "Checking accuracy on test set\n",
      "Got 669 / 1000 correct (66.90)\n",
      "219 epoch,   500 iteration, loss:0.050\n",
      "Checking accuracy on test set\n",
      "Got 666 / 1000 correct (66.60)\n",
      "219 epoch,  1000 iteration, loss:0.079\n",
      " num 218 epoch \n",
      "####### Training Loss #######\n",
      "[0.06447159]\n",
      "Checking accuracy on test set\n",
      "Got 675 / 1000 correct (67.50)\n",
      "220 epoch,   500 iteration, loss:0.086\n",
      "Checking accuracy on test set\n",
      "Got 670 / 1000 correct (67.00)\n",
      "220 epoch,  1000 iteration, loss:0.080\n",
      " num 219 epoch \n",
      "####### Training Loss #######\n",
      "[0.08547021]\n",
      "Checking accuracy on test set\n",
      "Got 655 / 1000 correct (65.50)\n",
      "221 epoch,   500 iteration, loss:0.078\n",
      "Checking accuracy on test set\n",
      "Got 677 / 1000 correct (67.70)\n",
      "221 epoch,  1000 iteration, loss:0.064\n",
      " num 220 epoch \n",
      "####### Training Loss #######\n",
      "[0.07009444]\n",
      "Checking accuracy on test set\n",
      "Got 685 / 1000 correct (68.50)\n",
      "222 epoch,   500 iteration, loss:0.062\n",
      "Checking accuracy on test set\n",
      "Got 668 / 1000 correct (66.80)\n",
      "222 epoch,  1000 iteration, loss:0.083\n",
      " num 221 epoch \n",
      "####### Training Loss #######\n",
      "[0.07719453]\n",
      "Checking accuracy on test set\n",
      "Got 684 / 1000 correct (68.40)\n",
      "223 epoch,   500 iteration, loss:0.067\n",
      "Checking accuracy on test set\n",
      "Got 667 / 1000 correct (66.70)\n",
      "223 epoch,  1000 iteration, loss:0.076\n",
      " num 222 epoch \n",
      "####### Training Loss #######\n",
      "[0.07034929]\n",
      "Checking accuracy on test set\n",
      "Got 688 / 1000 correct (68.80)\n",
      "224 epoch,   500 iteration, loss:0.081\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking accuracy on test set\n",
      "Got 669 / 1000 correct (66.90)\n",
      "224 epoch,  1000 iteration, loss:0.072\n",
      " num 223 epoch \n",
      "####### Training Loss #######\n",
      "[0.07500361]\n",
      "Checking accuracy on test set\n",
      "Got 697 / 1000 correct (69.70)\n",
      "225 epoch,   500 iteration, loss:0.096\n",
      "Checking accuracy on test set\n",
      "Got 683 / 1000 correct (68.30)\n",
      "225 epoch,  1000 iteration, loss:0.088\n",
      " num 224 epoch \n",
      "####### Training Loss #######\n",
      "[0.08839882]\n",
      "Checking accuracy on test set\n",
      "Got 658 / 1000 correct (65.80)\n",
      "226 epoch,   500 iteration, loss:0.103\n",
      "Checking accuracy on test set\n",
      "Got 683 / 1000 correct (68.30)\n",
      "226 epoch,  1000 iteration, loss:0.055\n",
      " num 225 epoch \n",
      "####### Training Loss #######\n",
      "[0.07499506]\n",
      "Checking accuracy on test set\n",
      "Got 684 / 1000 correct (68.40)\n",
      "227 epoch,   500 iteration, loss:0.056\n",
      "Checking accuracy on test set\n",
      "Got 699 / 1000 correct (69.90)\n",
      "227 epoch,  1000 iteration, loss:0.052\n",
      " num 226 epoch \n",
      "####### Training Loss #######\n",
      "[0.05889051]\n",
      "Checking accuracy on test set\n",
      "Got 679 / 1000 correct (67.90)\n",
      "228 epoch,   500 iteration, loss:0.060\n",
      "Checking accuracy on test set\n",
      "Got 659 / 1000 correct (65.90)\n",
      "228 epoch,  1000 iteration, loss:0.062\n",
      " num 227 epoch \n",
      "####### Training Loss #######\n",
      "[0.06289028]\n",
      "Checking accuracy on test set\n",
      "Got 661 / 1000 correct (66.10)\n",
      "229 epoch,   500 iteration, loss:0.092\n",
      "Checking accuracy on test set\n",
      "Got 653 / 1000 correct (65.30)\n",
      "229 epoch,  1000 iteration, loss:0.088\n",
      " num 228 epoch \n",
      "####### Training Loss #######\n",
      "[0.08626694]\n",
      "Checking accuracy on test set\n",
      "Got 676 / 1000 correct (67.60)\n",
      "230 epoch,   500 iteration, loss:0.095\n",
      "Checking accuracy on test set\n",
      "Got 681 / 1000 correct (68.10)\n",
      "230 epoch,  1000 iteration, loss:0.107\n",
      " num 229 epoch \n",
      "####### Training Loss #######\n",
      "[0.09381728]\n",
      "Checking accuracy on test set\n",
      "Got 673 / 1000 correct (67.30)\n",
      "231 epoch,   500 iteration, loss:0.083\n",
      "Checking accuracy on test set\n",
      "Got 656 / 1000 correct (65.60)\n",
      "231 epoch,  1000 iteration, loss:0.082\n",
      " num 230 epoch \n",
      "####### Training Loss #######\n",
      "[0.08188464]\n",
      "Checking accuracy on test set\n",
      "Got 677 / 1000 correct (67.70)\n",
      "232 epoch,   500 iteration, loss:0.054\n",
      "Checking accuracy on test set\n",
      "Got 684 / 1000 correct (68.40)\n",
      "232 epoch,  1000 iteration, loss:0.080\n",
      " num 231 epoch \n",
      "####### Training Loss #######\n",
      "[0.06282458]\n",
      "Checking accuracy on test set\n",
      "Got 692 / 1000 correct (69.20)\n",
      "233 epoch,   500 iteration, loss:0.057\n",
      "Checking accuracy on test set\n",
      "Got 683 / 1000 correct (68.30)\n",
      "233 epoch,  1000 iteration, loss:0.088\n",
      " num 232 epoch \n",
      "####### Training Loss #######\n",
      "[0.08236979]\n",
      "Checking accuracy on test set\n",
      "Got 701 / 1000 correct (70.10)\n",
      "234 epoch,   500 iteration, loss:0.071\n",
      "Checking accuracy on test set\n",
      "Got 686 / 1000 correct (68.60)\n",
      "234 epoch,  1000 iteration, loss:0.051\n",
      " num 233 epoch \n",
      "####### Training Loss #######\n",
      "[0.06243976]\n",
      "Checking accuracy on test set\n",
      "Got 678 / 1000 correct (67.80)\n",
      "235 epoch,   500 iteration, loss:0.046\n",
      "Checking accuracy on test set\n",
      "Got 683 / 1000 correct (68.30)\n",
      "235 epoch,  1000 iteration, loss:0.068\n",
      " num 234 epoch \n",
      "####### Training Loss #######\n",
      "[0.05515516]\n",
      "Checking accuracy on test set\n",
      "Got 679 / 1000 correct (67.90)\n",
      "236 epoch,   500 iteration, loss:0.098\n",
      "Checking accuracy on test set\n",
      "Got 686 / 1000 correct (68.60)\n",
      "236 epoch,  1000 iteration, loss:0.042\n",
      " num 235 epoch \n",
      "####### Training Loss #######\n",
      "[0.07315325]\n",
      "Checking accuracy on test set\n",
      "Got 671 / 1000 correct (67.10)\n",
      "237 epoch,   500 iteration, loss:0.083\n",
      "Checking accuracy on test set\n",
      "Got 667 / 1000 correct (66.70)\n",
      "237 epoch,  1000 iteration, loss:0.096\n",
      " num 236 epoch \n",
      "####### Training Loss #######\n",
      "[0.08095327]\n",
      "Checking accuracy on test set\n",
      "Got 671 / 1000 correct (67.10)\n",
      "238 epoch,   500 iteration, loss:0.083\n",
      "Checking accuracy on test set\n",
      "Got 672 / 1000 correct (67.20)\n",
      "238 epoch,  1000 iteration, loss:0.099\n",
      " num 237 epoch \n",
      "####### Training Loss #######\n",
      "[0.08106742]\n",
      "Checking accuracy on test set\n",
      "Got 672 / 1000 correct (67.20)\n",
      "239 epoch,   500 iteration, loss:0.069\n",
      "Checking accuracy on test set\n",
      "Got 676 / 1000 correct (67.60)\n",
      "239 epoch,  1000 iteration, loss:0.060\n",
      " num 238 epoch \n",
      "####### Training Loss #######\n",
      "[0.06275095]\n",
      "Checking accuracy on test set\n",
      "Got 684 / 1000 correct (68.40)\n",
      "240 epoch,   500 iteration, loss:0.078\n",
      "Checking accuracy on test set\n",
      "Got 694 / 1000 correct (69.40)\n",
      "240 epoch,  1000 iteration, loss:0.093\n",
      " num 239 epoch \n",
      "####### Training Loss #######\n",
      "[0.0794142]\n",
      "Checking accuracy on test set\n",
      "Got 666 / 1000 correct (66.60)\n",
      "241 epoch,   500 iteration, loss:0.059\n",
      "Checking accuracy on test set\n",
      "Got 683 / 1000 correct (68.30)\n",
      "241 epoch,  1000 iteration, loss:0.085\n",
      " num 240 epoch \n",
      "####### Training Loss #######\n",
      "[0.07343744]\n",
      "Checking accuracy on test set\n",
      "Got 684 / 1000 correct (68.40)\n",
      "242 epoch,   500 iteration, loss:0.088\n",
      "Checking accuracy on test set\n",
      "Got 684 / 1000 correct (68.40)\n",
      "242 epoch,  1000 iteration, loss:0.049\n",
      " num 241 epoch \n",
      "####### Training Loss #######\n",
      "[0.06209835]\n",
      "Checking accuracy on test set\n",
      "Got 681 / 1000 correct (68.10)\n",
      "243 epoch,   500 iteration, loss:0.076\n",
      "Checking accuracy on test set\n",
      "Got 630 / 1000 correct (63.00)\n",
      "243 epoch,  1000 iteration, loss:0.078\n",
      " num 242 epoch \n",
      "####### Training Loss #######\n",
      "[0.07977069]\n",
      "Checking accuracy on test set\n",
      "Got 663 / 1000 correct (66.30)\n",
      "244 epoch,   500 iteration, loss:0.068\n",
      "Checking accuracy on test set\n",
      "Got 701 / 1000 correct (70.10)\n",
      "244 epoch,  1000 iteration, loss:0.056\n",
      " num 243 epoch \n",
      "####### Training Loss #######\n",
      "[0.06109131]\n",
      "Checking accuracy on test set\n",
      "Got 669 / 1000 correct (66.90)\n",
      "245 epoch,   500 iteration, loss:0.040\n",
      "Checking accuracy on test set\n",
      "Got 678 / 1000 correct (67.80)\n",
      "245 epoch,  1000 iteration, loss:0.063\n",
      " num 244 epoch \n",
      "####### Training Loss #######\n",
      "[0.0486477]\n",
      "Checking accuracy on test set\n",
      "Got 680 / 1000 correct (68.00)\n",
      "246 epoch,   500 iteration, loss:0.052\n",
      "Checking accuracy on test set\n",
      "Got 639 / 1000 correct (63.90)\n",
      "246 epoch,  1000 iteration, loss:0.061\n",
      " num 245 epoch \n",
      "####### Training Loss #######\n",
      "[0.06286478]\n",
      "Checking accuracy on test set\n",
      "Got 676 / 1000 correct (67.60)\n",
      "247 epoch,   500 iteration, loss:0.084\n",
      "Checking accuracy on test set\n",
      "Got 674 / 1000 correct (67.40)\n",
      "247 epoch,  1000 iteration, loss:0.063\n",
      " num 246 epoch \n",
      "####### Training Loss #######\n",
      "[0.06973384]\n",
      "Checking accuracy on test set\n",
      "Got 647 / 1000 correct (64.70)\n",
      "248 epoch,   500 iteration, loss:0.058\n",
      "Checking accuracy on test set\n",
      "Got 705 / 1000 correct (70.50)\n",
      "248 epoch,  1000 iteration, loss:0.087\n",
      " num 247 epoch \n",
      "####### Training Loss #######\n",
      "[0.06962318]\n",
      "Checking accuracy on test set\n",
      "Got 697 / 1000 correct (69.70)\n",
      "249 epoch,   500 iteration, loss:0.044\n",
      "Checking accuracy on test set\n",
      "Got 684 / 1000 correct (68.40)\n",
      "249 epoch,  1000 iteration, loss:0.059\n",
      " num 248 epoch \n",
      "####### Training Loss #######\n",
      "[0.05366644]\n",
      "Checking accuracy on test set\n",
      "Got 652 / 1000 correct (65.20)\n",
      "250 epoch,   500 iteration, loss:0.062\n",
      "Checking accuracy on test set\n",
      "Got 640 / 1000 correct (64.00)\n",
      "250 epoch,  1000 iteration, loss:0.070\n",
      " num 249 epoch \n",
      "####### Training Loss #######\n",
      "[0.08202382]\n",
      "Checking accuracy on test set\n",
      "Got 688 / 1000 correct (68.80)\n",
      "251 epoch,   500 iteration, loss:0.070\n",
      "Checking accuracy on test set\n",
      "Got 687 / 1000 correct (68.70)\n",
      "251 epoch,  1000 iteration, loss:0.046\n",
      " num 250 epoch \n",
      "####### Training Loss #######\n",
      "[0.05284653]\n",
      "Checking accuracy on test set\n",
      "Got 683 / 1000 correct (68.30)\n",
      "252 epoch,   500 iteration, loss:0.082\n",
      "Checking accuracy on test set\n",
      "Got 679 / 1000 correct (67.90)\n",
      "252 epoch,  1000 iteration, loss:0.070\n",
      " num 251 epoch \n",
      "####### Training Loss #######\n",
      "[0.07297068]\n",
      "Checking accuracy on test set\n",
      "Got 665 / 1000 correct (66.50)\n",
      "253 epoch,   500 iteration, loss:0.091\n",
      "Checking accuracy on test set\n",
      "Got 658 / 1000 correct (65.80)\n",
      "253 epoch,  1000 iteration, loss:0.069\n",
      " num 252 epoch \n",
      "####### Training Loss #######\n",
      "[0.07438047]\n",
      "Checking accuracy on test set\n",
      "Got 699 / 1000 correct (69.90)\n",
      "254 epoch,   500 iteration, loss:0.063\n",
      "Checking accuracy on test set\n",
      "Got 662 / 1000 correct (66.20)\n",
      "254 epoch,  1000 iteration, loss:0.091\n",
      " num 253 epoch \n",
      "####### Training Loss #######\n",
      "[0.07426912]\n",
      "Checking accuracy on test set\n",
      "Got 676 / 1000 correct (67.60)\n",
      "255 epoch,   500 iteration, loss:0.068\n",
      "Checking accuracy on test set\n",
      "Got 679 / 1000 correct (67.90)\n",
      "255 epoch,  1000 iteration, loss:0.072\n",
      " num 254 epoch \n",
      "####### Training Loss #######\n",
      "[0.06655219]\n",
      "Checking accuracy on test set\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 692 / 1000 correct (69.20)\n",
      "256 epoch,   500 iteration, loss:0.057\n",
      "Checking accuracy on test set\n",
      "Got 659 / 1000 correct (65.90)\n",
      "256 epoch,  1000 iteration, loss:0.096\n",
      " num 255 epoch \n",
      "####### Training Loss #######\n",
      "[0.0896666]\n",
      "Checking accuracy on test set\n",
      "Got 656 / 1000 correct (65.60)\n",
      "257 epoch,   500 iteration, loss:0.066\n",
      "Checking accuracy on test set\n",
      "Got 668 / 1000 correct (66.80)\n",
      "257 epoch,  1000 iteration, loss:0.072\n",
      " num 256 epoch \n",
      "####### Training Loss #######\n",
      "[0.06543233]\n",
      "Checking accuracy on test set\n",
      "Got 661 / 1000 correct (66.10)\n",
      "258 epoch,   500 iteration, loss:0.053\n",
      "Checking accuracy on test set\n",
      "Got 688 / 1000 correct (68.80)\n",
      "258 epoch,  1000 iteration, loss:0.068\n",
      " num 257 epoch \n",
      "####### Training Loss #######\n",
      "[0.06677199]\n",
      "Checking accuracy on test set\n",
      "Got 630 / 1000 correct (63.00)\n",
      "259 epoch,   500 iteration, loss:0.075\n",
      "Checking accuracy on test set\n",
      "Got 668 / 1000 correct (66.80)\n",
      "259 epoch,  1000 iteration, loss:0.064\n",
      " num 258 epoch \n",
      "####### Training Loss #######\n",
      "[0.06710336]\n",
      "Checking accuracy on test set\n",
      "Got 630 / 1000 correct (63.00)\n",
      "260 epoch,   500 iteration, loss:0.066\n",
      "Checking accuracy on test set\n",
      "Got 663 / 1000 correct (66.30)\n",
      "260 epoch,  1000 iteration, loss:0.104\n",
      " num 259 epoch \n",
      "####### Training Loss #######\n",
      "[0.08585501]\n",
      "Checking accuracy on test set\n",
      "Got 663 / 1000 correct (66.30)\n",
      "261 epoch,   500 iteration, loss:0.080\n",
      "Checking accuracy on test set\n",
      "Got 678 / 1000 correct (67.80)\n",
      "261 epoch,  1000 iteration, loss:0.061\n",
      " num 260 epoch \n",
      "####### Training Loss #######\n",
      "[0.06749882]\n",
      "Checking accuracy on test set\n",
      "Got 680 / 1000 correct (68.00)\n",
      "262 epoch,   500 iteration, loss:0.078\n",
      "Checking accuracy on test set\n",
      "Got 685 / 1000 correct (68.50)\n",
      "262 epoch,  1000 iteration, loss:0.057\n",
      " num 261 epoch \n",
      "####### Training Loss #######\n",
      "[0.06556444]\n",
      "Checking accuracy on test set\n",
      "Got 690 / 1000 correct (69.00)\n",
      "263 epoch,   500 iteration, loss:0.070\n",
      "Checking accuracy on test set\n",
      "Got 699 / 1000 correct (69.90)\n",
      "263 epoch,  1000 iteration, loss:0.080\n",
      " num 262 epoch \n",
      "####### Training Loss #######\n",
      "[0.07723308]\n",
      "Checking accuracy on test set\n",
      "Got 684 / 1000 correct (68.40)\n",
      "264 epoch,   500 iteration, loss:0.067\n",
      "Checking accuracy on test set\n",
      "Got 666 / 1000 correct (66.60)\n",
      "264 epoch,  1000 iteration, loss:0.052\n",
      " num 263 epoch \n",
      "####### Training Loss #######\n",
      "[0.05909674]\n",
      "Checking accuracy on test set\n",
      "Got 684 / 1000 correct (68.40)\n",
      "265 epoch,   500 iteration, loss:0.039\n",
      "Checking accuracy on test set\n",
      "Got 692 / 1000 correct (69.20)\n",
      "265 epoch,  1000 iteration, loss:0.073\n",
      " num 264 epoch \n",
      "####### Training Loss #######\n",
      "[0.05254273]\n",
      "Checking accuracy on test set\n",
      "Got 679 / 1000 correct (67.90)\n",
      "266 epoch,   500 iteration, loss:0.060\n",
      "Checking accuracy on test set\n",
      "Got 683 / 1000 correct (68.30)\n",
      "266 epoch,  1000 iteration, loss:0.044\n",
      " num 265 epoch \n",
      "####### Training Loss #######\n",
      "[0.04948385]\n",
      "Checking accuracy on test set\n",
      "Got 678 / 1000 correct (67.80)\n",
      "267 epoch,   500 iteration, loss:0.052\n",
      "Checking accuracy on test set\n",
      "Got 684 / 1000 correct (68.40)\n",
      "267 epoch,  1000 iteration, loss:0.073\n",
      " num 266 epoch \n",
      "####### Training Loss #######\n",
      "[0.0642953]\n",
      "Checking accuracy on test set\n",
      "Got 697 / 1000 correct (69.70)\n",
      "268 epoch,   500 iteration, loss:0.073\n",
      "Checking accuracy on test set\n",
      "Got 693 / 1000 correct (69.30)\n",
      "268 epoch,  1000 iteration, loss:0.064\n",
      " num 267 epoch \n",
      "####### Training Loss #######\n",
      "[0.06360546]\n",
      "Checking accuracy on test set\n",
      "Got 693 / 1000 correct (69.30)\n",
      "269 epoch,   500 iteration, loss:0.064\n",
      "Checking accuracy on test set\n",
      "Got 654 / 1000 correct (65.40)\n",
      "269 epoch,  1000 iteration, loss:0.066\n",
      " num 268 epoch \n",
      "####### Training Loss #######\n",
      "[0.06657372]\n",
      "Checking accuracy on test set\n",
      "Got 665 / 1000 correct (66.50)\n",
      "270 epoch,   500 iteration, loss:0.060\n",
      "Checking accuracy on test set\n",
      "Got 697 / 1000 correct (69.70)\n",
      "270 epoch,  1000 iteration, loss:0.072\n",
      " num 269 epoch \n",
      "####### Training Loss #######\n",
      "[0.06156669]\n",
      "Checking accuracy on test set\n",
      "Got 680 / 1000 correct (68.00)\n",
      "271 epoch,   500 iteration, loss:0.096\n",
      "Checking accuracy on test set\n",
      "Got 662 / 1000 correct (66.20)\n",
      "271 epoch,  1000 iteration, loss:0.070\n",
      " num 270 epoch \n",
      "####### Training Loss #######\n",
      "[0.08049628]\n",
      "Checking accuracy on test set\n",
      "Got 673 / 1000 correct (67.30)\n",
      "272 epoch,   500 iteration, loss:0.070\n",
      "Checking accuracy on test set\n",
      "Got 661 / 1000 correct (66.10)\n",
      "272 epoch,  1000 iteration, loss:0.062\n",
      " num 271 epoch \n",
      "####### Training Loss #######\n",
      "[0.06855798]\n",
      "Checking accuracy on test set\n",
      "Got 662 / 1000 correct (66.20)\n",
      "273 epoch,   500 iteration, loss:0.071\n",
      "Checking accuracy on test set\n",
      "Got 678 / 1000 correct (67.80)\n",
      "273 epoch,  1000 iteration, loss:0.064\n",
      " num 272 epoch \n",
      "####### Training Loss #######\n",
      "[0.06810143]\n",
      "Checking accuracy on test set\n",
      "Got 651 / 1000 correct (65.10)\n",
      "274 epoch,   500 iteration, loss:0.093\n",
      "Checking accuracy on test set\n",
      "Got 659 / 1000 correct (65.90)\n",
      "274 epoch,  1000 iteration, loss:0.103\n",
      " num 273 epoch \n",
      "####### Training Loss #######\n",
      "[0.10138902]\n",
      "Checking accuracy on test set\n",
      "Got 674 / 1000 correct (67.40)\n",
      "275 epoch,   500 iteration, loss:0.069\n",
      "Checking accuracy on test set\n",
      "Got 676 / 1000 correct (67.60)\n",
      "275 epoch,  1000 iteration, loss:0.044\n",
      " num 274 epoch \n",
      "####### Training Loss #######\n",
      "[0.05891282]\n",
      "Checking accuracy on test set\n",
      "Got 692 / 1000 correct (69.20)\n",
      "276 epoch,   500 iteration, loss:0.071\n",
      "Checking accuracy on test set\n",
      "Got 683 / 1000 correct (68.30)\n",
      "276 epoch,  1000 iteration, loss:0.065\n",
      " num 275 epoch \n",
      "####### Training Loss #######\n",
      "[0.06185239]\n",
      "Checking accuracy on test set\n",
      "Got 691 / 1000 correct (69.10)\n",
      "277 epoch,   500 iteration, loss:0.053\n",
      "Checking accuracy on test set\n",
      "Got 651 / 1000 correct (65.10)\n",
      "277 epoch,  1000 iteration, loss:0.059\n",
      " num 276 epoch \n",
      "####### Training Loss #######\n",
      "[0.05083514]\n",
      "Checking accuracy on test set\n",
      "Got 681 / 1000 correct (68.10)\n",
      "278 epoch,   500 iteration, loss:0.026\n",
      "Checking accuracy on test set\n",
      "Got 656 / 1000 correct (65.60)\n",
      "278 epoch,  1000 iteration, loss:0.051\n",
      " num 277 epoch \n",
      "####### Training Loss #######\n",
      "[0.04619986]\n",
      "Checking accuracy on test set\n",
      "Got 686 / 1000 correct (68.60)\n",
      "279 epoch,   500 iteration, loss:0.073\n",
      "Checking accuracy on test set\n",
      "Got 689 / 1000 correct (68.90)\n",
      "279 epoch,  1000 iteration, loss:0.067\n",
      " num 278 epoch \n",
      "####### Training Loss #######\n",
      "[0.06607088]\n",
      "Checking accuracy on test set\n",
      "Got 692 / 1000 correct (69.20)\n",
      "280 epoch,   500 iteration, loss:0.043\n",
      "Checking accuracy on test set\n",
      "Got 678 / 1000 correct (67.80)\n",
      "280 epoch,  1000 iteration, loss:0.055\n",
      " num 279 epoch \n",
      "####### Training Loss #######\n",
      "[0.05337443]\n",
      "Checking accuracy on test set\n",
      "Got 682 / 1000 correct (68.20)\n",
      "281 epoch,   500 iteration, loss:0.062\n",
      "Checking accuracy on test set\n",
      "Got 689 / 1000 correct (68.90)\n",
      "281 epoch,  1000 iteration, loss:0.063\n",
      " num 280 epoch \n",
      "####### Training Loss #######\n",
      "[0.05704292]\n",
      "Checking accuracy on test set\n",
      "Got 667 / 1000 correct (66.70)\n",
      "282 epoch,   500 iteration, loss:0.064\n",
      "Checking accuracy on test set\n",
      "Got 685 / 1000 correct (68.50)\n",
      "282 epoch,  1000 iteration, loss:0.073\n",
      " num 281 epoch \n",
      "####### Training Loss #######\n",
      "[0.06925867]\n",
      "Checking accuracy on test set\n",
      "Got 685 / 1000 correct (68.50)\n",
      "283 epoch,   500 iteration, loss:0.048\n",
      "Checking accuracy on test set\n",
      "Got 673 / 1000 correct (67.30)\n",
      "283 epoch,  1000 iteration, loss:0.058\n",
      " num 282 epoch \n",
      "####### Training Loss #######\n",
      "[0.051798]\n",
      "Checking accuracy on test set\n",
      "Got 681 / 1000 correct (68.10)\n",
      "284 epoch,   500 iteration, loss:0.046\n",
      "Checking accuracy on test set\n",
      "Got 659 / 1000 correct (65.90)\n",
      "284 epoch,  1000 iteration, loss:0.077\n",
      " num 283 epoch \n",
      "####### Training Loss #######\n",
      "[0.05755549]\n",
      "Checking accuracy on test set\n",
      "Got 703 / 1000 correct (70.30)\n",
      "285 epoch,   500 iteration, loss:0.062\n",
      "Checking accuracy on test set\n",
      "Got 688 / 1000 correct (68.80)\n",
      "285 epoch,  1000 iteration, loss:0.061\n",
      " num 284 epoch \n",
      "####### Training Loss #######\n",
      "[0.06999175]\n",
      "Checking accuracy on test set\n",
      "Got 655 / 1000 correct (65.50)\n",
      "286 epoch,   500 iteration, loss:0.053\n",
      "Checking accuracy on test set\n",
      "Got 687 / 1000 correct (68.70)\n",
      "286 epoch,  1000 iteration, loss:0.111\n",
      " num 285 epoch \n",
      "####### Training Loss #######\n",
      "[0.07751645]\n",
      "Checking accuracy on test set\n",
      "Got 669 / 1000 correct (66.90)\n",
      "287 epoch,   500 iteration, loss:0.072\n",
      "Checking accuracy on test set\n",
      "Got 674 / 1000 correct (67.40)\n",
      "287 epoch,  1000 iteration, loss:0.058\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " num 286 epoch \n",
      "####### Training Loss #######\n",
      "[0.06819486]\n",
      "Checking accuracy on test set\n",
      "Got 697 / 1000 correct (69.70)\n",
      "288 epoch,   500 iteration, loss:0.041\n",
      "Checking accuracy on test set\n",
      "Got 667 / 1000 correct (66.70)\n",
      "288 epoch,  1000 iteration, loss:0.061\n",
      " num 287 epoch \n",
      "####### Training Loss #######\n",
      "[0.05033919]\n",
      "Checking accuracy on test set\n",
      "Got 656 / 1000 correct (65.60)\n",
      "289 epoch,   500 iteration, loss:0.081\n",
      "Checking accuracy on test set\n",
      "Got 688 / 1000 correct (68.80)\n",
      "289 epoch,  1000 iteration, loss:0.065\n",
      " num 288 epoch \n",
      "####### Training Loss #######\n",
      "[0.06699356]\n",
      "Checking accuracy on test set\n",
      "Got 670 / 1000 correct (67.00)\n",
      "290 epoch,   500 iteration, loss:0.081\n",
      "Checking accuracy on test set\n",
      "Got 680 / 1000 correct (68.00)\n",
      "290 epoch,  1000 iteration, loss:0.095\n",
      " num 289 epoch \n",
      "####### Training Loss #######\n",
      "[0.08763869]\n",
      "Checking accuracy on test set\n",
      "Got 696 / 1000 correct (69.60)\n",
      "291 epoch,   500 iteration, loss:0.052\n",
      "Checking accuracy on test set\n",
      "Got 675 / 1000 correct (67.50)\n",
      "291 epoch,  1000 iteration, loss:0.047\n",
      " num 290 epoch \n",
      "####### Training Loss #######\n",
      "[0.04979164]\n",
      "Checking accuracy on test set\n",
      "Got 657 / 1000 correct (65.70)\n",
      "292 epoch,   500 iteration, loss:0.046\n",
      "Checking accuracy on test set\n",
      "Got 660 / 1000 correct (66.00)\n",
      "292 epoch,  1000 iteration, loss:0.091\n",
      " num 291 epoch \n",
      "####### Training Loss #######\n",
      "[0.06800831]\n",
      "Checking accuracy on test set\n",
      "Got 714 / 1000 correct (71.40)\n",
      "293 epoch,   500 iteration, loss:0.059\n",
      "Checking accuracy on test set\n",
      "Got 659 / 1000 correct (65.90)\n",
      "293 epoch,  1000 iteration, loss:0.075\n",
      " num 292 epoch \n",
      "####### Training Loss #######\n",
      "[0.06563071]\n",
      "Checking accuracy on test set\n",
      "Got 690 / 1000 correct (69.00)\n",
      "294 epoch,   500 iteration, loss:0.049\n",
      "Checking accuracy on test set\n",
      "Got 670 / 1000 correct (67.00)\n",
      "294 epoch,  1000 iteration, loss:0.042\n",
      " num 293 epoch \n",
      "####### Training Loss #######\n",
      "[0.04775892]\n",
      "Checking accuracy on test set\n",
      "Got 673 / 1000 correct (67.30)\n",
      "295 epoch,   500 iteration, loss:0.082\n",
      "Checking accuracy on test set\n",
      "Got 693 / 1000 correct (69.30)\n",
      "295 epoch,  1000 iteration, loss:0.093\n",
      " num 294 epoch \n",
      "####### Training Loss #######\n",
      "[0.083139]\n",
      "Checking accuracy on test set\n",
      "Got 679 / 1000 correct (67.90)\n",
      "296 epoch,   500 iteration, loss:0.055\n",
      "Checking accuracy on test set\n",
      "Got 699 / 1000 correct (69.90)\n",
      "296 epoch,  1000 iteration, loss:0.045\n",
      " num 295 epoch \n",
      "####### Training Loss #######\n",
      "[0.05439909]\n",
      "Checking accuracy on test set\n",
      "Got 643 / 1000 correct (64.30)\n",
      "297 epoch,   500 iteration, loss:0.053\n",
      "Checking accuracy on test set\n",
      "Got 676 / 1000 correct (67.60)\n",
      "297 epoch,  1000 iteration, loss:0.087\n",
      " num 296 epoch \n",
      "####### Training Loss #######\n",
      "[0.06695238]\n",
      "Checking accuracy on test set\n",
      "Got 712 / 1000 correct (71.20)\n",
      "298 epoch,   500 iteration, loss:0.057\n",
      "Checking accuracy on test set\n",
      "Got 640 / 1000 correct (64.00)\n",
      "298 epoch,  1000 iteration, loss:0.054\n",
      " num 297 epoch \n",
      "####### Training Loss #######\n",
      "[0.05216285]\n",
      "Checking accuracy on test set\n",
      "Got 678 / 1000 correct (67.80)\n",
      "299 epoch,   500 iteration, loss:0.072\n",
      "Checking accuracy on test set\n",
      "Got 673 / 1000 correct (67.30)\n",
      "299 epoch,  1000 iteration, loss:0.048\n",
      " num 298 epoch \n",
      "####### Training Loss #######\n",
      "[0.06689913]\n",
      "Checking accuracy on test set\n",
      "Got 688 / 1000 correct (68.80)\n",
      "300 epoch,   500 iteration, loss:0.074\n",
      "Checking accuracy on test set\n",
      "Got 668 / 1000 correct (66.80)\n",
      "300 epoch,  1000 iteration, loss:0.053\n",
      " num 299 epoch \n",
      "####### Training Loss #######\n",
      "[0.06236406]\n",
      "Checking accuracy on test set\n",
      "Got 690 / 1000 correct (69.00)\n",
      "301 epoch,   500 iteration, loss:0.085\n",
      "Checking accuracy on test set\n",
      "Got 686 / 1000 correct (68.60)\n",
      "301 epoch,  1000 iteration, loss:0.045\n",
      " num 300 epoch \n",
      "####### Training Loss #######\n",
      "[0.06184217]\n",
      "Checking accuracy on test set\n",
      "Got 687 / 1000 correct (68.70)\n",
      "302 epoch,   500 iteration, loss:0.084\n",
      "Checking accuracy on test set\n",
      "Got 699 / 1000 correct (69.90)\n",
      "302 epoch,  1000 iteration, loss:0.035\n",
      " num 301 epoch \n",
      "####### Training Loss #######\n",
      "[0.06187848]\n",
      "Checking accuracy on test set\n",
      "Got 662 / 1000 correct (66.20)\n",
      "303 epoch,   500 iteration, loss:0.061\n",
      "Checking accuracy on test set\n",
      "Got 676 / 1000 correct (67.60)\n",
      "303 epoch,  1000 iteration, loss:0.085\n",
      " num 302 epoch \n",
      "####### Training Loss #######\n",
      "[0.07352694]\n",
      "Checking accuracy on test set\n",
      "Got 690 / 1000 correct (69.00)\n",
      "304 epoch,   500 iteration, loss:0.060\n",
      "Checking accuracy on test set\n",
      "Got 665 / 1000 correct (66.50)\n",
      "304 epoch,  1000 iteration, loss:0.063\n",
      " num 303 epoch \n",
      "####### Training Loss #######\n",
      "[0.05754504]\n",
      "Checking accuracy on test set\n",
      "Got 694 / 1000 correct (69.40)\n",
      "305 epoch,   500 iteration, loss:0.056\n",
      "Checking accuracy on test set\n",
      "Got 680 / 1000 correct (68.00)\n",
      "305 epoch,  1000 iteration, loss:0.056\n",
      " num 304 epoch \n",
      "####### Training Loss #######\n",
      "[0.06334582]\n",
      "Checking accuracy on test set\n",
      "Got 699 / 1000 correct (69.90)\n",
      "306 epoch,   500 iteration, loss:0.065\n",
      "Checking accuracy on test set\n",
      "Got 692 / 1000 correct (69.20)\n",
      "306 epoch,  1000 iteration, loss:0.076\n",
      " num 305 epoch \n",
      "####### Training Loss #######\n",
      "[0.0661648]\n",
      "Checking accuracy on test set\n",
      "Got 718 / 1000 correct (71.80)\n",
      "307 epoch,   500 iteration, loss:0.035\n",
      "Checking accuracy on test set\n",
      "Got 709 / 1000 correct (70.90)\n",
      "307 epoch,  1000 iteration, loss:0.051\n",
      " num 306 epoch \n",
      "####### Training Loss #######\n",
      "[0.04731764]\n",
      "Checking accuracy on test set\n",
      "Got 684 / 1000 correct (68.40)\n",
      "308 epoch,   500 iteration, loss:0.058\n",
      "Checking accuracy on test set\n",
      "Got 710 / 1000 correct (71.00)\n",
      "308 epoch,  1000 iteration, loss:0.027\n",
      " num 307 epoch \n",
      "####### Training Loss #######\n",
      "[0.03902231]\n",
      "Checking accuracy on test set\n",
      "Got 685 / 1000 correct (68.50)\n",
      "309 epoch,   500 iteration, loss:0.064\n",
      "Checking accuracy on test set\n",
      "Got 665 / 1000 correct (66.50)\n",
      "309 epoch,  1000 iteration, loss:0.081\n",
      " num 308 epoch \n",
      "####### Training Loss #######\n",
      "[0.07137887]\n",
      "Checking accuracy on test set\n",
      "Got 650 / 1000 correct (65.00)\n",
      "310 epoch,   500 iteration, loss:0.063\n",
      "Checking accuracy on test set\n",
      "Got 665 / 1000 correct (66.50)\n",
      "310 epoch,  1000 iteration, loss:0.042\n",
      " num 309 epoch \n",
      "####### Training Loss #######\n",
      "[0.0535752]\n",
      "Checking accuracy on test set\n",
      "Got 700 / 1000 correct (70.00)\n",
      "311 epoch,   500 iteration, loss:0.055\n",
      "Checking accuracy on test set\n",
      "Got 639 / 1000 correct (63.90)\n",
      "311 epoch,  1000 iteration, loss:0.058\n",
      " num 310 epoch \n",
      "####### Training Loss #######\n",
      "[0.0577468]\n",
      "Checking accuracy on test set\n",
      "Got 672 / 1000 correct (67.20)\n",
      "312 epoch,   500 iteration, loss:0.068\n",
      "Checking accuracy on test set\n",
      "Got 686 / 1000 correct (68.60)\n",
      "312 epoch,  1000 iteration, loss:0.087\n",
      " num 311 epoch \n",
      "####### Training Loss #######\n",
      "[0.06806543]\n",
      "Checking accuracy on test set\n",
      "Got 683 / 1000 correct (68.30)\n",
      "313 epoch,   500 iteration, loss:0.043\n",
      "Checking accuracy on test set\n",
      "Got 677 / 1000 correct (67.70)\n",
      "313 epoch,  1000 iteration, loss:0.043\n",
      " num 312 epoch \n",
      "####### Training Loss #######\n",
      "[0.04389916]\n",
      "Checking accuracy on test set\n",
      "Got 684 / 1000 correct (68.40)\n",
      "314 epoch,   500 iteration, loss:0.058\n",
      "Checking accuracy on test set\n",
      "Got 647 / 1000 correct (64.70)\n",
      "314 epoch,  1000 iteration, loss:0.080\n",
      " num 313 epoch \n",
      "####### Training Loss #######\n",
      "[0.07847805]\n",
      "Checking accuracy on test set\n",
      "Got 690 / 1000 correct (69.00)\n",
      "315 epoch,   500 iteration, loss:0.050\n",
      "Checking accuracy on test set\n",
      "Got 683 / 1000 correct (68.30)\n",
      "315 epoch,  1000 iteration, loss:0.092\n",
      " num 314 epoch \n",
      "####### Training Loss #######\n",
      "[0.0743361]\n",
      "Checking accuracy on test set\n",
      "Got 669 / 1000 correct (66.90)\n",
      "316 epoch,   500 iteration, loss:0.063\n",
      "Checking accuracy on test set\n",
      "Got 699 / 1000 correct (69.90)\n",
      "316 epoch,  1000 iteration, loss:0.051\n",
      " num 315 epoch \n",
      "####### Training Loss #######\n",
      "[0.0603614]\n",
      "Checking accuracy on test set\n",
      "Got 652 / 1000 correct (65.20)\n",
      "317 epoch,   500 iteration, loss:0.055\n",
      "Checking accuracy on test set\n",
      "Got 692 / 1000 correct (69.20)\n",
      "317 epoch,  1000 iteration, loss:0.087\n",
      " num 316 epoch \n",
      "####### Training Loss #######\n",
      "[0.06619666]\n",
      "Checking accuracy on test set\n",
      "Got 681 / 1000 correct (68.10)\n",
      "318 epoch,   500 iteration, loss:0.079\n",
      "Checking accuracy on test set\n",
      "Got 694 / 1000 correct (69.40)\n",
      "318 epoch,  1000 iteration, loss:0.048\n",
      " num 317 epoch \n",
      "####### Training Loss #######\n",
      "[0.05645768]\n",
      "Checking accuracy on test set\n",
      "Got 685 / 1000 correct (68.50)\n",
      "319 epoch,   500 iteration, loss:0.054\n",
      "Checking accuracy on test set\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 681 / 1000 correct (68.10)\n",
      "319 epoch,  1000 iteration, loss:0.048\n",
      " num 318 epoch \n",
      "####### Training Loss #######\n",
      "[0.0448185]\n",
      "Checking accuracy on test set\n",
      "Got 669 / 1000 correct (66.90)\n",
      "320 epoch,   500 iteration, loss:0.077\n",
      "Checking accuracy on test set\n",
      "Got 684 / 1000 correct (68.40)\n",
      "320 epoch,  1000 iteration, loss:0.094\n",
      " num 319 epoch \n",
      "####### Training Loss #######\n",
      "[0.08120636]\n",
      "Checking accuracy on test set\n",
      "Got 697 / 1000 correct (69.70)\n",
      "321 epoch,   500 iteration, loss:0.050\n",
      "Checking accuracy on test set\n",
      "Got 676 / 1000 correct (67.60)\n",
      "321 epoch,  1000 iteration, loss:0.033\n",
      " num 320 epoch \n",
      "####### Training Loss #######\n",
      "[0.04696839]\n",
      "Checking accuracy on test set\n",
      "Got 687 / 1000 correct (68.70)\n",
      "322 epoch,   500 iteration, loss:0.060\n",
      "Checking accuracy on test set\n",
      "Got 714 / 1000 correct (71.40)\n",
      "322 epoch,  1000 iteration, loss:0.054\n",
      " num 321 epoch \n",
      "####### Training Loss #######\n",
      "[0.05568686]\n",
      "Checking accuracy on test set\n",
      "Got 694 / 1000 correct (69.40)\n",
      "323 epoch,   500 iteration, loss:0.091\n",
      "Checking accuracy on test set\n",
      "Got 630 / 1000 correct (63.00)\n",
      "323 epoch,  1000 iteration, loss:0.070\n",
      " num 322 epoch \n",
      "####### Training Loss #######\n",
      "[0.08578689]\n",
      "Checking accuracy on test set\n",
      "Got 705 / 1000 correct (70.50)\n",
      "324 epoch,   500 iteration, loss:0.116\n",
      "Checking accuracy on test set\n",
      "Got 688 / 1000 correct (68.80)\n",
      "324 epoch,  1000 iteration, loss:0.067\n",
      " num 323 epoch \n",
      "####### Training Loss #######\n",
      "[0.08653694]\n",
      "Checking accuracy on test set\n",
      "Got 658 / 1000 correct (65.80)\n",
      "325 epoch,   500 iteration, loss:0.048\n",
      "Checking accuracy on test set\n",
      "Got 647 / 1000 correct (64.70)\n",
      "325 epoch,  1000 iteration, loss:0.092\n",
      " num 324 epoch \n",
      "####### Training Loss #######\n",
      "[0.06950114]\n",
      "Checking accuracy on test set\n",
      "Got 653 / 1000 correct (65.30)\n",
      "326 epoch,   500 iteration, loss:0.078\n",
      "Checking accuracy on test set\n",
      "Got 686 / 1000 correct (68.60)\n",
      "326 epoch,  1000 iteration, loss:0.047\n",
      " num 325 epoch \n",
      "####### Training Loss #######\n",
      "[0.06145617]\n",
      "Checking accuracy on test set\n",
      "Got 692 / 1000 correct (69.20)\n",
      "327 epoch,   500 iteration, loss:0.053\n",
      "Checking accuracy on test set\n",
      "Got 692 / 1000 correct (69.20)\n",
      "327 epoch,  1000 iteration, loss:0.048\n",
      " num 326 epoch \n",
      "####### Training Loss #######\n",
      "[0.04896391]\n",
      "Checking accuracy on test set\n",
      "Got 638 / 1000 correct (63.80)\n",
      "328 epoch,   500 iteration, loss:0.055\n",
      "Checking accuracy on test set\n",
      "Got 685 / 1000 correct (68.50)\n",
      "328 epoch,  1000 iteration, loss:0.055\n",
      " num 327 epoch \n",
      "####### Training Loss #######\n",
      "[0.05633398]\n",
      "Checking accuracy on test set\n",
      "Got 660 / 1000 correct (66.00)\n",
      "329 epoch,   500 iteration, loss:0.072\n",
      "Checking accuracy on test set\n",
      "Got 687 / 1000 correct (68.70)\n",
      "329 epoch,  1000 iteration, loss:0.079\n",
      " num 328 epoch \n",
      "####### Training Loss #######\n",
      "[0.07924572]\n",
      "Checking accuracy on test set\n",
      "Got 680 / 1000 correct (68.00)\n",
      "330 epoch,   500 iteration, loss:0.061\n",
      "Checking accuracy on test set\n",
      "Got 680 / 1000 correct (68.00)\n",
      "330 epoch,  1000 iteration, loss:0.062\n",
      " num 329 epoch \n",
      "####### Training Loss #######\n",
      "[0.06471147]\n",
      "Checking accuracy on test set\n",
      "Got 658 / 1000 correct (65.80)\n",
      "331 epoch,   500 iteration, loss:0.048\n",
      "Checking accuracy on test set\n",
      "Got 687 / 1000 correct (68.70)\n",
      "331 epoch,  1000 iteration, loss:0.063\n",
      " num 330 epoch \n",
      "####### Training Loss #######\n",
      "[0.05268543]\n",
      "Checking accuracy on test set\n",
      "Got 671 / 1000 correct (67.10)\n",
      "332 epoch,   500 iteration, loss:0.094\n",
      "Checking accuracy on test set\n",
      "Got 680 / 1000 correct (68.00)\n",
      "332 epoch,  1000 iteration, loss:0.082\n",
      " num 331 epoch \n",
      "####### Training Loss #######\n",
      "[0.07966507]\n",
      "Checking accuracy on test set\n",
      "Got 690 / 1000 correct (69.00)\n",
      "333 epoch,   500 iteration, loss:0.069\n",
      "Checking accuracy on test set\n",
      "Got 686 / 1000 correct (68.60)\n",
      "333 epoch,  1000 iteration, loss:0.065\n",
      " num 332 epoch \n",
      "####### Training Loss #######\n",
      "[0.06743128]\n",
      "Checking accuracy on test set\n",
      "Got 689 / 1000 correct (68.90)\n",
      "334 epoch,   500 iteration, loss:0.081\n",
      "Checking accuracy on test set\n",
      "Got 659 / 1000 correct (65.90)\n",
      "334 epoch,  1000 iteration, loss:0.093\n",
      " num 333 epoch \n",
      "####### Training Loss #######\n",
      "[0.08189422]\n",
      "Checking accuracy on test set\n",
      "Got 677 / 1000 correct (67.70)\n",
      "335 epoch,   500 iteration, loss:0.053\n",
      "Checking accuracy on test set\n",
      "Got 692 / 1000 correct (69.20)\n",
      "335 epoch,  1000 iteration, loss:0.061\n",
      " num 334 epoch \n",
      "####### Training Loss #######\n",
      "[0.05613632]\n",
      "Checking accuracy on test set\n",
      "Got 655 / 1000 correct (65.50)\n",
      "336 epoch,   500 iteration, loss:0.036\n",
      "Checking accuracy on test set\n",
      "Got 662 / 1000 correct (66.20)\n",
      "336 epoch,  1000 iteration, loss:0.056\n",
      " num 335 epoch \n",
      "####### Training Loss #######\n",
      "[0.04263626]\n",
      "Checking accuracy on test set\n",
      "Got 701 / 1000 correct (70.10)\n",
      "337 epoch,   500 iteration, loss:0.031\n",
      "Checking accuracy on test set\n",
      "Got 678 / 1000 correct (67.80)\n",
      "337 epoch,  1000 iteration, loss:0.062\n",
      " num 336 epoch \n",
      "####### Training Loss #######\n",
      "[0.04727138]\n",
      "Checking accuracy on test set\n",
      "Got 654 / 1000 correct (65.40)\n",
      "338 epoch,   500 iteration, loss:0.065\n",
      "Checking accuracy on test set\n",
      "Got 675 / 1000 correct (67.50)\n",
      "338 epoch,  1000 iteration, loss:0.069\n",
      " num 337 epoch \n",
      "####### Training Loss #######\n",
      "[0.06379339]\n",
      "Checking accuracy on test set\n",
      "Got 661 / 1000 correct (66.10)\n",
      "339 epoch,   500 iteration, loss:0.032\n",
      "Checking accuracy on test set\n",
      "Got 693 / 1000 correct (69.30)\n",
      "339 epoch,  1000 iteration, loss:0.070\n",
      " num 338 epoch \n",
      "####### Training Loss #######\n",
      "[0.05371742]\n",
      "Checking accuracy on test set\n",
      "Got 682 / 1000 correct (68.20)\n",
      "340 epoch,   500 iteration, loss:0.070\n",
      "Checking accuracy on test set\n",
      "Got 674 / 1000 correct (67.40)\n",
      "340 epoch,  1000 iteration, loss:0.039\n",
      " num 339 epoch \n",
      "####### Training Loss #######\n",
      "[0.05015322]\n",
      "Checking accuracy on test set\n",
      "Got 687 / 1000 correct (68.70)\n",
      "341 epoch,   500 iteration, loss:0.050\n",
      "Checking accuracy on test set\n",
      "Got 703 / 1000 correct (70.30)\n",
      "341 epoch,  1000 iteration, loss:0.049\n",
      " num 340 epoch \n",
      "####### Training Loss #######\n",
      "[0.05050963]\n",
      "Checking accuracy on test set\n",
      "Got 673 / 1000 correct (67.30)\n",
      "342 epoch,   500 iteration, loss:0.040\n",
      "Checking accuracy on test set\n",
      "Got 689 / 1000 correct (68.90)\n",
      "342 epoch,  1000 iteration, loss:0.076\n",
      " num 341 epoch \n",
      "####### Training Loss #######\n",
      "[0.05839926]\n",
      "Checking accuracy on test set\n",
      "Got 678 / 1000 correct (67.80)\n",
      "343 epoch,   500 iteration, loss:0.074\n",
      "Checking accuracy on test set\n",
      "Got 684 / 1000 correct (68.40)\n",
      "343 epoch,  1000 iteration, loss:0.057\n",
      " num 342 epoch \n",
      "####### Training Loss #######\n",
      "[0.06414009]\n",
      "Checking accuracy on test set\n",
      "Got 685 / 1000 correct (68.50)\n",
      "344 epoch,   500 iteration, loss:0.051\n",
      "Checking accuracy on test set\n",
      "Got 678 / 1000 correct (67.80)\n",
      "344 epoch,  1000 iteration, loss:0.044\n",
      " num 343 epoch \n",
      "####### Training Loss #######\n",
      "[0.05275133]\n",
      "Checking accuracy on test set\n",
      "Got 710 / 1000 correct (71.00)\n",
      "345 epoch,   500 iteration, loss:0.025\n",
      "Checking accuracy on test set\n",
      "Got 676 / 1000 correct (67.60)\n",
      "345 epoch,  1000 iteration, loss:0.047\n",
      " num 344 epoch \n",
      "####### Training Loss #######\n",
      "[0.03472924]\n",
      "Checking accuracy on test set\n",
      "Got 691 / 1000 correct (69.10)\n",
      "346 epoch,   500 iteration, loss:0.075\n",
      "Checking accuracy on test set\n",
      "Got 711 / 1000 correct (71.10)\n",
      "346 epoch,  1000 iteration, loss:0.034\n",
      " num 345 epoch \n",
      "####### Training Loss #######\n",
      "[0.05171276]\n",
      "Checking accuracy on test set\n",
      "Got 671 / 1000 correct (67.10)\n",
      "347 epoch,   500 iteration, loss:0.067\n",
      "Checking accuracy on test set\n",
      "Got 689 / 1000 correct (68.90)\n",
      "347 epoch,  1000 iteration, loss:0.028\n",
      " num 346 epoch \n",
      "####### Training Loss #######\n",
      "[0.04481862]\n",
      "Checking accuracy on test set\n",
      "Got 677 / 1000 correct (67.70)\n",
      "348 epoch,   500 iteration, loss:0.080\n",
      "Checking accuracy on test set\n",
      "Got 681 / 1000 correct (68.10)\n",
      "348 epoch,  1000 iteration, loss:0.049\n",
      " num 347 epoch \n",
      "####### Training Loss #######\n",
      "[0.06506968]\n",
      "Checking accuracy on test set\n",
      "Got 640 / 1000 correct (64.00)\n",
      "349 epoch,   500 iteration, loss:0.069\n",
      "Checking accuracy on test set\n",
      "Got 674 / 1000 correct (67.40)\n",
      "349 epoch,  1000 iteration, loss:0.072\n",
      " num 348 epoch \n",
      "####### Training Loss #######\n",
      "[0.07107701]\n",
      "Checking accuracy on test set\n",
      "Got 681 / 1000 correct (68.10)\n",
      "350 epoch,   500 iteration, loss:0.057\n",
      "Checking accuracy on test set\n",
      "Got 694 / 1000 correct (69.40)\n",
      "350 epoch,  1000 iteration, loss:0.070\n",
      " num 349 epoch \n",
      "####### Training Loss #######\n",
      "[0.05812959]\n",
      "finish training \n",
      "\n",
      "now begin saving datum for next step plotting\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now plotting accuracies and losses\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (1,) and (700,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-9b630f3ad2bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mall_cnn_c_class1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_ALL_Conv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mALL_CNN_C\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m all_cnn_c_class1 = running_model_B(run_num, all_cnn_c_class1, net_name, lr, epoch, \n\u001b[0;32m---> 10\u001b[0;31m                         loaderA_train, loaderA_test)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-dc4bcd2e77a1>\u001b[0m in \u001b[0;36mrunning_model_B\u001b[0;34m(run_num, net, net_name, lr_list, epoch_list, loader_train, loader_test)\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray_epoch_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'--r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'iteration'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Anacondas/anaconda3/envs/cs231n/lib/python3.6/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2747\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2748\u001b[0m     return gca().plot(\n\u001b[0;32m-> 2749\u001b[0;31m         *args, scalex=scalex, scaley=scaley, data=data, **kwargs)\n\u001b[0m\u001b[1;32m   2750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2751\u001b[0m \u001b[0;31m# Autogenerated by boilerplate.py.  Do not edit as changes will be lost.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Anacondas/anaconda3/envs/cs231n/lib/python3.6/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1783\u001b[0m                         \u001b[0;34m\"the Matplotlib list!)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlabel_namer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1784\u001b[0m                         RuntimeWarning, stacklevel=2)\n\u001b[0;32m-> 1785\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1787\u001b[0m         inner.__doc__ = _add_data_doc(inner.__doc__,\n",
      "\u001b[0;32m~/Anacondas/anaconda3/envs/cs231n/lib/python3.6/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1602\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_alias_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1604\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1605\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1606\u001b[0m             \u001b[0mlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Anacondas/anaconda3/envs/cs231n/lib/python3.6/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_grab_next_args\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    391\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Anacondas/anaconda3/envs/cs231n/lib/python3.6/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_of\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_xy_from_xy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'plot'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Anacondas/anaconda3/envs/cs231n/lib/python3.6/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_xy_from_xy\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m             raise ValueError(\"x and y must have same first dimension, but \"\n\u001b[0;32m--> 231\u001b[0;31m                              \"have shapes {} and {}\".format(x.shape, y.shape))\n\u001b[0m\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             raise ValueError(\"x and y can be no greater than 2-D, but have \"\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (1,) and (700,)"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr = [0.01, 0.005, 0.001, 0.0005]\n",
    "epoch = [200, 250, 300] # first20\n",
    "\n",
    "run_num = 10\n",
    "net_name = 'ALL_CNN_C_Class1'\n",
    "\n",
    "#torch.cuda.set_device(0)\n",
    "all_cnn_c_class1 = new_ALL_Conv.ALL_CNN_C()\n",
    "all_cnn_c_class1 = running_model_B(run_num, all_cnn_c_class1, net_name, lr, epoch, \n",
    "                        loaderA_train, loaderA_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
